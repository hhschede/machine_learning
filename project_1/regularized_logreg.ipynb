{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing\n",
    "\n",
    "# modules\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import datetime\n",
    "\n",
    "# functions\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "\n",
    "#data\n",
    "(labels, data, ids) = load_csv_data(\"data/train.csv\")  # load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dimensions of the training data set was 1000 samples and 30 columns. After feature and sample filtering, there are 909 samples and 23 columns\n",
      "[[ 3.40050374e-01  2.13531849e-01  4.24416038e-01 ...  1.49229172e+00\n",
      "   3.12267654e-01  3.51886179e-01]\n",
      " [ 7.77906349e-01  7.54924948e-01  5.70705173e-01 ...  4.88912825e-01\n",
      "   8.27504735e-01 -3.21857058e-01]\n",
      " [ 0.00000000e+00  3.70988790e+00  1.18523856e+00 ...  1.42399155e+00\n",
      "  -1.47157837e+00 -3.41637392e-01]\n",
      " ...\n",
      " [ 3.36854202e-01 -1.37467690e+00  2.95412175e-01 ...  1.22146311e-17\n",
      "   2.87916790e-17 -7.84827057e-01]\n",
      " [-9.68957162e-01 -4.26060523e-01 -7.17521064e-01 ...  1.22146311e-17\n",
      "   2.87916790e-17 -7.84827057e-01]\n",
      " [ 1.32474397e+00  2.20102315e+00  3.11994136e-01 ... -3.15902671e-01\n",
      "  -3.04717926e-01 -3.85694958e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Function that returns tuple (data_processed and new_labels)\n",
    "\n",
    "def process_data(data, labels, sample_filtering=True, feature_filtering=True):\n",
    "    # Preparing data\n",
    "\n",
    "    # set values of -999 to NaN. \n",
    "    # Then calculate the means of the features. \n",
    "    # Replace NaN values with new values.\n",
    "\n",
    "    data_process = np.array(data[:1000,:])\n",
    "    labels_select = np.array(labels[:1000])\n",
    "    lab = []\n",
    "    for entry in labels_select:\n",
    "        if int(entry) == 1:\n",
    "            lab.append(1)\n",
    "        else:\n",
    "            lab.append(0)\n",
    "    lab = np.array(lab)\n",
    "    data_process[data_process == -999] = np.nan\n",
    "\n",
    "    # Filtering weak features and samples\n",
    "\n",
    "    # retrieve percentage for each feature \n",
    "    # - how many nan's are there?\n",
    "    \n",
    "    if feature_filtering:\n",
    "        nan_count = []\n",
    "        for c in data_process.T:\n",
    "            count = 0\n",
    "            for e in c:\n",
    "                if np.isnan(e):\n",
    "                    count += 1\n",
    "            pcent = count / data_process.shape[0]\n",
    "            nan_count.append(pcent)\n",
    "\n",
    "        # filter out features which have nan values of 50%\n",
    "\n",
    "        data_set_filtered = []\n",
    "        for idx, entry in enumerate(nan_count):\n",
    "            if entry < 0.6:\n",
    "                #append the column of the original dataset that is good\n",
    "                data_set_filtered.append(data_process.T[idx]) \n",
    "        #save that shit as an np array\n",
    "        data_set_filtered = np.array(data_set_filtered).T #save that shit as an np array\n",
    "        \n",
    "    if sample_filtering:\n",
    "        nan_count_2 = []\n",
    "        data_set_filtered_2 = [] # dataset filtered for columns and samples\n",
    "        y = [] # array that gets rid of entries that are no longer corresponding in the dataframe\n",
    "        for sample in data_set_filtered:\n",
    "            count = 0\n",
    "            for col in sample:\n",
    "                if np.isnan(col):\n",
    "                    count += 1\n",
    "            pcent = count / data_set_filtered.shape[1]\n",
    "            nan_count_2.append(pcent)\n",
    "\n",
    "        for idx, entry in enumerate(nan_count_2):\n",
    "            if entry < 0.15:\n",
    "                y.append(lab[idx])\n",
    "                data_set_filtered_2.append(data_set_filtered[idx])\n",
    "        data_set_filtered_2 = np.array(data_set_filtered_2) # turn dat shit into an array\n",
    "        y = np.array(y) # also this one gotta be an array\n",
    "\n",
    "\n",
    "    # print new dimensions of the dataframe after filtering\n",
    "\n",
    "    print('The original dimensions of the training data set was {0} samples'\n",
    "          ' and {1} columns. After feature and sample filtering, there are'\n",
    "          ' {2} samples and {3} columns'.format(data_process.shape[0],\n",
    "                                                data_process.shape[1],\n",
    "                                                data_set_filtered_2.shape[0],\n",
    "                                                data_set_filtered_2.shape[1]))\n",
    "\n",
    "    # Getting Rid of NAN and Replacing with Mean\n",
    "\n",
    "    # variable reassigned\n",
    "    data_nan = data_set_filtered_2.copy() \n",
    "    # create list with average values of columns, excluding nans\n",
    "    column_means = [] \n",
    "    for column in data_nan.T:\n",
    "        column_means.append(np.nanmean(column))\n",
    "    # variable containing locations of nan in data frame\n",
    "    inds = np.where(np.isnan(data_nan)) \n",
    "    # reassign locations of nan to the column means\n",
    "    data_nan[inds] = np.take(column_means, inds[1])\n",
    "    return (data_nan, y)\n",
    "\n",
    "def standardize(data):\n",
    "     # standardize and normalize the features\n",
    "    for idx, column in enumerate(data.T):\n",
    "        mean = np.mean(column)\n",
    "        variance = np.std(column)\n",
    "        for entry, value in enumerate(column):\n",
    "            data[entry, idx] = (value - mean)/variance\n",
    "    return data\n",
    "\n",
    "(new_data, new_labels) = process_data(data, labels)\n",
    "data_std = standardize(new_data)\n",
    "print(data_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do some regularized Logistic Regression!\n",
    "\n",
    "def reg_logreg(y, tx, initial_w, max_iters = 10000, gamma = 0.01, lam = 0.1, method = \"sgd\", batch_size = 250, writing = False):\n",
    "    \n",
    "    def reg_logreg_loss(y, tx, w, lam):\n",
    "        loss = np.mean((-y * np.log(sigmoid(tx.dot(w))) - (1 - y) * np.log(1 - sigmoid(tx.dot(w)))) + lam * np.linalg.norm(w))\n",
    "        return loss\n",
    "\n",
    "    ws = np.zeros([max_iters + 1, tx.shape[1]])\n",
    "    ws[0] = initial_w \n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    if method == \"gd\":\n",
    "        for i in range(max_iters):\n",
    "            h = sigmoid(np.dot(tx, w))\n",
    "            grad = np.dot(tx.T, (h - y)) / y.shape[0] + lam * w / y.shape[0]\n",
    "\n",
    "            w -= gamma * grad\n",
    "            ws[i+1] = w\n",
    "            losses.append(compute_loss(y, h, w, method = \"logistic\"))\n",
    "\n",
    "            if writing:\n",
    "                if i % 500 == 0:\n",
    "                    print(\"iteration: {iter} | loss : {l}\".format(\n",
    "                        iter = i, l=losses[-1] ))\n",
    "    if method == \"sgd\":\n",
    "        for i in range(max_iters):   \n",
    "            for mini_y, mini_X in batch_iter(y, tx, batch_size):                \n",
    "                h = sigmoid(np.dot(mini_X, w))\n",
    "                grad = np.dot(mini_X.T, (h - mini_y)) / mini_y.shape[0]\n",
    "                \n",
    "                w -= gamma * grad\n",
    "                \n",
    "                ws[i+1] = w\n",
    "                losses.append(reg_logreg_loss(mini_y, mini_x, w, lam))\n",
    "\n",
    "        if writing:\n",
    "            if i % 500 == 0:\n",
    "                print(\"iteration: {iter} | loss : {l}\".format(\n",
    "                    iter = i, l=losses[-1] ))\n",
    "    print(np.linalg.norm(ws[-1]))\n",
    "    return losses, ws\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.271477710425607\n"
     ]
    }
   ],
   "source": [
    "loss, ws = reg_logreg(new_labels, data_std, np.zeros(data_std.shape[1]), method = 'gd', gamma = 0.8, lam = 0.2, max_iters= 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
