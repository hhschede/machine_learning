{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation of modules and functions\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Functions\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels_raw, data_raw, ids_raw) = load_csv_data(\"data/train.csv\")\n",
    "(t_labels, t_data_raw, t_ids) = load_csv_data(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select portion of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'labels_raw_portion = labels_raw[:10000]\\ndata_raw_portion = data_raw[:10000,:]\\nids_raw_portion = ids_raw[:10000]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"labels_raw_portion = labels_raw[:10000]\n",
    "data_raw_portion = data_raw[:10000,:]\n",
    "ids_raw_portion = ids_raw[:10000]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data filtering and standardization\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dimensions of the training data set was 250000 samples and 30 columns. After feature and sample filtering, there are 250000 samples and 30 columns\n"
     ]
    }
   ],
   "source": [
    "data_, data_t_, labels = process_data(data_raw, t_data_raw, labels_raw, ids_raw)\n",
    "data, means, variance = standardize(data_)\n",
    "# need to standardize test using moments calculated from training\n",
    "data_t = standardize_test(data_t_, means, variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build data into matrix form\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tx = build_model_data(data, labels)\n",
    "y_t, tx_t = build_model_data(data_t,t_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into train and test set\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_data(tx, y)\n",
    "\n",
    "print('Standardized and randomized samples are found as the variables X_train, y_train, X_test, y_test. Values' +\n",
    "      ' are split for testing and training sets with the ratio of 0.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression with gradient descent (GD)\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_w = np.ones(X_train.shape[1])\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "losses, ws = least_squares_GD(y_train, X_train, initial_w, max_iters = 250) # fit model and retrieve W's across iterations\n",
    "test_losses = list(map(lambda x: compute_loss(y_test, X_test, x, method = 'MSE'), ws)) # retrieve losses using test set with ws\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(losses, label='Training set loss', c='blue')\n",
    "plt.plot(test_losses, label='Test set loss', c='red')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Cost Function Loss Over Iterations of Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make plot with label prediction accuracy\n",
    "\n",
    "pred_ytrain = list(map(lambda x: predict_labels(x, X_train), ws)) # Training prediction\n",
    "pred_accuracytrain = list(map(lambda x: pred_accuracy(x, y_train), pred_ytrain))\n",
    "pred_ytest = list(map(lambda x: predict_labels(x, X_test), ws)) # Test prediction\n",
    "pred_accuracytest = list(map(lambda x: pred_accuracy(x, y_test), pred_ytest))\n",
    "\n",
    "\n",
    "plt.plot(pred_accuracytrain, label='Training set prediction accuracy', c='blue')\n",
    "plt.plot(pred_accuracytest, label='Test set prediction accuracy', c='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Prediction Accuracy')\n",
    "plt.title('Prediction Accuracy Over Iterations of Gradient Descent')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression with stochastic gradient descent (SGD)\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_w = np.ones(X_train.shape[1])\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "losses, ws = least_squares_SGD(y_train, X_train, initial_w, max_iters = 250, tol = 1e-4, patience = 5) # fit model, retrieve parameters ws\n",
    "test_losses = list(map(lambda x: compute_loss(y_test, X_test, x, method = 'MSE'), ws)) # retrieve losses using test set with ws\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(losses, label='Training set loss', c='blue')\n",
    "plt.plot(test_losses, label='Test set loss', c='red')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Cost Function Loss Over Iterations of Stochastic Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make plot with label prediction accuracy\n",
    "\n",
    "pred_ytrain = list(map(lambda x: predict_labels(x, X_train), ws)) # Training prediction\n",
    "pred_accuracytrain = list(map(lambda x: pred_accuracy(x, y_train), pred_ytrain))\n",
    "pred_ytest = list(map(lambda x: predict_labels(x, X_test), ws)) # Test prediction\n",
    "pred_accuracytest = list(map(lambda x: pred_accuracy(x, y_test), pred_ytest))\n",
    "\n",
    "\n",
    "plt.plot(pred_accuracytrain, label='Training set prediction accuracy', c='blue')\n",
    "plt.plot(pred_accuracytest, label='Test set prediction accuracy', c='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Prediction Accuracy')\n",
    "plt.title('Prediction Accuracy Over Iterations of Stochastic Gradient Descent')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression using least squares normal equations\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = least_squares(y_train, X_train)\n",
    "\n",
    "pred_y = predict_labels(w, X_test)\n",
    "pred_accuracy(pred_y,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression using the normal equations with additional polynomial degrees\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1,15)\n",
    "rmse_tr = np.zeros(len(degrees))\n",
    "rmse_ts = np.zeros(len(degrees))\n",
    "pred_tr = np.zeros(len(degrees))\n",
    "pred_ts = np.zeros(len(degrees))\n",
    "\n",
    "\n",
    "for ind, degree in enumerate(degrees):\n",
    "    #train the model\n",
    "    X_test_poly = build_poly(X_test,degree)\n",
    "    X_train_poly = build_poly(X_train,degree)\n",
    "    w = least_squares(y_train, X_train_poly)\n",
    "    rmse_tr[ind] = (np.sqrt(2 * compute_loss(y_train, X_train_poly, w)))\n",
    "    pred_tr[ind] = pred_accuracy(predict_labels(w,X_train_poly),y_train)\n",
    "    \n",
    "    #test the model\n",
    "    \n",
    "    rmse_ts[ind] = (np.sqrt(2 * compute_loss(y_test, X_test_poly, w)))\n",
    "    pred_ts[ind] = (pred_accuracy(predict_labels(w, X_test_poly),y_test))\n",
    "    \n",
    "    # print the update\n",
    "    print(\"degree: {d} \\t rmse_ts: {a} \\t  pred_ts: {b}\".format(d = degree, a = rmse_ts[ind], b = pred_ts[ind]))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# plot the loss\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(degrees,rmse_tr, c='blue')\n",
    "plt.plot(degrees,rmse_ts, c='red')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['training set', 'testing set'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#plot the accuracy\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(degrees,pred_tr, c='blue')\n",
    "plt.plot(degrees,pred_ts, c='red')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['training set', 'testing set'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression using ridge regression\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-5, 0, 15)\n",
    "rmse_tr = []\n",
    "rmse_ts = []\n",
    "pred_tr = []\n",
    "pred_ts = []\n",
    "\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    \n",
    "    w = ridge_regression(y_train, X_train, lambda_)\n",
    "    rmse_tr.append(np.sqrt(2 * compute_loss(y_train, X_train, w)))\n",
    "    pred_tr.append(pred_accuracy(predict_labels(w, X_train),y_train))\n",
    "    \n",
    "    \n",
    "    rmse_ts.append(np.sqrt(2 * compute_loss(y_test, X_test, w)))\n",
    "    pred_ts.append(pred_accuracy(predict_labels(w, X_test),y_test))\n",
    "    \n",
    "plt.style.use('seaborn')\n",
    "plt.semilogx(lambdas,rmse_tr, c='blue')\n",
    "plt.semilogx(lambdas,rmse_ts, c='red')\n",
    "plt.legend(['training set', 'testing set'], loc='upper left')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.semilogx(lambdas,pred_tr, c='blue')\n",
    "plt.semilogx(lambdas,pred_ts, c='red')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['training set', 'testing set'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# print(np.max(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "losses, ws = logistic_regression(y_train, X_train, initial_w, method = 'gd', max_iters = 6000) # fit model, retrieve parameters ws\n",
    "test_losses = list(map(lambda x: compute_loss(y_test, X_test, x, method = 'MSE'), ws)) # retrieve losses using test set with ws\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(losses, label='Training set loss', c='blue')\n",
    "plt.plot(test_losses, label='Test set loss', c='red')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Logistic Regression: Cost Function Loss Over Iterations of Stochastic Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make plot with label prediction accuracy\n",
    "\n",
    "pred_ytrain = list(map(lambda x: predict_labels(x, X_train), ws)) # Training prediction\n",
    "pred_accuracytrain = list(map(lambda x: pred_accuracy(x, y_train), pred_ytrain))\n",
    "pred_ytest = list(map(lambda x: predict_labels(x, X_test), ws)) # Test prediction\n",
    "pred_accuracytest = list(map(lambda x: pred_accuracy(x, y_test), pred_ytest))\n",
    "\n",
    "\n",
    "plt.plot(pred_accuracytrain, label='Training set prediction accuracy', c='blue')\n",
    "plt.plot(pred_accuracytest, label='Test set prediction accuracy', c='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Prediction Accuracy')\n",
    "plt.title('Logistic Regression Prediction Accuracy Over Iterations of Gradient Descent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression using Newton's method\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_w = np.random.rand(X_train.shape[1])\n",
    "# losses, ws = logistic_hessian(y_train, X_train, initial_w) # fit model, retrieve parameters ws\n",
    "# test_losses = list(map(lambda x: compute_loss(y_test, X_test, x, method = 'MSE'), ws)) # retrieve losses using test set with ws\n",
    "\n",
    "# plt.style.use('seaborn')\n",
    "# plt.plot(losses, label='Training set loss', c='blue')\n",
    "# plt.plot(test_losses, label='Test set loss', c='red')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Logistic Regression: Cost Function Loss Over Iterations of Stochastic Gradient Descent')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Make plot with label prediction accuracy\n",
    "\n",
    "# pred_ytrain = list(map(lambda x: predict_labels(x, X_train), ws)) # Training prediction\n",
    "# pred_accuracytrain = list(map(lambda x: pred_accuracy(x, y_train), pred_ytrain))\n",
    "# pred_ytest = list(map(lambda x: predict_labels(x, X_test), ws)) # Test prediction\n",
    "# pred_accuracytest = list(map(lambda x: pred_accuracy(x, y_test), pred_ytest))\n",
    "\n",
    "\n",
    "# plt.plot(pred_accuracytrain, label='Training set prediction accuracy', c='blue')\n",
    "# plt.plot(pred_accuracytest, label='Test set prediction accuracy', c='red')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Prediction Accuracy')\n",
    "# plt.title('Logistic Regression Prediction Accuracy Over Iterations of Gradient Descent')\n",
    "# plt.show()\n",
    "\n",
    "\"\"\"IF YOU READ THIS: I tried doing something here so that LR is faster and more accurate,\n",
    "the problem is that we can't calculate things that way, it's too big, but maybe I'll find a way\n",
    "another time.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-5, 0, 5)\n",
    "loss_tr = []\n",
    "loss_ts = []\n",
    "pred_tr = []\n",
    "pred_ts = []\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "\n",
    "# There is a runtime warning but just be patient\n",
    "\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    \n",
    "    losses, ws = reg_logistic_regression(y_train, X_train, initial_w, lamb = lambda_, methods ='gd', gamma = 0.01, max_iters=6000)\n",
    "    loss_tr.append(losses[-1])\n",
    "    best_w = ws[-1]\n",
    "    \n",
    "    pred_tr.append(pred_accuracy(predict_labels(best_w, X_train),y_train))\n",
    "    \n",
    "    test_losses = compute_loss(y_test, X_test, best_w, lam = lambda_, method = 'reg_logistic') # retrieve losses using test set with ws\n",
    "    loss_ts.append(test_losses)\n",
    "    pred_ts.append(pred_accuracy(predict_labels(best_w, X_test),y_test))\n",
    "\n",
    "    \n",
    "plt.style.use('seaborn')\n",
    "plt.semilogx(lambdas,loss_tr, c='blue')\n",
    "plt.semilogx(lambdas,loss_ts, c='red')\n",
    "plt.legend(['training set', 'testing set'], loc='upper left')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.semilogx(lambdas,pred_tr, c='blue')\n",
    "plt.semilogx(lambdas,pred_ts, c='red')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['training set', 'testing set'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lamb = 0.1\n",
    "# losses, ws = reg_logistic_regression(y_train, X_train, \n",
    "#                          initial_w, lamb, methods = 'gd', \n",
    "#                          gamma = 0.01, max_iters= 6000)\n",
    "\n",
    "# test_losses = list(map(lambda x: compute_loss(y_test, X_test, x, lam = lamb, method = 'reg_logistic'), ws)) # retrieve losses using test set with ws\n",
    "# plt.style.use('seaborn')\n",
    "# plt.plot(losses, label='Training set loss', c='blue')\n",
    "# plt.plot(test_losses, label='Test set loss', c='red')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Regularized Logistic Regression: Cost Function Loss Over Iterations of Gradient Descent')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Make plot with label prediction accuracy\n",
    "\n",
    "# pred_ytrain = list(map(lambda x: predict_labels(x, X_train), ws)) # Training prediction\n",
    "# pred_accuracytrain = list(map(lambda x: pred_accuracy(x, y_train), pred_ytrain))\n",
    "# pred_ytest = list(map(lambda x: predict_labels(x, X_test), ws)) # Test prediction\n",
    "# pred_accuracytest = list(map(lambda x: pred_accuracy(x, y_test), pred_ytest))\n",
    "\n",
    "\n",
    "# plt.plot(pred_accuracytrain, label='Training set prediction accuracy', c='blue')\n",
    "# plt.plot(pred_accuracytest, label='Test set prediction accuracy', c='red')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Prediction Accuracy')\n",
    "# plt.title('Regularized Logistic Regression Prediction Accuracy Over Iterations of Gradient Descent')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the polynomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 12\n",
    "X_test_poly = build_poly(data_test,degree)\n",
    "X_train_poly = build_poly(data,degree)\n",
    "w = least_squares(labels, X_train_poly)\n",
    "rmse_tr = (np.sqrt(2 * compute_loss(labels, X_train_poly, w)))\n",
    "pred_tr = pred_accuracy(predict_labels(w,X_train_poly),labels)\n",
    "\n",
    "test_prediction = predict_labels(w, X_test_poly)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"prediction.csv\"\n",
    "ids = ids_test\n",
    "y_pred = test_prediction\n",
    "create_csv_submission(ids, y_pred, name)\n",
    "\n",
    "# this yielded like 0.62 on kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.ones(tx.shape[1])\n",
    "y[y == -1] = 0\n",
    "losses, ws = logistic_regression(y, tx, initial_w, method = 'gd', max_iters = 6000) # fit model, retrieve parameters ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tx_t.dot(ws[-1])\n",
    "pred[np.where(pred <= 0.5)] = -1\n",
    "pred[np.where(pred > 0.5)] = 1\n",
    "\n",
    "name = \"prediction.csv\"\n",
    "create_csv_submission(t_ids, pred, name)\n",
    "\n",
    "# this yields 0.73 on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
