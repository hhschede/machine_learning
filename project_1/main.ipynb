{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation of modules and functions\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Modules\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "warnings.simplefilter(\"error\")\n",
    "\n",
    "# Functions\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels_raw, data_raw, ids_raw) = load_csv_data(\"data/train.csv\")\n",
    "(t_labels, t_data_raw, t_ids) = load_csv_data(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data filtering and standardization\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dimensions of the training data set was 250000 samples and 30 columns\n",
      " After feature and sample filtering, there are 250000 samples and 23 columns\n"
     ]
    }
   ],
   "source": [
    "data_, data_t_, labels = process_data(data_raw, t_data_raw, labels_raw, ids_raw,sample_filtering = False, feature_filtering = True, replace = 'mean')\n",
    "\n",
    "# Should not standardize all this stuff in the beginning! Only at the very end after the model has been selected!!\n",
    "# Check down two inputs, where the sets are standardized!\n",
    "\n",
    "# data, means, variance = standardize(data_)\n",
    "# need to standardize test using moments calculated from training\n",
    "# data_t = standardize_test(data_t_, means, variance)\n",
    "\n",
    "data = data_.copy()\n",
    "data_t = data_t_.copy()\n",
    "\n",
    "# perform PCA\n",
    "\n",
    "# eigVal, eigVec, sumEigVal = PCA(data, threshold = 0.7)\n",
    "# data = data.dot(eigVec)\n",
    "# data_t = data_t.dot(eigVec)\n",
    "# print(\"we have reduce the number of feature with PCA to {0}\".format(eigVec.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build data into matrix form\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tx = build_model_data(data, labels)\n",
    "y_t, tx_t = build_model_data(data_t,t_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into train and test set\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized and randomized samples are found as the variables X_train, y_train, X_test, y_test. Values are split for testing and training sets with the ratio of 0.8\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = split_data(tx, y)\n",
    "\n",
    "# Create the standardized X_train for use of testing the model on the test set. Note that this should\n",
    "# Not be done before crossvalidation, as the crossvalidation function standardizes the sets within the loops\n",
    "\n",
    "X_train_standardized, means, variance = standardize(X_train)\n",
    "X_test_standardized = standardize_test(X_test, means, variance)\n",
    "\n",
    "print('Standardized and randomized samples are found as the variables X_train, y_train, X_test, y_test. Values' +\n",
    "      ' are split for testing and training sets with the ratio of 0.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression with gradient descent (GD)\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAFlCAYAAADh+TGmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3Xl8FPX9x/HXzB65QxIIKpeKShU8EW9A6SFFxBMFsR61rcrPC7WKF4fIIaJRK6JVW21ViqCtR6s9QBEBBcUDRRBFuQMEkkDOPWa+vz82WUACZLObhCXv5+MRkp397sx3PhDe853TMsYYREREZL9nN3cHREREpGko9EVERFoIhb6IiEgLodAXERFpIRT6IiIiLYRCX0REpIVQ6EtCjB07lvPPP5/zzz+fo48+mr59+0ZfV1dXxzw/YwxXX30127Ztq/P9GTNmcNFFF3HeeefRv39/7rvvPsrLy/c631mzZjF58uQ63+vduzdLly6Nua/xCIVCPPnkk5xzzjmce+65nHvuuTzwwANs3bq10Zb56KOPMm7cOAA+//xzRo8endD5T5s2jWnTpgHw8ssv89xzzyV0/rFYt24d/fv35/zzz2fx4sW7vL9s2TJuuukmzj77bPr160f//v35y1/+kpBlH3PMMWzYsIEvvviCYcOGNXg+e/pdmDFjBieeeGL0d+3cc8/l+uuv5+uvv46n6wm1p985aXre5u6A7B/uu+++6M8//elPefjhhznmmGMaPD/Hcfjwww/rfO+zzz7jmWee4dVXX6VVq1aEw2FGjRrFmDFjeOihh/Y438WLF1NZWdngfiXaLbfcQkZGBn/7299o1aoVwWCQP//5z1x22WXMmDGDjIyMRl3+t99+y6ZNmxI6z08++YSjjz4agMsvvzyh847Vhx9+yIEHHsif/vSnXd5bsmQJv/vd7xg7diw//elPASguLmbo0KFYlsWVV16ZkD4cd9xxPPbYYw3+/J5+FwBOOeUUpkyZEn39wQcfcM011/CPf/yDgw46qMHLTZR97XeupVPoS5P49ttvGTduHNu2bcNxHK6++mouvPBCysvLufvuu1m9ejW2bXPMMcdw//33c/fddwOR0Hjuuec44IADovMqKirCdV2qq6tp1aoVXq+XW2+9le+//x6IjIymTJnCzJkzcV2Xjh07MmrUKNasWcOrr76K4zhkZmZyyy231Lv/TzzxBO+88w4ej4fOnTszYsQI2rRpwzvvvMMf//hHPB4PHo+H4cOHc+KJJ+52+o4++eQTli5dysyZM/F4PAD4/X6uv/56Pv30U2bMmIHP52P+/Pk8+eSTACxfvpxrr72Wd999lxUrVtRZ0/nz5/PQQw+RkpJCdXU1M2bMwO/377JOa9eu5cknn6SsrIx7772XcePGMXPmTJ5++mnC4TBpaWncddddHHfccTz66KN89dVXbNq0ia5du3L77bczcuRISkpKKCoqon379jz++OMsXLiQOXPmsGDBAlJSUtiwYQOVlZXce++9fPPNN4wdO5bS0lIsy+K3v/0t5513HvPnz2fy5Mm0a9eO7777DsdxGDNmDCeccAILFy5k4sSJ1N5D7P/+7//4+c9/vsu6TJ06lalTp2LbNvn5+YwcOZJ169YxefJkysrKuPrqq3nhhRd2+syjjz7KtddeGw18gLy8PO6//35WrFgRbVOf9c7Ly2PBggWMGzcO27Y59thjo32eP38+EydO5I033iAYDPLQQw+xaNEiHMehW7du3HvvvWRmZtK7d28uvfRS5s+fT2FhIRdddBE33XTTHn8X6tKrVy/69OnDtGnTuPXWWyksLGTMmDFs3LiRUCjEgAEDuPbaawmFQowZM4bPP/8cn89Hp06dmDBhAmlpacyaNYvHH38cYwwZGRmMGTOGLl268Mknn/DII49QXV2NbdvcfPPNnHnmmcyYMYPZs2fjui5r1qwhLS2NiRMnUlpa2uDfOWkkRiTB+vTpYxYvXhx9HQwGTb9+/czSpUuNMcZs3brV9O3b1yxevNi8+uqr5tprrzXGGBMKhczdd99tVq9ebUKhkOnSpYvZunXrLvMPBAJm2LBh5qijjjIXXnihGTNmjJk9e3b0/RkzZpjbbrvNhEIhY4wxL730krnuuuuMMcYUFBSYsWPH1tnvXr16ma+//nqX6a+88oq57LLLTGVlZXQetX0+66yzous6e/Zs89RTT+1x+o7++Mc/mltvvbXOvjz//PPmxhtvNNu2bTMnnnii2bJlizHGmAkTJpg//OEPe6zpvHnzzFFHHWUKCwvrnPeONZg+fboZOnSoMcaY7777zgwYMMCUlpYaY4xZunSpOeOMM0x1dbUpKCgw55xzjgmHw8YYY/70pz+Z5557zhhjjOM45te//rV54YUXjDHG3H777eb555/faVnBYND06dPHzJw50xhjTGFhoTnjjDPMF198YebNm2e6du1qli1bFq3LlVdeaYwx5vLLLzfvvPOOMcaYJUuWmAceeGCX9fnggw/M2WefHa3R9OnTzbnnnrvL+v3Y8ccfb7777rs639uxVvVZ70AgYE499VTz0UcfGWOM+cc//mG6dOliCgsLzbx588x5551njDHmscceM5MmTTKu6xpjjJk4cWJ0nXr16mUmTZpkjDFm/fr1plu3bmb9+vV7/F3Y3fq98MIL5vrrrzfGGDNkyJDo70dVVZW5/PLLzX/+8x/z4Ycfmv79+0c/8+CDD5rPP//cbNiwwZx44onRf1tvv/22ue6660xxcbE5++yzzbp164wxkb/DXr16mcLCQjN9+nRz0kknmQ0bNhhjjBk5cqS5++67ozXc3e+cND2N9KXRrVixgjVr1jB8+PDotGAwyNKlSzn11FN5/PHHufLKKzn99NP5zW9+Q8eOHQmHw7udn9/v59FHH2Xjxo189NFHfPzxx9x555307NmTRx55hNmzZ/P1119z8cUXA+C6LsFgsMH9nzNnDhdffDFpaWkAXHXVVfTu3ZtwOMw555zD0KFDOeusszj99NO55pprAHY7/cd2t57BYBDLssjKyuJnP/sZb775Jr/61a/45z//yfTp0/dY0w4dOtC+fXsOPPDAmNZz3rx5bNy4cafd2pZlsXr1agCOP/746B6Ja665ho8//pjnn3+elStXsmLFCk466aTdznvFihUYY/jZz34GwIEHHsgvfvELPvjgA0444QQ6dOjAT37yEwC6devG22+/DUC/fv0YNWoUM2fO5PTTT6/z2PgHH3xA//79ycvLA+CSSy5h/PjxFBYW7nF9zY/uQD527Fg+/vjj6L+X//znP/Ve76VLl5KWlsYpp5wCwAUXXFDnuRKzZ8+msrKSDz74AIic09G2bdvo+7X1Oeigg8jNzWXr1q3k5+fvcT3qYlkWaWlplJeX8+mnn1JQUEBBQQEAlZWVLF26lKuuugrHcbjkkkvo2bMn/fr149hjj+Xtt9/mqKOO4sgjjwQifwf9+vVj1qxZFBUVMXTo0OhybNtm+fLlQOQchtq9EN26deP999+Pud/S+BT60uhc1yUnJ4c33ngjOq2oqIjs7GxSUlL43//+x4IFC/joo4+46qqrGDduHGecccZu5zdjxgzy8/M566yzoicwXXfddfz85z9n1KhROI7D9ddfz6WXXgpAIBDY7QmB9eE4DpZl7bQ+tWF9xx13cOmllzJv3jxee+01/vrXvzJt2rTdTt9R9+7deemllwgEAqSkpOz03oIFC+jZsycQCbFx48bRoUMHjjzySNq1a0dpaelua7po0SLS09NjXk/XdaMbTrUKCwuj/5HvOM8HH3yQZcuWceGFF3LKKacQCAR2CdEfz3vHGtZOq61jampqdLplWdF5XX755fz85z9n7ty5zJkzh8mTJ/Pvf/97p8MVP/77McZgjNnjhiMQPXxw2GGHAdvPS1m1alV0gzGe9a7dUNiR4ziMHDky+u+7vLycUCgUfX93dYjVl19+SZcuXXAcB2CnQzzFxcWkpqaSnp7OW2+9xaeffspHH33EsGHDuO6668jNzd1pXq7rsnz5clzXpUuXLjv9O964cSN5eXm8/vrrO/0bjqfv0rh09r40usMPPxzbtvnXv/4FRM6oPvfcc1m2bBkvvvgiI0aMoFevXtx5552ceuqpfP3113g8HizL2u1/3JMmTWLjxo3R199++y2dOnUiMzOTnj17Mn369OjZ/I8++mj0uKjX691rGPxYr169ePXVV6mqqgLgxRdf5JRTTsEYQ58+fQiHwwwZMoQRI0awdOlSQqFQndN/vNwePXpw/PHHc/fdd0c3SsLhMJMnT2bt2rVccskl0XaBQICnn346uiGzp5rGwuPxRPt16qmn8sEHH/DDDz8AkbOuL7jgAgKBwC6fmzt3LldffTXnn38+ubm5fPjhh7iuC9Rd48MPPxzXdZk1axYAGzZsiI7e92TgwIEsX76ciy++mAceeICSkhKKi4t3atOrVy/++c9/UlJSAmzfKOzQocMe53377bfz1FNPMWfOnOi06upq/ve//9UZ2Hta76OOOopgMMjcuXMB+O9//1vn1SQ9e/bkxRdfJBQK4TgO99xzz15P8tvb78KPvfvuu8ydO5dLL72UVq1a0a1bt+j5DFu3bmXQoEHMnj2bmTNn8pvf/Ibu3btz8803M2DAAL788kuOP/54li9fHj2v4b///S933303J5xwAitWrGDRokVA5ETIvn37smXLlj32pyG/c9J4NNKXRuf3+3nqqacYP3589CSx22+/neOOO47DDjuMjz/+mP79+5Oamkr79u25/PLLsSyLs88+m8suu4wpU6ZER2MQGfkGAgF++9vfRnfbd+7cmeeeew7btrnsssvYtGkTgwYNAqBdu3aMHz8egNNOO4077rgDr9fLvffeu0tfBw8ejG1v3xa+6667GDx4MBs3bmTgwIE4jsOhhx7KQw89hM/n46677mLYsGF4vV4sy2LChAm7ne717vrrVlBQwJ///Gcuv/xyjDGEQiFOO+00pk2bRmZm5k7r/Oyzz9KnT5+91nT+/Pn1/rs54YQTeOqpp7j55pv5wx/+wOjRoxk2bBjGGLxeL1OmTIke1tjRDTfcwLhx43jkkUfw+Xz06NGDVatWAZFLHydNmrRTe7/fz5QpUxg3bhyPPfYYrutyyy23cNJJJ+2xv8OHD2f8+PE88sgjWJbFsGHDdjlsceaZZ7Jy5UquuOIKjDHk5eXx9NNP77Jn4ceOPvponn/+eZ588kkmTZqE4zgEg0FOO+00pk6dWudndrfefr+fJ598ktGjRzNp0iS6detGTk7OLp+/6aabmDhxIhdccEH0RL4777xzj/3c0+8CRPYKnX/++dG2BxxwAM8//3z0cMejjz7KmDFjeOuttwgGg1xwwQWcc845hMNh5syZw7nnnkt6ejo5OTmMHTuWtm3b8tBDD3HHHXfgOA5ZWVk8/PDDtGnThj/84Q9MmDCBYDCIMYaHH354r4eR9vY7J03LMtoHIyIi0iJo976IiEgLodAXERFpIRT6IiIiLYRCX0REpIVQ6IuIiLQQ+/0le0VFZQmdX25uOiUlenhEvFTH+KmGiaE6xk81TIxE1TE/P2u372mkHyOvt+6bdkhsVMf4qYaJoTrGTzVMjKaoo0JfRESkhVDoi4iItBAKfRERkRZCoS8iItJCKPRFRERaCIW+iIhIC6HQFxERaSEU+iIiIg30xBOPcuON1zJkyMVcdFF/brzxWu67b/gu7UaNuptQKLTb+Zx3Xt86p1dXVzN06DWsWrUyIf3d7+/IJyIi0lhuuulWAN5++y1WrVrJ0KE31dnu/vsnxDzvZcu+ZtKkCRQVbYqrjztq1ND/4osvePjhh3nxxRdZunQpDzzwAB6PB7/fz8SJE2nTpg3Tp09n2rRpeL1ehg4dSp8+fSguLub3v/891dXVtG3blgkTJpCWllZnWxEREYCM0feR8tbrCZ1nYMAFVIweG/PnPv30E5566gl8Ph/nnXchzz33NC+//Crr1q3hiScexXUN5eVlDBv2e4455rg65xEMBhk/fhIPPDAy3tWIarTQf/bZZ3nzzTdJS0sDYNy4cYwYMYKjjjqKadOm8eyzz/Lb3/6WF198kddee41AIMCQIUM444wzmDJlCueeey4XXXQRzzzzDK+88gr9+/evs63f72+sVRAREWmwYDDIs8/+BYDnnnsagB9++J4bb7yVww47nP/+99+8/fZbuw39Y489PuF9arTQ79SpE0888QR33nknAAUFBbRt2xYAx3FISUlh8eLFnHDCCfj9fvx+P506dWLZsmUsWrSI6667DoDevXtTUFBAx44d62x77LHHNtYq1C0cbtrliYhIvVSMHtugUXlj6dTp4F2mtWnTlhdeeI6UlBQqKyvJyMho0j412ol8ffv2xevdvk1RG/iffvopL730EldffTXl5eVkZW1/GlBGRgbl5eU7Tc/IyKCsrGy3bZvc5g1Nv0wREUk6tm3tMu3xxyfxm99cx3333c9hhx2OMaZJ+9SkJ/K9/fbbPPXUUzzzzDPk5eWRmZlJRUVF9P2KigqysrKi01NTU6moqCA7O3u3bfcmNzc9oU8uMoWle3xsodSf6hg/1TAxVMf4tfQaZmWlkp7uj9YhJyedlBRf9LXHY5Ofn8VFF13IfffdQevWrTnwwAPZtq2E/Pys6AZCXXX0+73k5qYnpMaWacTNjLVr13Lbbbcxffp03njjDV555RWmTJlCTk4OAEVFRVxzzTW8+uqrBINBLrnkEt544w0eeughunXrFj2mD3DhhRfW2TYlJWWPfSgqKkvoOrUJl7LZm5PQebZE+flZCf+7aWlUw8RQHeOnGiZGouq4p42DJhnpO47DuHHjOOigg7jppsjlDCeddBI333wzV1xxBUOGDMEYw6233kpKSgpDhw5l+PDhTJ8+ndzcXB555BHS09PrbNvUmnZHjIiISOI06kh/X5Dorc/W4VK2aKQfN40M4qcaJobqGD/VMDGaYqSvO/LFyGisLyIiSUqhLyIi0kIo9GPw929n8It/XkbI2f39k0VERPZVCv0YvLt6Ju+tn8+GysLm7oqIiEjM9MCdGNhWZBvJcZ1m7omIiOwLnnjiUb75ZinFxVuorq6mXbv25OTkMnbsxJ3ajRp1N/fdNwafz1fnfM47ry8ffjh/p2n/+9+/mT79b3g8Hg477HBuv/0ubDu+sbpCPwYeK3KTH9co9EVEpPGeshcIVPPss0/x17++QmpqKqNG3cP8+R/Qs+eZcfVXoR8Duyb0NdIXEdn3jJ5/H2+tSOxT9gYcdgGjT2/6p+z5fH6efvrPpKamApH73fj98d+bRqEfA0/N7n3XuM3cExER2dfF85Q927bJy2sNwKuvTqOqqoqTTjol7j4p9GOw/Zi+nrQnIrKvGX362AaNyhtLvE/Zc12XKVP+wJo1qxg37iEsa9cH+MRKZ+/HoPaYvqNj+iIishfxPmVv0qTxBIMBJkx4JLqbP14a6cfAv9rPqR+einuRQl9ERGJ39tn9uOuu28nLyyM/vy1bt5bW2e6bb5bxz3++wXHHncDNN18PwCWXXMaZZ/aJa/m6934MHrlsAmmzUuj+35M59fieCZtvS6R7dcdPNUwM1TF+qmFi6N77+xi7JLJF5lRUNXNPREREYqfQj4FdVQGAW7KlmXsiIiISO4V+DGpPnHRCOntfRESSj0I/BjVX7OGGdCKfiIgkH4V+DGpD3wnrKXsiIpJ8FPox0EhfRESSma7Tj0Hptg7kAmGFvoiI0LhP2Zs9exYvvfQXLAvOO+8iBgy4IO7+KvRjsLm0E7lsoGSLv7m7IiIi+4DGesqe4zg8/fRknnvuRdLS0vjVry6hV6+zyMnJiau/Cv1Y1J69r5P3RUT2OaNHp/DWW4mNtQEDwoweHYj5c/E+Zc/j8fDSSzPwer2UlBRjDKSlpcW9PjqmH4PaS/ZCumRPRET2IhgMMmXKc/zyl/2j02qfsvf441MYNOhy3n77rd1+3uv18v7773L11Zdx/PEn4PXGv0GjkX4soify7dd3LhYRSUqjRwcaNCpvLPE+ZQ/gzDN/Sq9eZzFu3Gj+/e9/0b//eXH1SSP9GNSO9MNBt3k7IiIi+7x4nrJXUVHOjTdeSzAYxLZt0tLSsO34I1sj/VhEr9NX6IuISOzq+5S9jIxMfvGLX3LDDb/D6/Vy2GFHcPbZ/eJevp6yF4PfnzCVzus2ELzUZtjkYQmbb0ukp3LFTzVMDNUxfqphYugpe/uY6M15wvv1dpKIiOynFPoxsGqOzyj0RUQkGSn0YxG9Tl/H9EVEJPko9GNQeyKm62ikLyIiyUehHwPLE/nu6tb7IiKShBT6Mdh+Il/z9kNERKQhdJ1+DKIn8mn3voiI0LhP2as1ceI4srOzd/swn1go9GMQHelr976IiNB4T9mr9frrr/H9999x/PHdG9zHHSn0Y2B7akf6zdwRERHZxfzR77PirW8TOs/DBhzB6aPPjPlz8T5lD+Crrxbz9ddfcf75F7Fq1co41yRCx/RjUHvbY6PQFxGRvYjnKXubN2/mz39+httuG57QPmmkH4PtI30d0xcR2decPvrMBo3KG0s8T9l7772ZlJaW8vvf3xw9X+Dggw/hnHMGxNUnhX4MrJrQN+6uT04SERHZ0e6esjdy5FgOOeRQ/vSnP1JYuL7Oz15yyWAuuWQwsP18gXgDHxp59/4XX3zBFVdcAcCqVau47LLLGDJkCKNGjcJ1I3e1mzx5MgMHDmTw4MEsXrw45rZNyY6Gvkb6IiISu9qn7P3f//2W1atXsXlzUZMuv9FG+s8++yxvvvkmaWlpAEyYMIFhw4ZxyimnMHLkSGbNmkW7du1YuHAhM2bMoLCwkJtuuonXXnstprZNydZIX0RE6vDjUXj37j3o3r1H9PWrr0aO3Q8e/CsGD/7VLp9/883/1Hve8Wi0kX6nTp144oknoq+XLFnCySefDEDv3r2ZP38+ixYtomfPnliWRbt27XAch+Li4pjaNqVo6OtEPhERSUKNNtLv27cva9eujb42xmBZkdDMyMigrKyM8vJycnJyom1qp8fSNi8vb4/9yM1Nx+v1JGSdUlNqy2Xv8XnFUj+qYfxUw8RQHeOnGiZGY9exyU7ks+3tOxUqKirIzs4mMzOTioqKnaZnZWXF1HZvSkoqE7QG4BgXH5Gz94uKyhI235YoPz9LNYyTapgYqmP8VMPESFQd97Th0GTX6Xft2pUFCxYAMGfOHHr06EH37t2ZO3curuuyfv16XNclLy8vprZNyeOJlEvH9EVEJBk12Uh/+PDhjBgxgoKCAjp37kzfvn3xeDz06NGDQYMG4bouI0eOjLltU7Jrq+U2+aJFRETiZhlj9uvrzxK5y2nK7/4Jbyznu0MMBQtvT9h8WyLtDoyfapgYqmP8VMPE2K927+8PPN7aJ+5o976IiCQfhX4MvLXH9PfrfSMiIrK/UujHwOPXSF9ERJKXQj8Gtqf2MXvN2w8REZGGUOjHwOurucmP0UhfRESSj0I/Bt7aE/kU+iIikoQU+jHw+GtG+jqmLyIiSUihH4PtI/3m7YeIiEhDKPRj4PXX3JJPu/dFRCQJKfRj4KvZvW8p9EVEJAkp9GOwfaTfvP0QERFpCIV+DLw+nb0vIiLJS6EfA1+KD9DufRERSU4K/Rj4ojfnad5+iIiINIRCPwbe6Ei/mTsiIiLSAAr9GPhSak7kQ7v3RUQk+Sj0Y+CtCX3LbeaOiIiINIBCPwbelJrr9DXSFxGRJKTQj4GndqSvY/oiIpKEFPoxiI70FfoiIpKEFPoxiD5lT7v3RUQkCSn0Y+DRSF9ERJKYQj8GXh3TFxGRJKbQj4HHb+NiKfRFRCQpKfRj4PHZuNg6oi8iIklJoR8DywKjkb6IiCQphX4MPB400hcRkaSl0I+Bx1M70tdQX0REko9CPwbR0NdYX0REkpBCPwa2XbN7XyN9ERFJQgr9GBksFU1ERJKS8itGro7pi4hIklLox8joiL6IiCQphX6MXCxsjfRFRCQJKfRjpJG+iIgkK4V+jIylkb6IiCQnhX6MDGikLyIiScnblAsLhULcddddrFu3Dtu2eeCBB/B6vdx1111YlsURRxzBqFGjsG2byZMnM3v2bLxeL/fccw/HHnssq1atqrNtU3KxsNBIX0REkk+TJub7779POBxm2rRp3HDDDTz22GNMmDCBYcOGMXXqVIwxzJo1iyVLlrBw4UJmzJhBQUEB999/P0CdbZuasSxshb6IiCShJg39Qw89FMdxcF2X8vJyvF4vS5Ys4eSTTwagd+/ezJ8/n0WLFtGzZ08sy6Jdu3Y4jkNxcXGdbZta7b33jY7ri4hIkmnS3fvp6emsW7eOfv36UVJSwtNPP83HH3+MZUWOkmdkZFBWVkZ5eTk5OTnRz9VON8bs0nZvcnPT8Xo9CVsHA9gY8tqk47WbtHz7nfz8rObuQtJTDRNDdYyfapgYjV3HJk2tF154gZ49e3L77bdTWFjIVVddRSgUir5fUVFBdnY2mZmZVFRU7DQ9Kytrp+P3tW33pqSkMqHrYKzIMf0Nm0pJ8aQkdN4tSX5+FkVFe99ok91TDRNDdYyfapgYiarjnjYcmnT3fnZ2NllZkc60atWKcDhM165dWbBgAQBz5syhR48edO/enblz5+K6LuvXr8d1XfLy8ups29RcIiN9x3WafNkiIiLxaNKR/tVXX80999zDkCFDCIVC3HrrrRx99NGMGDGCgoICOnfuTN++ffF4PPTo0YNBgwbhui4jR44EYPjw4bu0bWq1J/K5uE2+bBERkXhYZj8/Iy3Ru5zubvciHcNFXL76Glql5uz9A1In7Q6Mn2qYGKpj/FTDxNjvdu/vD2q3kMLhcLP2Q0REJFYK/RiZmqsHwiGFvoiIJBeFfoxqR/rODlcdiIiIJAOFfoxMzY33nUCgeTsiIiISI4V+jKKhH9RIX0REkotCP1Y1oR8OBZu3HyIiIjFS6Mcoeva+TuQTEZEko9CPVc1IP1St3fsiIpJcFPoxMtHQ1214RUQkuSj0Y6WRvoiIJCmFfqyiJ/JppC8iIslFoR8jo5G+iIgkKYV+zGpuwxsVNcchAAAgAElEQVTU2fsiIpJcFPqxsiMX7Wn3voiIJBuFfqxqj+kH3ebth4iISIwU+rGK3ntfI30REUkuCv1YRc/e1zF9ERFJLgr9WEVDX7v3RUQkuSj0Y1VTMUehLyIiSUahH6voo3V1TF9ERJKLQj9GVu3u/bDZc0MREZF9jEI/RlZNxXTJnoiIJBuFfoxqQ98JK/RFRCS5KPRjVbN73w3rmL6IiCQXhX6MLJ29LyIiSUqhHyPLjgz1Xe3eFxGRJKPQj5VG+iIikqQU+jGyoyfy6ZI9ERFJLgr9GNUe03d1Hp+IiCQZhX6Mao/p65I9ERFJNvUO/U2bNgHwySef8PLLL1NdXd1ondqX1e7eN8p8ERFJMvUK/VGjRvHYY4/x3Xffcfvtt7NkyRLuu+++xu7bPsnyRL47jo7pi4hIcqlX6H/55ZeMGzeOd955h4EDBzJ+/Hh++OGHxu7bPsmuCX2j0BcRkSRTr9B3HAfXdZk1axa9e/emqqqKqqqqxu7bPsmuvU5fJ/KJiEiSqVfoX3DBBfTs2ZP27dtz3HHHcfHFFzNo0KDG7ts+yfZESmYU+iIikmS89Wn061//mquuugq75iy2l19+mdzc3Ebt2L7Kiu7eb95+iIiIxKpeI/333nuPRx55hIqKCvr168cvf/lL/v73vzd23/ZJHk/N7n1Xx/RFRCS51Cv0J0+ezIABA3j77bc59thjeffdd3nppZcatMA//vGPDBo0iIsuuogZM2awatUqLrvsMoYMGcKoUaNwXTe6zIEDBzJ48GAWL14MsNu2Tcn2RkLfuFaTL1tERCQe9b5O/8gjj2T27Nn89Kc/JSMjg1AoFPPCFixYwGeffcbf/vY3XnzxRTZs2MCECRMYNmwYU6dOxRjDrFmzWLJkCQsXLmTGjBkUFBRw//33A9TZtql5ak/k03X6IiKSZOoV+m3atOGBBx7gyy+/pFevXjz44IO0a9cu5oXNnTuXLl26cMMNN3D99ddz1llnsWTJEk4++WQAevfuzfz581m0aBE9e/bEsizatWuH4zgUFxfX2bap2TVnQeiYvoiIJJt6ncj3yCOPMHPmTK666irS09Pp2LEjN954Y8wLKykpYf369Tz99NOsXbuWoUOHYozBsiKj54yMDMrKyigvLycnJyf6udrpdbXdm9zcdLxeT8x93Z3aY/qWscjPz0rYfFsi1S9+qmFiqI7xUw0To7HrWK/Qz8jIoKKigocffphwOMwpp5xCenp6zAvLycmhc+fO+P1+OnfuTEpKChs2bIi+X1FRQXZ2NpmZmVRUVOw0PSsrK3r1wI5t96akpDLmfu6J7Y30wXWgqGjvGx1St/z8LNUvTqphYqiO8VMNEyNRddzThkO9du8/9NBDzJs3j/PPP5+LLrqIBQsWMH78+Jg7cuKJJ/LBBx9gjGHjxo1UVVVx2mmnsWDBAgDmzJlDjx496N69O3PnzsV1XdavX4/ruuTl5dG1a9dd2jY1T03oG528LyIiSaZeI/158+bx+uuvR0faZ511FgMGDIh5YX369OHjjz9m4MCBGGMYOXIkHTp0YMSIERQUFNC5c2f69u2Lx+OhR48eDBo0CNd1GTlyJADDhw/fpW1T83htHACdvS8iIkmmXqHvOA7hcBi/3x997fE07Dj5nXfeucu0ui7/u+mmm7jpppt2mnbooYc2+FLBRPF4LRz0lD0REUk+9Qr9AQMGcOWVV9K/f38A/vWvf3Huuec2asf2Vdq9LyIiyapeoX/99dfTtWtXPvzwQ4wxXH/99cyePbuRu7Zv8vpqToMw2r0vIiLJpV6hD5Hr4nv37h19fdtttzF69OjG6NM+zVsz0tcxfRERSTb1viPfj5kWun/b44+cy9BCV19ERJJYg0O/9iY5LY3Pp5G+iIgkpz3u3r/iiivqDHdjDIFAoNE6tS/z+iIjfUvH9EVEJMnsMfR/fMmcgLdm9z7avS8iIklmj6Ff+3Ab2c4XDX2N9EVEJLk0+Jh+S1W7e18n8omISLJR6MfIlxLZOaJj+iIikmwU+jHy+muOiGikLyIiSUahH6PaY/oa6YuISLJR6MfIn+Jr7i6IiIg0iEI/RtFj+ro5j4iIJBmFfow00hcRkWSl0I+RL7X2mH4zd0RERCRGCv0Y+VP9kR90Ip+IiCQZhX6M/GmR0NdIX0REko1CP0YpNSN9jfNFRCTZKPRj5EvVzXlERCQ5KfRj5I3ehreZOyIiIhIjhX6MvKleHGw8euKOiIgkGYV+jDx+D2G8Cn0REUk6Cv0YefweQvgU+iIiknQU+jHy+GzCePEq9EVEJMko9GPk8RAJfddt7q6IiIjERKEfo2joGxej0b6IiCQRhX6MbBvCePAaQ8gNNXd3RERE6k2h3wCOZeHFpTpc1dxdERERqTeFfgM4VqRslRWVzdwTERGR+lPoN4BjRe68X1Wpkb6IiCQPhX4DuDVVq9JIX0REkohCvwGiI/2tpc3cExERkfpT6DeA8US+B0q3NW9HREREYqDQbwBTU7XK0vLm7YiIiEgMFPoNUTPSryzViXwiIpI8FPoNURv6W6ubtx8iIiIxaJbQ37JlC2eeeSYrVqxg1apVXHbZZQwZMoRRo0bh1tzTfvLkyQwcOJDBgwezePFigN22bWpWTehXbdUd+UREJHk0eeiHQiFGjhxJamoqABMmTGDYsGFMnToVYwyzZs1iyZIlLFy4kBkzZlBQUMD999+/27bNwfZFvleVBZtl+SIiIg3R5KE/ceJEBg8eTNu2bQFYsmQJJ598MgC9e/dm/vz5LFq0iJ49e2JZFu3atcNxHIqLi+ts2xxqQz9QHm6W5YuIiDSEtykX9ve//528vDx69erFM888A4AxBqvmuveMjAzKysooLy8nJycn+rna6XW13Zvc3HS8Xk9C18OXEumDE4D8/KyEzrslUe3ipxomhuoYP9UwMRq7jk0a+q+99hqWZfHhhx+ydOlShg8fTnFxcfT9iooKsrOzyczMpKKiYqfpWVlZ2La9S9u9KSlJ7F3z8vOzsGqqVl3uUFS09w0P2VV+fpZqFyfVMDFUx/iphomRqDruacOhSXfvv/zyy7z00ku8+OKLHHXUUUycOJHevXuzYMECAObMmUOPHj3o3r07c+fOxXVd1q9fj+u65OXl0bVr113aNgd/zUg/XGWaZfkiIiIN0aQj/boMHz6cESNGUFBQQOfOnenbty8ej4cePXowaNAgXNdl5MiRu23bHHwpkcMF4aDVLMsXERFpCMsYs18PVxO9yyk/P4vb+vyNVrOXs/aIIOPn3ZXQ+bcU2h0YP9UwMVTH+KmGibHf7d7fX6SkR3aQuCGN9EVEJHko9BsgLSNyzZ4JqXwiIpI8lFoNkJ5Vc6F+WOUTEZHkodRqgPSslMgPjsonIiLJQ6nVAJmt0gCwHB3TFxGR5KHQb4CsvHQALI30RUQkiSi1GiA7NwMA29VIX0REkodCvwHSclJxsBX6IiKSVBT6DZCa4yeMF4/T3D0RERGpP4V+A/izakJ/v76XoYiI7G8U+g2QmumNhL7b3D0RERGpP4V+A6SmWYTx4t2/H1sgIiL7GYV+A/j91Iz0FfoiIpI8FPoNYFkQxsZrtH9fRESSh0K/gcKWjQ8X11Xwi4hIclDoN5BjRUpXVVXVzD0RERGpH4V+A9Xegbe6srJ5OyIiIlJPCv0Gcq3I3fgqKzTSFxGR5KDQb6DakX5lWVnzdkRERKSeFPoNZGoqV1W6tXk7IiIiUk8K/QZya0f6JeXN2xEREZF6Uug3lCfyrby4onn7ISIiUk8K/YaqedpORbHO3hcRkeSg0G8ob+Rb1bZg8/ZDRESknhT6DWR7IyN9hb6IiCQLhX4DWb5I6Ae2KvRFRCQ5KPQbyPZHbs5TXRZu5p6IiIjUj0K/gTxpkYP6wVKFvoiIJAeFfgOlZfoBCFXoKXsiIpIcFPoNlJGVDoBbYZq5JyIiIvWj0G+g3NbZuFjYVVZzd0VERKReFPoN1OaAPCpJxxdQCUVEJDkosRrogIMzqCCDlICnubsiIiJSLwr9BmrdOSsS+g6Eq3UGv4iI7PsU+g2Uf2gG5WQCUL6htJl7IyIisncK/QZq3QYqiJzBv3HN+mbujYiIyN4p9BvI64Vqb+TM/U2rNzRzb0RERPbO25QLC4VC3HPPPaxbt45gMMjQoUM5/PDDueuuu7AsiyOOOIJRo0Zh2zaTJ09m9uzZeL1e7rnnHo499lhWrVpVZ9vm4vpdCEPJSoW+iIjs+5o0Md98801ycnKYOnUqzz77LA888AATJkxg2LBhTJ06FWMMs2bNYsmSJSxcuJAZM2ZQUFDA/fffD1Bn2+ZkpzkAbF29pVn7ISIiUh9NGvq//OUvueWWW6KvPR4PS5Ys4eSTTwagd+/ezJ8/n0WLFtGzZ08sy6Jdu3Y4jkNxcXGdbZtTSlYk9EvWVjdrP0REROqjSXfvZ2RkAFBeXs7NN9/MsGHDmDhxIpZlRd8vKyujvLycnJycnT5XVlaGMWaXtnuTm5uO15vYa+nz87Mi827rgZVQXbx9mtSfahY/1TAxVMf4qYaJ0dh1bNLQBygsLOSGG25gyJAhDBgwgEmTJkXfq6ioIDs7m8zMTCoqKnaanpWVtdPx+9q2e1NSUpnQ/ufnZ1FUFNnYyGsbKV94qxWdJvWzYx2lYVTDxFAd46caJkai6rinDYcm3b2/efNmrrnmGu644w4GDhwIQNeuXVmwYAEAc+bMoUePHnTv3p25c+fiui7r16/HdV3y8vLqbNucOnVIp4pU7Kom33YSERGJWZOm1dNPP822bduYMmUKU6ZMAeDee+9l7NixFBQU0LlzZ/r27YvH46FHjx4MGjQI13UZOXIkAMOHD2fEiBE7tW1OhxySy1wyyQpUY5wwlkfhLyIi+y7LGLNfPxs20bucdtz98tX7Jfzlkvc4lJX85rNfk9I+N6HL2p9pd2D8VMPEUB3jpxomxn63e39/k39ENutpB0DRxxubuTciIiJ7ptCPQ+u2HtbQHoB1CwubuTciIiJ7ptCPg9cLm/yRyxC//3Ql+/mREhERSXIK/TilpgXYYrWiZGkJbnliLw8UERFJJIV+nA7Jq2KNOQSqoOSzoubujoiIyG4p9ON05ok2a+gAwKZ5mzFOuJl7JCIiUjeFfpxOOS+fNXQEYOXCNZgNFXv5hIiISPNQ6Mfp+N4ZbCaX9b5sVs7/ga2flmGCwebuloiIyC4U+nFKT4dD8r/j/fDZYODTl7+EFVUY12nuromIiOxEoZ8APbtu5RvTldKcKpbPXkbJVwH4vgzjus3dNRERkSiFfgL89Ox8wOKtTj6MMbw16nWqV/ng2zJMKNTc3RMREQEU+glx8gUH4LWCrFh7Jet/toKKzeW8de/fCa7zYX8TxC3T9fsiItL8FPoJ0CYfftVnNRQfwbPpFgefcyibvy9i+s0vU/xNFZ7vfLhrS7W7X0REmpVCP0FuefgAvFYA88FIHuw+ihOvPpmyTWW8ctNLfD7jCzwbW8HSco36RUSk2Sj0E6R9B7jiF2ug5DC++c/vebLLeM4Zex6p2WnMe24O/7hzBpVrwPedH3flVh3rFxGRJqfQT6B7p7TlJ20L4eMb+Pe7p3OH9TvOm3Ixh/U8gvVfruXl373Awpc/wd6Uib00hFu4Vbv8RUSkySj0Eyg72+Ivb3o4IKMEZk7k/Teu5BdLTiVvWBvOvusc/BkpLPjrfF767QusnFuId0MrrK8rcIu26Ql9IiLS6BT6Cda5cxp/e7uaw9tugoU3s+bPr9P/vZuZ1vGvDH7uV5wwsAflRWX8a/QbvDrsbxR+shXfumyspeWR8NfIX0REGolCvxEcfVQm0/8X4uye62DdaVhPf84fXk/l55/1wBlkGPLHqzis5xFsWFrIP+6czj/ufJXChVvxrs2KjPw3bNWDe0REJOEss5/vVy4qKkvo/PLzs+o9z/LKcqb+AwruSae4qhUc+Cn0vY1+R7o80G0ymWsy+PCFuaz6eCUAbY84gO6DTuLwnl0wnkqc3DDk+7HTUhO6DvuCWOoodVMNE0N1jJ9qmBiJqmN+ftZu3/OMHj16dNxL2IdVVib24TcZGSn1nqff5+e4Iw19Lqlk87elfPvVkfDF1Xy3No8/udextdUSBl78K4457Xiqy6tZ+/lqvpuznK//8xWhCkNe3kGkVvpwyyowbgDSvFjW/rFzJpY6St1Uw8RQHeOnGiZGouqYkZGy2/c00o9RQ7bEjDGUVZbx3icWTw03fPp9e7AcOPpveHtO4FdHHMNNh99L7pZcPvv7JyybtZRQZRDLtjjk5M4cfe6xdDrxEPBW4bZyMHk2VlY6lmUldN2akkYG8VMNE0N1jJ9qmBhNMdJX6Mconr8UNxSiuLqSv79l8dcJHpZvPBBwoeurcFoB/Q9L4Yaf3McJ3hP5dvY3fPWvL9i0fCMAaTlpHN6rC0eceSTtuh2E66/EZBtMbnJuAOg/ifiphomhOsZPNUwMhX4C7EuhX8sJBthUHeCf/3J5+UGbrwvbR95otxBOeYIjj5rHFYf/jksPHIKz0uHrf3/Jt3OWU1UauZtfRutMjujdhUNPO5yDuh6IlRbAzXIxWWDlpGF5vPGuZqPTfxLxUw0TQ3WMn2qYGAr9BNgXQ79WOBigOBDgf7Nd3nw8xOzFh2CwIbUEur2C59gX6Xe4jysOv5neWWex8csNfDt7Gd/N/ZZAWTUAvnQ/nU48mENPOYyDTzqE1BwLNzMMmQaT5cHK2Df3Aug/ifiphomhOsZPNUwMhX4C7MuhX8sNBdkaqOLL7x3emBLk3++0pqgqL/Jm3rdw7ItkH/kG5xzSifMPvYqeGb3Z/GURKxd8zw8LVrCtcGt0XnkHt6b9sR3pcFxH2nU7gNS2Htw0t2YjwIuVnrZPbAToP4n4qYaJoTrGTzVMDIV+AiRD6NcyrktlVQWbg2Hen+Xy7p8DzPrsYAJOzSV7ecvhyDdIO+Jf9D3cyy8OGciZOT8jdVMKKxd8z6pPVlL49TrC1duv8c/tlEeHYztyULf2tD2iDdmdUjEZBtINJt3GykrB8voaZX32RP9JxE81TAzVMX6qYWIo9BMgmUJ/R24wyLZQNeu2BJn5usuCf4SYu6wT1U56pEF6ERw6CzrP4vCOX9D3iK70bn8+J6WdTNX3laxbvIa1X6yh8Kt1hKq3P9zHn5HCAT85kAO6HEjbnxxA/uE5pHfwQ5oV2RBIs7EyGn9DQP9JxE81TAzVMX6qYWIo9BMgWUN/R04wQFkoQNG2IHPfdVg4I8Dsz9qypSp/e6OcH+CQ97A6zueIA1fQu3M+p3bsR4/MU/Gt8rJxWSEbl29g47INlK4r2Wn+KVmptD6kTeTr0Nbkdsom7ydZ+FunQCqYVAOZPix/CpadmPsE6D+J+KmGiaE6xk81TAyFfgLsD6G/IzcUoiJYRXkwxJJlIRa9E+Tz9wwffd+JilD29ob+Mmi/ENovIOugTzmu/VZO7nQIxx14FkdZ3fCt8rHpmw1s+nYjW1ZuZuv6Uoy78z+FzPwsWh/Smpz2ebRql0WrDplkH5JJ1iFZWGlg/EC6FyvVH/MVA81dx/2BapgYqmP8VMPEUOgnwP4W+jsyxhAOVFPphNhWGWDJV2GWzK5i2UcOi1bksXbbITt/IG0LtP0SDviS9DZfc3j+Jo4/xEO3dkdzRMYJHLDlAMxaQ/HKzWxZuZnNP2ymYnP5Lsu1vTatDsqhVfscctq1IrNtOlnt08nskEVWx0z8rX3gtyDNE9kgqONQwb5Ux2SlGiaG6hg/1TAxmiL09/0LumW3LMvCl5pGK9JolQEd+8DZZwSoDAWoCofYtPkbFn8U4pv3qlj2tc3SjW3YsKo3ZtVZVAKLa77IWhe5SqD1F6TmfE/HVkUcdmI1XS/MpHNOV9qWdSR7cxZmo6F0XSml60rYuq6EkjXFdfbLl+Yj64BsstpmkdU2g8wDMshsl0HGgRmkH5hGers0wri45VVYqT7w+PaJKwpERPZ3GunHKNm2aI0xuMEAVeEggWCQ4rJqli8x/DC/nO++CPP16jRWl7aluPIgdn3oogvZayFnJbRajTd7DW0yNtOhVQUHt3E5PDeT9rQjq+oA0kpSsbbYVBdVUbZpG9s2biNYEdhtv3xpPtJzM8jISye9dSrpbdJJz08n/YB0Mg5IJ61tKiltUknNT8OTnYLl9SbFTYeaUrL9W9xXqY7xUw0TQyN9iZtlWXhSUslMSSUzA1rnwhGdwPR1CAcDVIdDBMNBysq/4YcVsPazbaz+KsC3Kw0/bMpi/daD2Lq6J2ATBjbUfH1Su4CUUshaD5kbIKuQlPQicjqVkt+1nPYpDh19Hg7ypJFrskitboV3mw9TanC3hSkrKmP9klLYy2anP8NPanYqaTmRr9ScVFLzIl9prVNJbZNCSl4aqflppLRJIyU3HU9aivYeiIj8iEK/hbJsD77UdGqPtrfOgUM6AGeCccKEQ0GCTphQqJqKyuWsX+ey4ZsqNi4tZ9XKalZt8LC+JJ2N29qwbdsBVG3uCkAA2Fjz9dWOC7RDkL45cl5B+hZI30zKwV6yuli0tSs40A7SGpc8LHKMlwzHR0rYh6fai6mwCG4LUf5tOW7Yrdf6eVO8pGT68WelkJLlJyXLjz87hZRWKaRk+/Hn+Elp5cefk0pK61T82an4siPf/Zl+fBl+LFsbDSKyf1Hoyy4sjxefxxvdIMjJgfbtgJMir43rghMmFA4RCgcJhbZSVb2FjRsdtqyoYPP3lRSurmD1hiAbin0UbUunpLIVZYFsKre2J1B0NBDZQAgAm4Gvd9cZbxWklkLKVmhbSoq/mFZ2KTlWGa2sSloRIJsQma5DhmtIc8HvWPjCYcJBh8pNVZjVBuq3rbDzolO9+NJ9+NN9+DIiX/6a794MH/5MH74sP77syAaFLzMFX4Yfb7ofX5oPb7oXb5oPb6oXb1rNz2lebUyISLNR6EvMLNsG24/f58dPBgA5wEHtgBN2bmtcBxyHUDiEEw4SdkoIBIpwHT8rvy5h25pKNq+tpHBTJRu2hNlSalNc5mdrdSoVwTQqgulUBTMJVOQRLDmUgJvCJmBTTD02+Kki1V9MqqeUNG8pqZ4y0qxy0q0K0q1qUk2AVEKkEibFdUhxXfyug686jLeiCm8heJyElA9PiieyIVDz5Uvz4kn14E334UuLbCB40iMbCN5UL7bfiycl8rMnxYsnxYMnxcOGNplUBEKR+aV48fg92CkevCme7e38npr2kdc65CHSsiVd6Luuy+jRo/nmm2/w+/2MHTuWgw8+uLm7Jbth2R6wPfh9fqjZQMgicqJJ23atd/s547rgOmBcQqEgTrgKxymjqsqwbUuQsg2VlBUFKNlQzpYt1WwpDlBcZigttymvtigPeKkIeqkK+akKpRIIpRIIpVERTmOr05lwdSZOOAOMp/7rgsFHkBSC+AlEv/t3mOYjhM+qwmdXR76sAD4C+KxQ5D0TwmccfJUO/oowXhPAZ1x8rrvLaZSNwfJaWD4Lj8/G9tvYPhuP38b2ebD9Hjy+mo0HnwdP7Zffi+2zsb125D1v5HORNjWf9drYO7zn+fHrHT/n9WB5LGyvjWXXfPfa2LYV+e61I+977O3tPDa2p+b9naZb2pARiUHShf7MmTMJBoO88sorfP755zz44IM89dRTzd0tSbDI3oRIDPp9KdHpmbmQ3w44Jrb5GdcFE/lyww6OW44b3kpVpUN5aYDKoioqi6upKA1Stq2asrJqyiqDlFeGKa9yKK9yqQwYKgMWVUGLqpCH6qCX6rCHYNhPRdhLSTiFsOMnFM4m7LQl7KTgOGk4jg/X9WNcP67rB7fuWxzbOJENA8I130N4COPFwUu4jp/r+55T8zqMJ+zgDYfxVkVe27h4cPAQwiZQ87NDMsWoAYxdcz6oDcYCY1nRn7Ei07EsjL3958h3sD1W9LNWzXTLirSxbCKHY3b42appZ3l+NN2zfXq0ncfe4f2aDReb7Rs1thX92fZYkfdr23kiG0C10+2d2tV81q75jG1Hv2w78r7HU7tRFNlQsmvex2NjWRa2bWPbnpo+WtjUbEzZ29/Hqq2JtUNNrMi/jx2mUVRNcUlFZP1ta/vnar7XTovWy9qxTc08a/6uovXe6X2i0yLLrfkX+qPlyN4lXegvWrSIXr16AXD88cfz1Vdf7eUTIjUbETVjaY8Xasf3qdmQeyBwZOMs1xgDxgAG3ADGVOGGXUJBl+oKh0BVgOC2MIGKKqoqqqioqKKirJrK6hCBqhDVVWGqq8MEAi7VNV/BEASC4BibimoIhSHkWIQci6ADYdci7FiEXA9hx8JxPTiun7CxcRwPYdeDa2wc14NrvLiuF9f14BgvpmaacTzg2tjGxnaJ/Ox6sI2FZSxs18J2bTzGYAOemo0HGwcPbs33un+2cWu+TPRnq85pZoe2u59m42K5NdPdPbete1pkufX+O2WvF5xIM9v+d2RFX0e+Wzu1gZoNxB9Ni+zXo+Ftrdo/THSZP/43UzsvA1QdaDH206F7XqkESbrQLy8vJzMzM/ra4/EQDofxepNuVaQFqB2pAGB7IoNOH3jTIK1VfPPel66Ndl0Ihw2uC07IxQ27uCEHJ+REXgdDBAMhQqEwoWCAcMglGAwRDkWmhUNO5CvsEA5F5uWEDGHH4IQhFHZxHZdw2BAOu7iuIRwCxwXHsXHCBsexcIwHJxzZ8HFdg2NcXBdcY3Adg2sMjguusSLTXAvbhmDI4BoX17WgZrrrEtlWqzmXw7iRjThjbIwL1PyMC8ZE/oPHqRmumsg0y6Vmd0PNa2PA2BgDlqn9XORnajaoIhuJVs0ui8jPte/Xto1GT02S1Pl6h5Spfb27z1l1tt0xNmv6XmebyMj/x+9H2myP2ej8duhHbextn2a2f3aHZVj8uD87z29HO84nOv35niwAAAhySURBVM3UMW2HZW+fZnb6vsv8zJ7axTZtxw3N0uIMmkrSJWVmZiYVFRXR167r7jHwc3PT8Xrrf9y2PvZ04wOpP9UxfqqhSPOq3dap13djdvkOkJJmR6/qaezf6aQL/e7du/Pee+9xzjnn8Pnnn9OlS5c9ti8pqUzo8vel0VUyUx3jpxomhuoYP9UwPmU1MaU78tXhF7/4BfPmzWPw4MEYYxg/fnxzd0lERCQpJF3o27bNmDFjmrsbIiIiSacpLg0WERGRfYBCX0REpIVQ6IuIiLQQCn0REZEWQqEvIiLSQij0RUREWgiFvoiISAuh0BcREWkhFPoiIiIthGWMMXtvJiIiIslOI30REZEWQqEvIiLSQij0RUREWgiFvoiISAuh0BcREWkhFPoiIiIthLe5O5AsXNdl9OjRfPPNN/j9fsaOHcvBBx/c3N1KGhdccAFZWVkAdOjQgUGDBjFu3Dg8Hg89e/5/e3cXEsUehgH8GVfRcpPSiBCUjMpQsRIzoa0uRMxKN0wrqTXQPgwlu6j8SFNRpAvxQlMi8EJCKF0ioy5KJDPLjxAt1lKEyMqislXTzVJ3/ueq6eTXMSlXzz6/q5md3b/vPrz6Mrsyo0FiYqKFK5y/nj59ivz8fFy9ehXd3d1ISUmBJElYu3YtMjMzYWNjg0uXLqG2tha2trZIS0uDr6+vpcued/6dY3t7O+Lj47Fq1SoAQHR0NHbt2sUcpzA6Ooq0tDT09PRgZGQEJ0+exJo1a9iLv2myHFeuXDm3vShoRu7evSuSk5OFEEK0traK+Ph4C1e0cHz79k1otdpfHgsPDxfd3d1ClmVx9OhRYTAYLFTd/HblyhWxZ88eERUVJYQQ4sSJE6KxsVEIIURGRoa4d++eMBgMQqfTCVmWRU9Pj4iIiLBkyfPS+BwrKipEaWnpL89hjlPT6/UiNzdXCCGE0WgUO3bsYC/OwmQ5znUv8uP9GWppacG2bdsAABs3boTBYLBwRQtHR0cHhoeHERsbi5iYGDx58gQjIyNwd3eHJEnQaDRoaGiwdJnzkru7O4qKipT99vZ2BAQEAAC2b9+Ox48fo6WlBRqNBpIkwdXVFWazGUaj0VIlz0vjczQYDKitrcWhQ4eQlpaGoaEh5jiNnTt3IikpSdlXqVTsxVmYLMe57kUO/RkaGhqCWq1W9lUqFcbGxixY0cLh4OCAuLg4lJaWIjs7G6mpqVi0aJFy3NHREYODgxascP4KCQmBre3Pb+GEEJAkCcDP3Mb3JvOcaHyOvr6+OHfuHMrLy+Hm5obi4mLmOA1HR0eo1WoMDQ3h1KlTOH36NHtxFibLca57kUN/htRqNUwmk7Ivy/Ivf0Roah4eHggPD4ckSfDw8MCSJUvQ39+vHDeZTHBycrJghQuHjc3PX9kfuY3vTZPJpPz/BE0uODgYPj4+yvbz58+Z4394//49YmJioNVqERYWxl6cpfE5znUvcujPkJ+fH+rq6gAAbW1tWLdunYUrWjj0ej0uXrwIAPjw4QOGh4exePFivH79GkII1NfXw9/f38JVLgxeXl5oamoCANTV1cHf3x9+fn6or6+HLMt49+4dZFmGs7OzhSud3+Li4vDs2TMAQENDA7y9vZnjNHp7exEbG4uzZ88iMjISAHtxNibLca57kaeqMxQcHIxHjx7h4MGDEEIgLy/P0iUtGJGRkUhNTUV0dDQkSUJeXh5sbGxw5swZmM1maDQabNiwwdJlLgjJycnIyMhAQUEBVq9ejZCQEKhUKvj7++PAgQOQZRkXLlywdJnzXlZWFnJycmBnZ4fly5cjJycHarWaOU7h8uXL+PLlC0pKSlBSUgIAOH/+PHJzc9mLv2GyHFNSUpCXlzdnvci77BEREVkJfrxPRERkJTj0iYiIrASHPhERkZXg0CciIrISHPpERERWgkOfiBSenp4AgMHBQSQkJPyxdXU6nbKt1Wr/2LpE9Hs49IlogoGBAbx48eKPrdfc3KxsV1VV/bF1iej38OI8RDRBbm4uPn78iISEBBQXF+PmzZsoKyuDLMvw9vZGZmYm7O3tERgYCB8fH3z69Al6vR7Z2dno6upCb28vPD09UVBQgPz8fABAVFQUKisr4enpic7OTgwPDyM9PR2dnZ2QJAlxcXHYu3cvbty4gYcPH2JgYABv3rzB1q1bkZWVZdlAiP4neKZPRBOkp6djxYoVKC4uRldXFyoqKnDt2jVUVVXBxcUFpaWlAIC+vj4cO3YMVVVVaGtrg52dHa5fv47q6moMDg7iwYMHSE9PBwBUVlb+8jOKioqwbNky3L59G2VlZSgqKkJHRwcAoLW1FYWFhbh16xbu37+Pzs7OuQ2A6H+KZ/pENK2mpiZ0d3dj//79AIDR0VF4eXkpx39cQnnz5s1YunQpysvL8fLlS7x69Qpfv36dct3GxkblctbOzs4ICgpCc3Mz1Go1Nm3apNxlzM3NDQMDA3/r7RFZFQ59IpqW2WxGaGiocsZuMplgNpuV4w4ODgCAmpoaFBYWIiYmBhEREejr68N0V/kef0wIoaxrb2+vPC5J0rTrENHM8eN9IprA1tYWY2NjAIAtW7aguroanz9/hhACWVlZKCsrm/CahoYGhIaGYt++fXByckJTU5MyxFUqlbLeD4GBgdDr9QAAo9GImpoaBAQE/OV3RmTdOPSJaAIXFxe4urpCp9Nh/fr1SExMxJEjR7B7927Isozjx49PeE1UVBTu3LmDsLAwJCUlwc/PD2/fvgUABAUFQavV4vv378rzExIS0N/fj7CwMBw+fBjx8fHw9vaes/dIZI14lz0iIiIrwTN9IiIiK8GhT0REZCU49ImIiKwEhz4REZGV4NAnIiKyEhz6REREVoJDn4iIyEpw6BMREVmJfwAY/UUwsOf2VwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the 4 cross validation test sets is 20.292589039083627 with variance 5.315927246818269. Compare this to the training set mean loss of 19.848784472175616\n"
     ]
    }
   ],
   "source": [
    "# Initialize w vector\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "k_ = 4\n",
    "\n",
    "# Perform linear regression by gradient descent with cross validation (k=4)\n",
    "test_loss_mean, test_loss_var, vector_test_loss, train_loss_mean, w = least_squares_GD(y_train, X_train, initial_w, gamma = 0.00001, k=k_, max_iters = 250)\n",
    "\n",
    "means_over_time = vector_test_loss.mean(axis=0)\n",
    "error1 = abs(means_over_time - vector_test_loss[0])\n",
    "error2 = abs(means_over_time - vector_test_loss[1])\n",
    "error3 = abs(means_over_time- vector_test_loss[2])\n",
    "error4 = abs(means_over_time - vector_test_loss[3])\n",
    "x = np.arange(len(error1))\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "# for i in np.arange(4):\n",
    "#     plt.plot(x, vector_test_loss[i], label= 'Trial {}'.format(i))\n",
    "plt.plot(x, vector_test_loss[0], label='Trial 1', c='red')\n",
    "plt.fill_between(x, vector_test_loss[0]-error1, vector_test_loss[0]+error1,\n",
    "    alpha=0.2, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "plt.plot(x, vector_test_loss[1], label='Trial 2', c = 'green')\n",
    "plt.fill_between(x, vector_test_loss[1] - error2, vector_test_loss[1] + error2, alpha = 0.2, edgecolor='#FF3F1B', facecolor = '#12E99F')\n",
    "plt.plot(x, vector_test_loss[2], label='Trial 3', c='blue')\n",
    "plt.fill_between(x, vector_test_loss[2]-error3, vector_test_loss[2]+error3,\n",
    "    alpha=0.2, edgecolor='#CC4F1B', facecolor='#12E2FF')\n",
    "plt.plot(x, vector_test_loss[3], label='Trial 4', c='purple')\n",
    "plt.fill_between(x, vector_test_loss[3]-error4, vector_test_loss[3]+error4,\n",
    "    alpha=0.2, edgecolor='#CC4F1B', facecolor='#FF00FF')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Test Set Loss Over Iterations of Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('The mean of the {} cross validation test sets is {} with variance {}. \\\n",
    "Compare this to the training set mean loss of {}'.format(k_,test_loss_mean, test_loss_var,train_loss_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a test for prediction accuracy\n",
    "\n",
    "w = least_squares_GD(y_train, X_train, initial_w, gamma = 0.00001, k=0, max_iters = 250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression with stochastic gradient descent (SGD)\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_w = np.ones(X_train.shape[1])\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "losses, ws = least_squares_SGD(y_train, X_train, initial_w, max_iters = 250, tol = 1e-4, patience = 5) # fit model, retrieve parameters ws\n",
    "test_losses = list(map(lambda x: compute_loss(y_test, X_test, x, method = 'MSE'), ws)) # retrieve losses using test set with ws\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(losses, label='Training set loss', c='blue')\n",
    "plt.plot(test_losses, label='Test set loss', c='red')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Cost Function Loss Over Iterations of Stochastic Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make plot with label prediction accuracy\n",
    "\n",
    "pred_ytrain = list(map(lambda x: predict_labels(x, X_train), ws)) # Training prediction\n",
    "pred_accuracytrain = list(map(lambda x: pred_accuracy(x, y_train), pred_ytrain))\n",
    "pred_ytest = list(map(lambda x: predict_labels(x, X_test), ws)) # Test prediction\n",
    "pred_accuracytest = list(map(lambda x: pred_accuracy(x, y_test), pred_ytest))\n",
    "\n",
    "\n",
    "plt.plot(pred_accuracytrain, label='Training set prediction accuracy', c='blue')\n",
    "plt.plot(pred_accuracytest, label='Test set prediction accuracy', c='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Prediction Accuracy')\n",
    "plt.title('Prediction Accuracy Over Iterations of Stochastic Gradient Descent')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression using least squares normal equations\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = least_squares(y_train, X_train)\n",
    "\n",
    "pred_y = predict_labels(w, X_test)\n",
    "pred_accuracy(pred_y,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression using the normal equations with additional polynomial degrees\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1,15)\n",
    "rmse_tr = np.zeros(len(degrees))\n",
    "rmse_ts = np.zeros(len(degrees))\n",
    "pred_tr = np.zeros(len(degrees))\n",
    "pred_ts = np.zeros(len(degrees))\n",
    "\n",
    "\n",
    "for ind, degree in enumerate(degrees):\n",
    "    #train the model\n",
    "    X_test_poly = build_poly(X_test,degree)\n",
    "    X_train_poly = build_poly(X_train,degree)\n",
    "    w = least_squares(y_train, X_train_poly)\n",
    "    rmse_tr[ind] = (np.sqrt(2 * compute_loss(y_train, X_train_poly, w)))\n",
    "    pred_tr[ind] = pred_accuracy(predict_labels(w,X_train_poly),y_train)\n",
    "    \n",
    "    #test the model\n",
    "    \n",
    "    rmse_ts[ind] = (np.sqrt(2 * compute_loss(y_test, X_test_poly, w)))\n",
    "    pred_ts[ind] = (pred_accuracy(predict_labels(w, X_test_poly),y_test))\n",
    "    \n",
    "    # print the update\n",
    "    print(\"degree: {d} \\t rmse_ts: {a} \\t  pred_ts: {b}\".format(d = degree, a = rmse_ts[ind], b = pred_ts[ind]))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# plot the loss\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(degrees,rmse_tr, c='blue')\n",
    "plt.plot(degrees,rmse_ts, c='red')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['training set', 'testing set'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#plot the accuracy\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(degrees,pred_tr, c='blue')\n",
    "plt.plot(degrees,pred_ts, c='red')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['training set', 'testing set'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression using ridge regression\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-5, 0, 15)\n",
    "rmse_tr = []\n",
    "rmse_ts = []\n",
    "pred_tr = []\n",
    "pred_ts = []\n",
    "\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    \n",
    "    w = ridge_regression(y_train, X_train, lambda_)\n",
    "    rmse_tr.append(np.sqrt(2 * compute_loss(y_train, X_train, w)))\n",
    "    pred_tr.append(pred_accuracy(predict_labels(w, X_train),y_train))\n",
    "    \n",
    "    \n",
    "    rmse_ts.append(np.sqrt(2 * compute_loss(y_test, X_test, w)))\n",
    "    pred_ts.append(pred_accuracy(predict_labels(w, X_test),y_test))\n",
    "    \n",
    "plt.style.use('seaborn')\n",
    "plt.semilogx(lambdas,rmse_tr, c='blue')\n",
    "plt.semilogx(lambdas,rmse_ts, c='red')\n",
    "plt.legend(['training set', 'testing set'], loc='upper left')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.semilogx(lambdas,pred_tr, c='blue')\n",
    "plt.semilogx(lambdas,pred_ts, c='red')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['training set', 'testing set'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# print(np.max(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression using gradient descent and stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[y_train == -1] = 0\n",
    "y_test[y_test == -1] = 0\n",
    "y[y == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "losses, ws = logistic_regression(y_train, X_train, initial_w, method = 'gd', max_iters = 2000, gamma = 0.0005) # fit model, retrieve parameters ws\n",
    "test_losses = list(map(lambda x: compute_loss(y_test, X_test, x, lam = 0, method = \"logistic\"), ws)) # retrieve losses using test set with ws\n",
    "\n",
    "\n",
    "\n",
    "# Make plot with label prediction accuracy\n",
    "\n",
    "pred_ytrain = list(map(lambda x: predict_labels_logistic(x, X_train), ws)) # Training prediction\n",
    "pred_accuracytrain = list(map(lambda x: pred_accuracy(x, y_train), pred_ytrain))\n",
    "pred_ytest = list(map(lambda x: predict_labels_logistic(x, X_test), ws)) # Test prediction\n",
    "pred_accuracytest = list(map(lambda x: pred_accuracy(x, y_test), pred_ytest))\n",
    "\n",
    "\n",
    "plotCurves(losses, pred_accuracytrain, test_losses, pred_accuracytest, \"Logistic Regression\")\n",
    "\n",
    "\"\"\"\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(losses, label='Training set loss', c='blue')\n",
    "plt.plot(test_losses, label='Test set loss', c='red')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Logistic Regression: Cost Function Loss Over Iterations of Stochastic Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(pred_accuracytrain, label='Training set prediction accuracy', c='blue')\n",
    "plt.plot(pred_accuracytest, label='Test set prediction accuracy', c='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Prediction Accuracy')\n",
    "plt.title('Logistic Regression Prediction Accuracy Over Iterations of Gradient Descent')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample = y[:10000] \n",
    "tx_sample = tx[:10000,:]\n",
    "\n",
    "X_train_sample, y_train_sample, X_test_sample, y_test_sample = split_data(tx_sample, y_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with varying learning rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_iter = 10000\n",
    "\n",
    "lr = [0.00001,0.00005,0.0001,0.0005,0.001] \n",
    "\n",
    "\n",
    "test_losses  = np.zeros([len(lr),(num_iter+1)])\n",
    "test_acc = np.zeros([len(lr),(num_iter+1)])\n",
    "train_losses = np.zeros([len(lr),(num_iter)])\n",
    "train_acc= np.zeros([len(lr),(num_iter+1)])\n",
    "\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    \n",
    "    l_tr, ws = logistic_regression(y_train_sample, X_train_sample, initial_w, max_iters = num_iter, gamma = lr[i], method = 'gd') # fit model, retrieve parameters ws\n",
    "    train_losses[i,:] = np.array(l_tr)\n",
    "    l_ts = list(map(lambda x: compute_loss(y_test_sample, X_test_sample, x, lam = 0, method = \"logistic\"), ws)) # retrieve losses using test set with ws\n",
    "    test_losses[i,:] = np.array(l_ts)\n",
    "    pred_ytrain = list(map(lambda x: predict_labels_logistic(x, X_train_sample), ws)) # Training prediction\n",
    "    train_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_train_sample), pred_ytrain))\n",
    "    pred_ytest = list(map(lambda x: predict_labels_logistic(x, X_test_sample), ws)) # Test prediction\n",
    "    test_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_test_sample), pred_ytest))\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    titre = \"learning rate \"+str(lr[i])\n",
    "    plotCurves(train_losses[i,:], train_acc[i,:], test_losses[i,:], test_acc[i,:], titre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 1000\n",
    "\n",
    "lr = [0.00001,0.00005,0.0001,0.0005,0.001] \n",
    "\n",
    "\n",
    "test_losses  = np.zeros([len(lr),(num_iter+1)])\n",
    "test_acc = np.zeros([len(lr),(num_iter+1)])\n",
    "train_losses = np.zeros([len(lr),(num_iter)])\n",
    "train_acc= np.zeros([len(lr),(num_iter+1)])\n",
    "\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    \n",
    "    l_tr, ws = logistic_regression(y_train_sample, X_train_sample, initial_w, max_iters = num_iter, gamma = lr[i], method = 'gd') # fit model, retrieve parameters ws\n",
    "    train_losses[i,:] = np.array(l_tr)\n",
    "    l_ts = list(map(lambda x: compute_loss(y_test_sample, X_test_sample, x, lam = 0, method = \"logistic\"), ws)) # retrieve losses using test set with ws\n",
    "    test_losses[i,:] = np.array(l_ts)\n",
    "    pred_ytrain = list(map(lambda x: predict_labels_logistic(x, X_train_sample), ws)) # Training prediction\n",
    "    train_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_train_sample), pred_ytrain))\n",
    "    pred_ytest = list(map(lambda x: predict_labels_logistic(x, X_test_sample), ws)) # Test prediction\n",
    "    test_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_test_sample), pred_ytest))\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    titre = \"learning rate \"+str(lr[i])\n",
    "    plotCurves(train_losses[i,:], train_acc[i,:], test_losses[i,:], test_acc[i,:], titre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 20000\n",
    "\n",
    "lr = [0.00005,0.0001] \n",
    "\n",
    "\n",
    "test_losses  = np.zeros([len(lr),(num_iter+1)])\n",
    "test_acc = np.zeros([len(lr),(num_iter+1)])\n",
    "train_losses = np.zeros([len(lr),(num_iter)])\n",
    "train_acc= np.zeros([len(lr),(num_iter+1)])\n",
    "\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    \n",
    "    l_tr, ws = logistic_regression(y_train_sample, X_train_sample, initial_w, max_iters = num_iter, gamma = lr[i], method = 'gd') # fit model, retrieve parameters ws\n",
    "    train_losses[i,:] = np.array(l_tr)\n",
    "    l_ts = list(map(lambda x: compute_loss(y_test_sample, X_test_sample, x, lam = 0, method = \"logistic\"), ws)) # retrieve losses using test set with ws\n",
    "    test_losses[i,:] = np.array(l_ts)\n",
    "    pred_ytrain = list(map(lambda x: predict_labels_logistic(x, X_train_sample), ws)) # Training prediction\n",
    "    train_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_train_sample), pred_ytrain))\n",
    "    pred_ytest = list(map(lambda x: predict_labels_logistic(x, X_test_sample), ws)) # Test prediction\n",
    "    test_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_test_sample), pred_ytest))\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    titre = \"learning rate \"+str(lr[i])\n",
    "    plotCurves(train_losses[i,:], train_acc[i,:], test_losses[i,:], test_acc[i,:], titre)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent with varying learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_iter = 10000\n",
    "\n",
    "lr = [0.00001,0.00005,0.0001,0.0005,0.001] \n",
    "\n",
    "\n",
    "test_losses  = np.zeros([len(lr),(num_iter+1)])\n",
    "test_acc = np.zeros([len(lr),(num_iter+1)])\n",
    "train_losses = np.zeros([len(lr),(num_iter)])\n",
    "train_acc= np.zeros([len(lr),(num_iter+1)])\n",
    "\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    \n",
    "    l_tr, ws = logistic_regression(y_train_sample, X_train_sample, initial_w, max_iters = num_iter, gamma = lr[i], method = 'sgd') # fit model, retrieve parameters ws\n",
    "    train_losses[i,:] = np.array(l_tr)\n",
    "    l_ts = list(map(lambda x: compute_loss(y_test_sample, X_test_sample, x, lam = 0, method = \"logistic\"), ws)) # retrieve losses using test set with ws\n",
    "    test_losses[i,:] = np.array(l_ts)\n",
    "    pred_ytrain = list(map(lambda x: predict_labels(x, X_train_sample), ws)) # Training prediction\n",
    "    train_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_train_sample), pred_ytrain))\n",
    "    pred_ytest = list(map(lambda x: predict_labels(x, X_test_sample), ws)) # Test prediction\n",
    "    test_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_test_sample), pred_ytest))\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    titre = \"learning rate \"+str(lr[i])\n",
    "    plotCurves(train_losses[i,:], train_acc[i,:], test_losses[i,:], test_acc[i,:], titre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_iter = 20000\n",
    "\n",
    "lr = [0.00005,0.0001,0.0005] \n",
    "\n",
    "\n",
    "test_losses  = np.zeros([len(lr),(num_iter+1)])\n",
    "test_acc = np.zeros([len(lr),(num_iter+1)])\n",
    "train_losses = np.zeros([len(lr),(num_iter)])\n",
    "train_acc= np.zeros([len(lr),(num_iter+1)])\n",
    "\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    \n",
    "    l_tr, ws = logistic_regression(y_train_sample, X_train_sample, initial_w, max_iters = num_iter, gamma = lr[i], method = 'sgd') # fit model, retrieve parameters ws\n",
    "    train_losses[i,:] = np.array(l_tr)\n",
    "    l_ts = list(map(lambda x: compute_loss(y_test_sample, X_test_sample, x, lam = 0, method = \"logistic\"), ws)) # retrieve losses using test set with ws\n",
    "    test_losses[i,:] = np.array(l_ts)\n",
    "    pred_ytrain = list(map(lambda x: predict_labels(x, X_train_sample), ws)) # Training prediction\n",
    "    train_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_train_sample), pred_ytrain))\n",
    "    pred_ytest = list(map(lambda x: predict_labels(x, X_test_sample), ws)) # Test prediction\n",
    "    test_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_test_sample), pred_ytest))\n",
    "    \n",
    "for i in range(len(lr)):\n",
    "    titre = \"learning rate \"+str(lr[i])\n",
    "    plotCurves(train_losses[i,:], train_acc[i,:], test_losses[i,:], test_acc[i,:], titre)\n",
    "    print(\"best test accuracy = {0}\".format(max(test_acc[i,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression using Newton's method\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_w = np.random.rand(X_train.shape[1])\n",
    "# losses, ws = logistic_hessian(y_train, X_train, initial_w) # fit model, retrieve parameters ws\n",
    "# test_losses = list(map(lambda x: compute_loss(y_test, X_test, x, method = 'MSE'), ws)) # retrieve losses using test set with ws\n",
    "\n",
    "# plt.style.use('seaborn')\n",
    "# plt.plot(losses, label='Training set loss', c='blue')\n",
    "# plt.plot(test_losses, label='Test set loss', c='red')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Logistic Regression: Cost Function Loss Over Iterations of Stochastic Gradient Descent')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Make plot with label prediction accuracy\n",
    "\n",
    "# pred_ytrain = list(map(lambda x: predict_labels(x, X_train), ws)) # Training prediction\n",
    "# pred_accuracytrain = list(map(lambda x: pred_accuracy(x, y_train), pred_ytrain))\n",
    "# pred_ytest = list(map(lambda x: predict_labels(x, X_test), ws)) # Test prediction\n",
    "# pred_accuracytest = list(map(lambda x: pred_accuracy(x, y_test), pred_ytest))\n",
    "\n",
    "\n",
    "# plt.plot(pred_accuracytrain, label='Training set prediction accuracy', c='blue')\n",
    "# plt.plot(pred_accuracytest, label='Test set prediction accuracy', c='red')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Prediction Accuracy')\n",
    "# plt.title('Logistic Regression Prediction Accuracy Over Iterations of Gradient Descent')\n",
    "# plt.show()\n",
    "\n",
    "\"\"\"IF YOU READ THIS: I tried doing something here so that LR is faster and more accurate,\n",
    "the problem is that we can't calculate things that way, it's too big, but maybe I'll find a way\n",
    "another time.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-10,0, 11)\n",
    "loss_tr = []\n",
    "loss_ts = []\n",
    "pred_tr = []\n",
    "pred_ts = []\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "\n",
    "# There is a runtime warning but just be patient\n",
    "\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    \n",
    "    losses, ws = reg_logistic_regression(y_train_sample, X_train_sample, initial_w, lamb = lambda_, methods ='gd', gamma = 0.0001, max_iters=5000)\n",
    "    loss_tr.append(losses[-1])\n",
    "    best_w = ws[-1]\n",
    "    \n",
    "    pred_tr.append(pred_accuracy(predict_labels_logistic(best_w, X_train_sample),y_train_sample))\n",
    "    \n",
    "    test_losses = compute_loss(y_test_sample, X_test_sample, best_w, lam = lambda_, method = 'reg_logistic') # retrieve losses using test set with ws\n",
    "    loss_ts.append(test_losses)\n",
    "    pred_ts.append(pred_accuracy(predict_labels_logistic(best_w, X_test_sample),y_test_sample))\n",
    "\n",
    "    \n",
    "plt.style.use('seaborn')\n",
    "plt.semilogx(lambdas,loss_tr, c='blue')\n",
    "plt.semilogx(lambdas,loss_ts, c='red')\n",
    "plt.legend(['training set', 'testing set'], loc='upper left')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.semilogx(lambdas,pred_tr, c='blue')\n",
    "plt.semilogx(lambdas,pred_ts, c='red')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['training set', 'testing set'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lamb = 0.1\n",
    "# losses, ws = reg_logistic_regression(y_train, X_train, \n",
    "#                          initial_w, lamb, methods = 'gd', \n",
    "#                          gamma = 0.01, max_iters= 6000)\n",
    "\n",
    "# test_losses = list(map(lambda x: compute_loss(y_test, X_test, x, lam = lamb, method = 'reg_logistic'), ws)) # retrieve losses using test set with ws\n",
    "# plt.style.use('seaborn')\n",
    "# plt.plot(losses, label='Training set loss', c='blue')\n",
    "# plt.plot(test_losses, label='Test set loss', c='red')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Regularized Logistic Regression: Cost Function Loss Over Iterations of Gradient Descent')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Make plot with label prediction accuracy\n",
    "\n",
    "# pred_ytrain = list(map(lambda x: predict_labels(x, X_train), ws)) # Training prediction\n",
    "# pred_accuracytrain = list(map(lambda x: pred_accuracy(x, y_train), pred_ytrain))\n",
    "# pred_ytest = list(map(lambda x: predict_labels(x, X_test), ws)) # Test prediction\n",
    "# pred_accuracytest = list(map(lambda x: pred_accuracy(x, y_test), pred_ytest))\n",
    "\n",
    "\n",
    "# plt.plot(pred_accuracytrain, label='Training set prediction accuracy', c='blue')\n",
    "# plt.plot(pred_accuracytest, label='Test set prediction accuracy', c='red')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Prediction Accuracy')\n",
    "# plt.title('Regularized Logistic Regression Prediction Accuracy Over Iterations of Gradient Descent')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the polynomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 12\n",
    "X_test_poly = build_poly(data_test,degree)\n",
    "X_train_poly = build_poly(data,degree)\n",
    "w = least_squares(labels, X_train_poly)\n",
    "rmse_tr = (np.sqrt(2 * compute_loss(labels, X_train_poly, w)))\n",
    "pred_tr = pred_accuracy(predict_labels(w,X_train_poly),labels)\n",
    "\n",
    "test_prediction = predict_labels(w, X_test_poly)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"prediction.csv\"\n",
    "ids = ids_test\n",
    "y_pred = test_prediction\n",
    "create_csv_submission(ids, y_pred, name)\n",
    "\n",
    "# this yielded like 0.62 on kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.ones(tx.shape[1])\n",
    "y[y == -1] = 0\n",
    "losses, ws = logistic_regression(y, tx, initial_w, method = 'sgd', max_iters = 6000) # fit model, retrieve parameters ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tx_t.dot(ws[-1])\n",
    "pred[np.where(pred <= 0.5)] = -1\n",
    "pred[np.where(pred > 0.5)] = 1\n",
    "\n",
    "name = \"prediction.csv\"\n",
    "create_csv_submission(t_ids, pred, name)\n",
    "\n",
    "# this yields 0.73 on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.ones(tx.shape[1])\n",
    "y[y == -1] = 0\n",
    "losses, ws = reg_logistic_regression(y, tx, initial_w, lamb = 0.1, methods = 'sgd', max_iters = 20000, gamma = 0.0001)\n",
    "\n",
    "pred = tx_t.dot(ws[-1])\n",
    "pred[np.where(pred <= 0.5)] = -1\n",
    "pred[np.where(pred > 0.5)] = 1\n",
    "\n",
    "name = \"prediction_2.csv\"\n",
    "create_csv_submission(t_ids, pred, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
