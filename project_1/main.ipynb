{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation of modules and functions\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "warnings.simplefilter(\"error\")\n",
    "\n",
    "# Functions\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "\n",
    "# Autoreload\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels_raw, data_raw, ids_raw) = load_csv_data(\"data/train.csv\")\n",
    "(t_labels, t_data_raw, t_ids) = load_csv_data(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data filtering and standardization\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dimensions of the training data set was 250000 samples and 30 columns\n",
      " After feature and sample filtering, there are 250000 samples and 23 columns\n"
     ]
    }
   ],
   "source": [
    "data_, data_t_, labels = process_data(data_raw, t_data_raw, labels_raw, ids_raw,sample_filtering = False, feature_filtering = True, replace = 'mean')\n",
    "\n",
    "# Should not standardize all this stuff in the beginning! Only at the very end after the model has been selected!!\n",
    "# Check down two inputs, where the sets are standardized.\n",
    "\n",
    "# data, means, variance = standardize(data_)\n",
    "# need to standardize test using moments calculated from training\n",
    "# data_t = standardize_test(data_t_, means, variance)\n",
    "\n",
    "data = data_.copy()\n",
    "data_t = data_t_.copy()\n",
    "\n",
    "# perform PCA\n",
    "\n",
    "# eigVal, eigVec, sumEigVal = PCA(data, threshold = 0.7)\n",
    "# data = data.dot(eigVec)\n",
    "# data_t = data_t.dot(eigVec)\n",
    "# print(\"we have reduce the number of feature with PCA to {0}\".format(eigVec.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build data into matrix form\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tx = build_model_data(data, labels)\n",
    "y_t, tx_t = build_model_data(data_t,t_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into train and test set\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized and randomized samples are found as the variables X_train, y_train, X_test, y_test. Values are split for testing and training sets with the ratio of 0.8\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = split_data(tx, y)\n",
    "\n",
    "# Create the standardized X_train for use of testing the model on the test set. Note that this should\n",
    "# Not be done before crossvalidation, as the crossvalidation function standardizes the sets within the loops\n",
    "\n",
    "X_train_standardized, means, variance = standardize(X_train)\n",
    "X_test_standardized = standardize_test(X_test, means, variance)\n",
    "\n",
    "print('Standardized and randomized samples are found as the variables X_train, y_train, X_test, y_test. Values' +\n",
    "      ' are split for testing and training sets with the ratio of 0.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression with gradient descent (GD)\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the 4 cross validation test sets is 3.2649763619044094 with variance 0.0007127728951152243. Compare this to the training set mean loss of 3.2641243599992364\n"
     ]
    }
   ],
   "source": [
    "# Initialize w vector\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "k_ = 4\n",
    "\n",
    "# Perform linear regression by gradient descent with cross validation (k=4)\n",
    "test_loss_mean, test_loss_var, vector_test_loss, train_loss_mean, w = least_squares_GD(y_train, X_train, initial_w, gamma = 0.00001, k=k_, max_iters = 2000)\n",
    "\n",
    "# Plot the k trials losses over iterations\n",
    "\n",
    "means_over_time = vector_test_loss.mean(axis=0)\n",
    "error1 = abs(means_over_time - vector_test_loss[0])\n",
    "error2 = abs(means_over_time - vector_test_loss[1])\n",
    "error3 = abs(means_over_time- vector_test_loss[2])\n",
    "error4 = abs(means_over_time - vector_test_loss[3])\n",
    "x = np.arange(len(error1))\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "# for i in np.arange(4):\n",
    "#     plt.plot(x, vector_test_loss[i], label= 'Trial {}'.format(i))\n",
    "plt.plot(x, vector_test_loss[0], label='Trial 1', c='red')\n",
    "plt.fill_between(x, vector_test_loss[0]-error1, vector_test_loss[0]+error1,\n",
    "    alpha=0.2, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "plt.plot(x, vector_test_loss[1], label='Trial 2', c = 'green')\n",
    "plt.fill_between(x, vector_test_loss[1] - error2, vector_test_loss[1] + error2, alpha = 0.2, edgecolor='#FF3F1B', facecolor = '#12E99F')\n",
    "plt.plot(x, vector_test_loss[2], label='Trial 3', c='blue')\n",
    "plt.fill_between(x, vector_test_loss[2]-error3, vector_test_loss[2]+error3,\n",
    "    alpha=0.2, edgecolor='#CC4F1B', facecolor='#12E2FF')\n",
    "plt.plot(x, vector_test_loss[3], label='Trial 4', c='purple')\n",
    "plt.fill_between(x, vector_test_loss[3]-error4, vector_test_loss[3]+error4,\n",
    "    alpha=0.2, edgecolor='#CC4F1B', facecolor='#FF00FF')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "# plt.xlim((200, 250))\n",
    "# plt.ylim(10,30)\n",
    "plt.title('Test Set Loss Over Iterations of Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('The mean of the {} cross validation test sets is {} with variance {}. \\\n",
    "Compare this to the training set mean loss of {}'.format(k_,test_loss_mean, test_loss_var,train_loss_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.38047249e-01 -2.11443079e-03 -1.60785716e-02 -5.33425788e-03\n",
      " -1.62886384e-02  2.11578935e-01 -9.34999859e-04 -2.80849531e-01\n",
      "  2.96991542e-01  4.91087216e-01  3.06763852e-01  6.68414615e-01\n",
      "  4.52229478e-01  2.97120495e-01  6.96310521e-01  8.57002338e-01\n",
      "  2.02551565e-02  2.06075889e-02 -4.84365228e-04  1.78296904e-01\n",
      "  3.11455446e-03  2.05205919e-01  9.29404932e-01  2.78535567e-01]\n",
      "0.56028 0.56273\n"
     ]
    }
   ],
   "source": [
    "# Performing a test for prediction accuracy\n",
    "\n",
    "w = least_squares_GD(y_train, X_train_standardized, initial_w, gamma = 0.00001, k=0, max_iters = 2000)\n",
    "test_pred_lab = predict_labels(w, X_test_standardized)\n",
    "train_pred_lab = predict_labels(w, X_train_standardized)\n",
    "test_accuracy = pred_accuracy(test_pred_lab, y_test)\n",
    "train_accuracy = pred_accuracy(train_pred_lab, y_train)\n",
    "print(test_accuracy, train_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bar chart shows potential variance in the test set. The difference between test and training accuracies is 0.002449999999999952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Percent Accuracy')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot bar chart\n",
    "\n",
    "print('The bar chart shows potential variance in the test set. \\\n",
    "The difference between test and training accuracies is {}'.format(train_accuracy - test_accuracy))\n",
    "names = ['Test Accuracy', 'Train Accuracy']\n",
    "values = [test_accuracy * 100, train_accuracy * 100]\n",
    "plt.style.use('seaborn')\n",
    "plt.bar(names, values, width = 0.3, yerr = [test_loss_var, 0])\n",
    "plt.ylabel('Percent Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression with stochastic gradient descent (SGD)\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_w = np.ones(X_train.shape[1])\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "losses, ws = least_squares_SGD(y_train, X_train, initial_w, max_iters = 250, tol = 1e-4, patience = 5) # fit model, retrieve parameters ws\n",
    "test_losses = list(map(lambda x: compute_loss(y_test, X_test, x, method = 'MSE'), ws)) # retrieve losses using test set with ws\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(losses, label='Training set loss', c='blue')\n",
    "plt.plot(test_losses, label='Test set loss', c='red')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Cost Function Loss Over Iterations of Stochastic Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make plot with label prediction accuracy\n",
    "\n",
    "pred_ytrain = list(map(lambda x: predict_labels(x, X_train), ws)) # Training prediction\n",
    "pred_accuracytrain = list(map(lambda x: pred_accuracy(x, y_train), pred_ytrain))\n",
    "pred_ytest = list(map(lambda x: predict_labels(x, X_test), ws)) # Test prediction\n",
    "pred_accuracytest = list(map(lambda x: pred_accuracy(x, y_test), pred_ytest))\n",
    "\n",
    "\n",
    "plt.plot(pred_accuracytrain, label='Training set prediction accuracy', c='blue')\n",
    "plt.plot(pred_accuracytest, label='Test set prediction accuracy', c='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Prediction Accuracy')\n",
    "plt.title('Prediction Accuracy Over Iterations of Stochastic Gradient Descent')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression using least squares normal equations\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bar chart shows potential variance in the test set. The difference between test and training accuracies is 0.0023849999999999705\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFXCAYAAACV2fZmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xl8TPf+x/H3JCNRwhXrVaqtvZSGW0tp1FJVLa2txBK06CZqqz3WEsGtKq2traXWaHmgbpcf2tsQlbqu9Icf1WrtqtHkYoJMkvn+/uijc28uMUHiS/J6/pU558yZT6Z6XjknMxOHMcYIAADccn62BwAAIL8iwgAAWEKEAQCwhAgDAGAJEQYAwBIiDACAJU7bAwD5zeTJk7Vr1y5J0uHDh1WuXDkVLFhQkhQTE+P9OruMMXr++ec1e/ZsFS1aVH369NGYMWNUsWLFm57VGKOFCxfq008/lTFGHo9HTZo00eDBg1WgQIFr3nf16tWSpLCwsJueA8irHLxPGLCnefPmevvtt1WrVq0b3kd6erpq1qypXbt2qWjRojk4nbRp0yatWLFCS5YsUWBgoC5fvqyIiAjVqlVLAwcOvOZ9X3/9dT344IPq3bt3js4E5CWcCQO3mR9++EFTpkzR+fPnlZGRod69e6t9+/ZyuVwaNWqUjh07Jj8/P9WqVUsTJ07UqFGjJEndu3fX+++/r+eee04LFixQcnKy3nnnHd1999368ccflZGRoUmTJqlOnTo6e/asRo0apZMnT6pYsWIqXry4atSooVdffTXTLImJifJ4PLp8+bICAwNVsGBBjR8/XsnJyZIkt9ut6dOna/fu3crIyFDNmjU1ZswYbd++XbGxsYqPj1dgYKC6du16y59H4E7A74SB20haWpoGDhyokSNHat26dVq2bJkWLFigvXv36osvvpDb7daGDRv00UcfKT09XSdOnNDUqVMlSStWrFCZMmUy7e+7775Tv379tH79erVt21azZs2SJE2aNEk1atTQp59+qrfeekt79uy56jwdO3bUXXfdpcaNGyssLEzTpk3TmTNnVLt2bUnSvHnzVLBgQa1bt04bN25UcHCwZs2apSeffFJNmjRRnz59CDBwDZwJA7eRw4cP6/jx4xoxYoR3mdvt1oEDB9SwYUO9/fbb6tmzpxo1aqQ+ffronnvuUXp6epb7K1++vKpVqyZJqlmzpj799FNJUmxsrPcxypQpo5YtW171/kWLFtWSJUt07NgxxcfHKz4+Xv369VPPnj01ePBg/f3vf9fFixe1bds2Sb//EFG6dOkceS6A/IAIA7cRj8ejYsWKacOGDd5liYmJKlq0qAIDA7V582bFx8dr586d6tWrl6ZMmaLGjRtnub//fJGXw+HQHy8B8ff313++HMTf3/+q91+4cKHq16+vkJAQVahQQc8995zi4+PVv39/DR48WBkZGRo3bpx3BpfLpbS0tJt6DoD8hMvRwG2kcuXK8vPz09/+9jdJ0smTJ9WmTRsdPHhQy5Yt09ixYxUaGqrhw4erYcOG+r//+z/5+/vL4XBc84z4vzVt2lQff/yxJCkpKUlbt26Vw+G4YruUlBS9+eabOnfunHfZoUOHVKNGDUnSo48+qmXLliktLU0ZGRkaPXq095K30+m8rpmA/IgzYeA2EhAQoHnz5ikqKkrz589Xenq6hg4dqoceekiVKlXSrl279PTTT6tgwYIqV66cunfvLofDoSeeeEJdu3bV3Llzs/U4Y8aMUWRkpNq2bavg4GDdfffduuuuu67Y7rXXXtPcuXPVuXNn+fn5yePxqFatWpo5c6YkacCAAZo2bZratWvnfWHW8OHDJUlNmjTRjBkzJEl9+/bNoWcIyFt4ixKQDy1fvly1atXSQw89pNTUVHXt2lVDhw695qVtADmPM2EgH6pUqZImTpwoj8ejtLQ0tW7dmgADFnAmDACAJbwwCwAAS4gwAACWEGEAACy55S/MSky8cKsf8rYVHFxIyckXbY8BADmO49u/lSpVJMt1nAlb5HRe/VOKAOBOx/Ete4gwAACWEGEAACwhwgAAWEKEAQCwhI+tBIDb2AvRX9oeId9ZNLL5LXusOz7C/AO141b+IwWAvIrL0QAAWEKEAQCwhAgDAGAJEQYAwBIiDACAJUQYAABLiDAAAJYQYQAALCHCAABYQoQBALCECAMAYAkRBgDAEiIMAIAlRBgAAEuIMAAAllzX3xNesGCBvvzyS6Wlpalr166qX7++Ro4cKYfDoSpVqmj8+PHy86PrAABkR7aLGR8frz179mjVqlVatmyZfvnlF02dOlWDBg3SypUrZYzR1q1bc3NWAADylGxHePv27apatar69++vl19+WU2bNtX+/ftVv359SVKTJk20Y8eOXBsUAIC8JtuXo5OTk3Xq1CnNnz9fJ06c0CuvvCJjjBwOhySpcOHCunDhgs/9BAcXktPpf+MT47ZQqlQR2yMAQK64lce3bEe4WLFiqlixogICAlSxYkUFBgbql19+8a5PSUlR0aJFfe4nOfnijU2K20piou8fuADgTpTTx7drRT3bl6P/8pe/aNu2bTLG6MyZM7p06ZIeeeQRxcfHS5JiY2P18MMP3/y0AADkE9k+E27WrJl27dqlTp06yRijcePGqXz58ho7dqxmzpypihUrqlWrVrk5KwAAecp1vUVp+PDhVyxbvnx5jg0DAEB+wpt6AQCwhAgDAGAJEQYAwBIiDACAJUQYAABLiDAAAJYQYQAALCHCAABYQoQBALCECAMAYAkRBgDAEiIMAIAlRBgAAEuIMAAAlhBhAAAsIcIAAFhChAEAsIQIAwBgCREGAMASIgwAgCVEGAAAS4gwAACWEGEAACwhwgAAWEKEAQCwhAgDAGAJEQYAwBIiDACAJdcd4d9++02PPfaYDh8+rKNHj6pr167q1q2bxo8fL4/HkxszAgCQJ11XhNPS0jRu3DgVLFhQkjR16lQNGjRIK1eulDFGW7duzZUhAQDIi64rwtOmTVNYWJhKly4tSdq/f7/q168vSWrSpIl27NiR8xMCAJBHObO74bp161S8eHGFhoZq4cKFkiRjjBwOhySpcOHCunDhgs/9BAcXktPpf4Pj4nZRqlQR2yMAQK64lce3bEd47dq1cjgc+uabb3TgwAGNGDFCSUlJ3vUpKSkqWrSoz/0kJ1+8sUlxW0lM9P0DFwDciXL6+HatqGc7witWrPB+HR4ergkTJmjGjBmKj49XgwYNFBsbq4YNG97cpAAA5CM39RalESNGaM6cOerSpYvS0tLUqlWrnJoLAIA8L9tnwv9p2bJl3q+XL1+eY8MAAJCf8GEdAABYQoQBALCECAMAYAkRBgDAEiIMAIAlRBgAAEuIMAAAlhBhAAAsIcIAAFhChAEAsIQIAwBgCREGAMASIgwAgCVEGAAAS4gwAACWEGEAACwhwgAAWEKEAQCwhAgDAGAJEQYAwBIiDACAJUQYAABLiDAAAJYQYQAALCHCAABYQoQBALCECAMAYAkRBgDAEiIMAIAlzuxumJaWptGjR+vkyZNyu9165ZVXVLlyZY0cOVIOh0NVqlTR+PHj5edH1wEAyI5sR3jjxo0qVqyYZsyYoeTkZLVv317Vq1fXoEGD1KBBA40bN05bt25Vy5Ytc3NeAADyjGyftj755JMaOHCg97a/v7/279+v+vXrS5KaNGmiHTt25PyEAADkUdk+Ey5cuLAkyeVy6bXXXtOgQYM0bdo0ORwO7/oLFy743E9wcCE5nf43OC5uF6VKFbE9AgDkilt5fMt2hCXp9OnT6t+/v7p166a2bdtqxowZ3nUpKSkqWrSoz30kJ1+8/ilx20lM9P0DFwDciXL6+HatqGf7cvTZs2f1wgsvaNiwYerUqZMkqUaNGoqPj5ckxcbG6uGHH77JUQEAyD+yHeH58+fr/Pnzmjt3rsLDwxUeHq5BgwZpzpw56tKli9LS0tSqVavcnBUAgDwl25ejIyMjFRkZecXy5cuX5+hAAADkF7ypFwAAS4gwAACWEGEAACwhwgAAWEKEAQCwhAgDAGAJEQYAwBIiDACAJUQYAABLiDAAAJYQYQAALCHCAABYQoQBALCECAMAYAkRBgDAEiIMAIAlRBgAAEuIMAAAlhBhAAAsIcIAAFhChAEAsIQIAwBgCREGAMASIgwAgCVEGAAAS4gwAACWEGEAACwhwgAAWEKEAQCwxHmzO/B4PJowYYK+//57BQQEaPLkybr33ntzYjYAAPK0mz4T3rJli9xut2JiYjR06FBFR0fnxFwAAOR5Nx3h3bt3KzQ0VJIUEhKiffv23fRQAADkBzd9OdrlcikoKMh729/fX+np6XI6r77rUqWK3OxDZvLJm8/m6P4A4HbCMS5vu+kz4aCgIKWkpHhvezyeLAMMAAD+7aYjXLduXcXGxkqSEhISVLVq1ZseCgCA/MBhjDE3s4M/Xh196NAhGWMUFRWlSpUq5dR8AADkWTcdYQAAcGP4sA4AACwhwgAAWEKEAQCwhAgDAGAJEQYAwBIiDACAJUQYAABLiDAAAJYQYUBSRkaGFi9erA4dOujZZ5/VU089pRkzZsjtdt+yGY4fP64HH3xQZ86cuWJd27ZttXnz5mvev1q1akpKStLWrVs1efLkq27Tpk0bxcfH+5xjwIABkqQzZ84oLCwsm9+Bb+Hh4fr8889zbH/AnY4IA5ImTJigPXv2aOnSpdqwYYM+/vhj/fzzzxozZswtm+Gee+5Ro0aNtG7dukzL9+zZowsXLqh58+bZ2k+LFi0UGRl5w3OcOnVKP//8sySpTJkyWr169Q3vC8C18eeOkO+dOHFCn3zyibZv3+79s5yFChXSxIkT9c9//lOSNHLkSP3rX//S8ePH1bRpU7388suaOHGiDh48KIfDodDQUA0ZMkROp1OzZ8/W5s2bVaBAAQUHB2vq1KkqXbp0lsv/U/fu3TV58mS9/PLLcjgckqQ1a9YoLCxM/v7++vnnnzVp0iSlpKQoMTFR1atX16xZsxQYGOjdx7p16/TFF19owYIF+vHHHzV69GhdunRJFStW1MWLF73bzZ8/X1u3btXly5d16dIljRgxQs2bN1dkZKTOnDmjPn36aOLEiWrbtq327NmjtLQ0RUdH65tvvpG/v79q166tUaNGKSgoSM2bN1f79u31zTff6PTp03r22Wc1aNCg6/rvEBMTo2XLlsnPz08lS5bU2LFjdf/99+sf//iHoqOj5fF4JEkvvfSSWrVqleVy4I5igHzu888/Nx07drzmNiNGjDC9evXy3h4+fLh54403jMfjMampqeaFF14wCxYsMKdOnTJ169Y1qampxhhjPvjgA7N58+Ysl/+3jIwM06JFC7Nz505jjDHnz5839erVM2fPnjXGGBMdHW3Wr19vjDHG7XabNm3amM8//9wYY0zVqlXNb7/9ZtauXWtefPFFY4wxzz77rFmzZo0xxph//OMfplq1ambnzp3mxIkTJjw83Fy6dMkYY8ymTZtMmzZtjDHG7Ny50zz99NPGGGOOHz9uQkJCjDHGvP322yYiIsK43W6TkZFhRo4cacaOHWuMMaZZs2YmOjraGGPML7/8YmrVqmWOHTt2xffXo0cP89lnn12xfMeOHebxxx83v/32mzHGmLVr15rWrVsbj8djevbsaTZt2mSMMebAgQNmwoQJxhiT5XLgTsKZMPI9Pz8/79nUtfzlL3/xfh0bG6tVq1bJ4XAoICBAYWFhWrp0qfr27avq1aurffv2atKkiZo0aaJHHnlEHo/nqsuvNktYWJjWrl2rBg0aaOPGjXrsscdUokQJSdKwYcMUFxen9957T0eOHNGvv/6a6ez2PyUnJ+v7779Xu3btvPNXqVJFklSuXDlNnz5dn3zyiY4eParvvvsu098Fv5rY2FgNHjxYBQoUkPT773f79+/vXd+iRQtJv1/CLlGihM6dO6d77rnH5/MqSdu2bdNTTz2l4sWLS5I6dOigKVOm6MSJE2rdurUmTZqkL7/8Uo0aNdKQIUMkKcvlwJ2E3wkj36tdu7Z++uknuVyuTMvPnDmjF198UZcvX5b0+yXqP3g8Hu/l4j9up6eny8/PT8uXL9fUqVNVrFgxRUVFafr06Vkuv5qOHTvq66+/lsvl0po1a9S9e3fvuiFDhmjNmjUqV66cevfurZo1a8r4+ENo/7ne6fz95+79+/erS5cucrlcaty4sfr27evzebra95yWlua9/Z+XxB0Oh8+5/nvfV5s7PT1dYWFh2rhxoxo3bqzt27frmWeeUWpqapbLgTsJEUa+V6ZMGbVt21ajR4/2htjlcmnChAkqVqyYChYseMV9Hn30US1fvlzGGLndbq1Zs0aNGjXSwYMH1aZNG1WqVEkvvfSSevfurb1792a5/GqCg4PVrFkzzZ49W/7+/goJCfGu2759u/r376+nnnpKkvTdd98pIyMjy/3UrFlTH330kaTfw3vo0CFJ0q5du/Tggw/q+eefV/369bV161bvfvz9/TPF9Q+hoaFatWqV0tLS5PF4tGLFCjVu3Di7T/M1hYaG6tNPP1VSUpIkae3atSpWrJjuvfdehYWF6cCBA+rQoYPeeOMNnT9/XomJiVkuB+4kXI4GJI0fP15z5871vgDK7Xbr8ccf975V579FRkZq8uTJatu2rdLS0hQaGqqXX35ZAQEBat26tTp27KhChQqpYMGCioyMVPXq1a+6PCvdunVT586dNWXKlEzLBw8erP79+6tQoUIKCgpSvXr1dOzYsSz3M3PmTI0aNUqrV69WhQoVVLFiRUm/v1Xpf/7nf9S6dWt5PB41a9ZM586dk8vlUuXKlRUYGKhOnTrprbfe8u7rlVde0bRp09SuXTulp6erdu3aGjt27PU8zZKk4cOHa9SoUZm+12HDhql3797q1auXPB6PihcvrgULFsjPz0+vv/66oqKiNGvWLDkcDkVERKh8+fJZLgfuJA5zPdeMAABAjuFyNAAAlhBhAAAsIcIAAFhChAEAsIQIAwBgyS1/i1Ji4oVb/ZC3reDgQkpOvvqnHQHAnYzj27+VKlUky3WcCVvkdPrbHgEAcgXHt+whwgAAWEKEAQCwxGeEPR6Pxo0bpy5duig8PFxHjx696jZ9+/bVqlWrcmVIAADyIp8R3rJli9xut2JiYjR06FBFR0dfsc2sWbN07ty5XBkQAIC8yuero3fv3q3Q0FBJUkhIiPbt25dp/eeffy6Hw6EmTZrkzoQ+vBD9pZXHze8WjWxuewQgX+AYd+vdyuObzwi7XC4FBQV5b/v7+ys9PV1Op1OHDh3Spk2bNHv2bL377rvZesDg4EK8ai4PuNZL7gHgTnYrj28+IxwUFKSUlBTvbY/H4/3D4OvXr9eZM2fUq1cvnTx5UgUKFFC5cuWueVbM+8byBt7vDSCvyunj27Wi7jPCdevW1VdffaWnnnpKCQkJqlq1qnfd8OHDvV/PmTNHJUuWtHZZGgCAO43PCLds2VJxcXEKCwuTMUZRUVFavHixKlSooBYtWtyKGQEAyJN8RtjPz0+TJk3KtKxSpUpXbDdgwICcmwoAgHyAD+sAAMASIgwAgCVEGAAAS4gwAACWEGEAACwhwgAAWEKEAQCwhAgDAGAJEQYAwBIiDACAJUQYAABLiDAAAJYQYQAALCHCAABYQoQBALCECAMAYAkRBgDAEiIMAIAlRBgAAEuIMAAAlhBhAAAsIcIAAFhChAEAsIQIAwBgic8IezwejRs3Tl26dFF4eLiOHj2aaf2KFSvUsWNHderUSV999VWuDQoAQF7j9LXBli1b5Ha7FRMTo4SEBEVHR2vevHmSpKSkJK1cuVLr169Xamqqnn76aTVt2lQOhyPXBwcA4E7n80x49+7dCg0NlSSFhIRo37593nXFixfXhg0bVKBAAZ09e1ZFixYlwAAAZJPPM2GXy6WgoCDvbX9/f6Wnp8vp/P2uTqdTy5cv15w5cxQeHu7zAYODC8np9L+JkXE7KFWqiO0RACBX3Mrjm88IBwUFKSUlxXvb4/F4A/yHHj16qHPnzurXr5927typhg0bZrm/5OSLNzEubheJiRdsjwAAuSKnj2/XirrPy9F169ZVbGysJCkhIUFVq1b1rvvpp58UEREhY4wKFCiggIAA+fnxgmsAALLD55lwy5YtFRcXp7CwMBljFBUVpcWLF6tChQpq0aKFqlevri5dusjhcCg0NFT169e/FXMDAHDH8xlhPz8/TZo0KdOySpUqeb+OiIhQREREzk8GAEAex7VjAAAsIcIAAFhChAEAsIQIAwBgCREGAMASIgwAgCVEGAAAS4gwAACWEGEAACwhwgAAWEKEAQCwhAgDAGAJEQYAwBIiDACAJUQYAABLiDAAAJYQYQAALCHCAABYQoQBALCECAMAYAkRBgDAEiIMAIAlRBgAAEuIMAAAljh9beDxeDRhwgR9//33CggI0OTJk3Xvvfd61y9ZskR/+9vfJEmPPfaYIiIicm9aAADyEJ9nwlu2bJHb7VZMTIyGDh2q6Oho77rjx49r48aNWr16tWJiYrR9+3YdPHgwVwcGACCv8HkmvHv3boWGhkqSQkJCtG/fPu+6P//5z3r//ffl7+8vSUpPT1dgYGAujQoAQN7iM8Iul0tBQUHe2/7+/kpPT5fT6VSBAgVUvHhxGWM0ffp01ahRQ/fff/819xccXEhOp//NTw6rSpUqYnsEAMgVt/L45jPCQUFBSklJ8d72eDxyOv99t9TUVI0ePVqFCxfW+PHjfT5gcvLFGxwVt5PExAu2RwCAXJHTx7drRd3n74Tr1q2r2NhYSVJCQoKqVq3qXWeM0auvvqpq1app0qRJ3svSAADAN59nwi1btlRcXJzCwsJkjFFUVJQWL16sChUqyOPx6Ntvv5Xb7da2bdskSUOGDFGdOnVyfXAAAO50PiPs5+enSZMmZVpWqVIl79d79+7N+akAAMgH+LAOAAAsIcIAAFhChAEAsIQIAwBgCREGAMASIgwAgCVEGAAAS4gwAACWEGEAACwhwgAAWEKEAQCwhAgDAGAJEQYAwBIiDACAJUQYAABLiDAAAJYQYQAALCHCAABYQoQBALCECAMAYAkRBgDAEiIMAIAlRBgAAEuIMAAAlhBhAAAs8Rlhj8ejcePGqUuXLgoPD9fRo0ev2CYpKUlPPPGEUlNTc2VIAADyIp8R3rJli9xut2JiYjR06FBFR0dnWr9t2za98MILOnv2bK4NCQBAXuQzwrt371ZoaKgkKSQkRPv27cu8Az8/LV68WMWKFcudCQEAyKOcvjZwuVwKCgry3vb391d6erqczt/v2rhx4+t6wODgQnI6/a9zTNxuSpUqYnsEAMgVt/L45jPCQUFBSklJ8d72eDzeAN+I5OSLN3xf3D4SEy/YHgEAckVOH9+uFXWfl6Pr1q2r2NhYSVJCQoKqVq2ac5MBAJCP+TylbdmypeLi4hQWFiZjjKKiorR48WJVqFBBLVq0uBUzAgCQJ/mMsJ+fnyZNmpRpWaVKla7Y7ssvv8y5qQAAyAf4sA4AACwhwgAAWEKEAQCwhAgDAGAJEQYAwBIiDACAJUQYAABLiDAAAJYQYQAALCHCAABYQoQBALCECAMAYAkRBgDAEiIMAIAlRBgAAEuIMAAAlhBhAAAsIcIAAFhChAEAsIQIAwBgCREGAMASIgwAgCVEGAAAS4gwAACW+Iywx+PRuHHj1KVLF4WHh+vo0aOZ1q9Zs0YdOnRQ586d9dVXX+XaoAAA5DVOXxts2bJFbrdbMTExSkhIUHR0tObNmydJSkxM1LJly7R27VqlpqaqW7duaty4sQICAnJ9cAAA7nQ+z4R3796t0NBQSVJISIj27dvnXfe///u/qlOnjgICAlSkSBFVqFBBBw8ezL1pAQDIQ3yeCbtcLgUFBXlv+/v7Kz09XU6nUy6XS0WKFPGuK1y4sFwu1zX3V6pUkWuuv16fvPlsju4PAG4nHOPyNp9nwkFBQUpJSfHe9ng8cjqdV12XkpKSKcoAACBrPiNct25dxcbGSpISEhJUtWpV77ratWtr9+7dSk1N1YULF3T48OFM6wEAQNYcxhhzrQ08Ho8mTJigQ4cOyRijqKgoxcbGqkKFCmrRooXWrFmjmJgYGWP00ksvqVWrVrdqdgAA7mg+IwwAAHIHH9YBAIAlRBgAAEt8vkUpr4uOjtb+/fuVmJioy5cv65577lFwcLBmz56d7X2cOHFCP/zwg5o1a5ZpeUJCgmbPni1jjDwej5o1a6bevXtnuZ/4+HgVL15cVapU8S47evSoRo4cqVWrVl339wYgb7uZ49eBAwe0detWRUREXHV9bGysTp8+rS5dutzwfF9//bUWLVokPz8/ZWRkqFOnTnrmmWey3H7z5s2qXbu2ypQp410WHx+v1atX66233rrhOW5rBsYYY9auXWtmzJhxQ/dds2aNmTlz5hXL27VrZ44cOWKMMcbtdpt27dqZAwcOZLmfoUOHmri4uEzLjhw5YsLCwm5oLgD5w80cv3JT06ZNzblz54wxxly4cME0b97cnD17Nsvte/ToYX788cdMy3bu3GkGDRqUq3PalO/PhK9l+vTp2rNnjzwej/r06aMnnnhCH374oT755BP5+fmpXr16eu211/T+++/L7XarTp06atq0qff+d999t5YtW6b27dvrgQceUExMjAICAuR2uzV+/HgdP35cGRkZGjp0qAICArRjxw4dOnRI7733XqafBK9m7969mjJlipxOpwIDAzV58mQFBwdr4MCBunjxoi5fvqzhw4fr4Ycf1ogRI3TixAldvnxZL774op588slcfuYA2BQfH6+//vWvKlCggDp37qyCBQtqxYoV3vVvv/22fvjhB+8Z5hNPPKG6devq559/VokSJTRnzhxt2LBBP/30k8LCwjR06FD9+c9/1vHjx1WrVi1NnDhRSUlJev311+V2u3X//fdr586d2rx5c6Y5SpQooQ8//FCtWrVS5cqV9dlnnykgIEAXLlzQmDFjlJycLEmKjIzU6dOndeDAAY0YMUIrV670+fHHcXFxmjVrlgIDA1WsWDFFRUUpPT1dgwYNkjFGaWlpmjhxou677z4NHDhQLpcbRHaFAAAF1klEQVRLly9f1rBhw9SgQYOcf9JvEBHOwpdffqkzZ85o1apVunz5sp577jk1atRI69at0xtvvKEHH3xQK1eulL+/v/r27asTJ05kCrD0+6WipUuXaty4cTpx4oTatm2r4cOHKyYmRqVLl9bUqVOVlJSknj17atOmTWrUqJE6dOjgM8CSNHbsWE2bNk3VqlXTF198oenTp+ull17Sv/71L33wwQc6e/asjh07pvPnz2v37t366KOPZIzRzp07c+kZA3A7SU1N1UcffSRJmj9/vhYuXKi77rpL48aN0/bt2zMdZ44fP66lS5eqbNmyCgsL0969ezPt68iRI/rggw9011136fHHH1diYqLee+89tWjRQt27d1dcXJzi4uKumGHevHlasmSJhgwZoqSkJIWFhSkiIkLz589Xw4YN1a1bNx05ckSjRo3SqlWr9MADD2jChAk+A2yM0dixY7Vq1SqVKVNGS5cu1bx589SgQQMVKVJEb775pn788Ue5XC4dO3ZMZ8+e1ZIlS/Tbb7/pyJEjN//k5iAinIVDhw5p3759Cg8PlyRlZGTo1KlTmjZtmhYtWqSTJ0+qbt26Mlm8w+vy5cs6cOCAIiIiFBERoaSkJI0aNUoff/yxDh06pISEBP3zn/+UJKWlpen8+fPXNd/Zs2dVrVo1SVK9evX0zjvvqHr16urcubOGDBmijIwM9ezZU3/60580atQoRUZGKiUlRe3bt7+JZwXAneL+++/3fl2iRAmNGDFChQsX1k8//aSQkJBM2wYHB6ts2bKSpLJlyyo1NTXT+goVKng/vrhUqVJKTU3V4cOHvceThx9++IrHP3funE6dOqVhw4Zp2LBhOnPmjAYMGKCaNWvq0KFD2rlzpz777DNJuu7jX3JysoKCgrw/SNSrV08zZ87UsGHDdOTIEb366qtyOp165ZVXVKVKFXXv3l1DhgxRenq695h+uyDCWahYsaIeeeQRTZgwQRkZGXr33XdVvnx5zZw5U2+88YYCAgLUq1cvfffdd3I4HFfE2OFw6PXXX9eiRYtUuXJlFS9eXGXLllVAQIAqVqyoChUqqF+/frp06ZLmz5+vIkWKyM/PTx6PJ1vzlSxZUj/88IOqVKmib7/9Vvfdd58OHDig1NRULVy4UKdPn1avXr1UrVo1ff/995o7d64uXbqkpk2b6plnnpGfHy+MB/KyP/4fv3DhgmbPnq2///3vkqTnn3/+qsera7na+qpVq2rPnj164IEHlJCQcMV6t9utQYMGaeXKlSpbtqxKlSqlkiVLeo+BzzzzjNq2bavffvvNe8Z+tWPp1QQHB8vlcunXX39V6dKlvcfA+Ph4lS5dWosWLdKePXs0c+ZM7wnIwoUL9euvvyosLOyKF9HaRISz0LJlS3377bfq1q2bLl68qFatWqlQoUKqVKmSOnbs6P3JsVatWgoICNB7772nBx54QK1bt5YkBQYGev8BZGRkSPr9r1C1a9dO6enpioyMVI8ePeRyudSjRw85HA499NBDmj59usqVK5fpp9iDBw+qQ4cO3tujR4/W5MmTNX78eEmS0+lUVFSUSpYsqXfeeUfr1q2T0+nUgAEDVLp0aZ0+fVrt27dXwYIF9eKLLxJgIB8JCgpS3bp11b59exUqVEhFixbVr7/+qvLly9/Ufvv166fhw4frs88+U+nSpb1/U+APpUqVUmRkpCIiIuR0OpWRkaGmTZvq0UcfVc2aNTVmzBitWbNGLpfL+wrtOnXqaPjw4Vq0aJGKFSvm3VdcXFymY+Cbb76pyZMna8CAAXI4HPrTn/6kqVOnyuFwaPDgwVq6dKn8/PzUv39/3XfffXr33Xe1fv16FShQQK+99tpNfd85jU/MAgBct6+//lrBwcGqXbu2duzYofnz5+vDDz+0PdYdhzNhAMB1K1++vEaPHi1/f395PB6NGTPG9kh3JM6EAQCwhF8OAgBgCREGAMASIgwAgCVEGAAAS4gwAACWEGEAACz5f6HogmwYUfP9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x396 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_ = 4\n",
    "test_loss_mean, test_loss_var, train_loss_mean, w_ = least_squares(y_train, X_train, k_)\n",
    "\n",
    "w = least_squares(y_train, X_train, k =0)\n",
    "test_pred_lab = predict_labels(w, X_test_standardized)\n",
    "train_pred_lab = predict_labels(w, X_train_standardized)\n",
    "test_accuracy = pred_accuracy(test_pred_lab, y_test)\n",
    "train_accuracy = pred_accuracy(train_pred_lab, y_train)\n",
    "# Two subplots, the axes array is 1-d\n",
    "\n",
    "print('The bar chart shows potential variance in the test set. \\\n",
    "The difference between test and training accuracies is {}'.format(train_accuracy - test_accuracy))\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "f, axarr = plt.subplots(2, sharex=True)\n",
    "axarr[0].bar(['Testing Prediction Accuracy', 'Training Prediction Accuracy'], [test_accuracy * 100, train_accuracy * 100], width=0.3, yerr=[test_loss_var, 0])\n",
    "axarr[0].set_title('Testing Set')\n",
    "axarr[1].set_title('Cross Validation Loss')\n",
    "axarr[1].bar(['Test Set Loss', 'Training Set Loss'], [test_loss_mean, train_loss_mean], width=0.3, yerr=[test_loss_var, 0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression using the normal equations with additional polynomial degrees\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1,15)\n",
    "rmse_tr = np.zeros(len(degrees))\n",
    "rmse_ts = np.zeros(len(degrees))\n",
    "pred_tr = np.zeros(len(degrees))\n",
    "pred_ts = np.zeros(len(degrees))\n",
    "\n",
    "\n",
    "for ind, degree in enumerate(degrees):\n",
    "    #train the model\n",
    "    X_test_poly = build_poly(X_test,degree)\n",
    "    X_train_poly = build_poly(X_train,degree)\n",
    "    w = least_squares(y_train, X_train_poly)\n",
    "    rmse_tr[ind] = (np.sqrt(2 * compute_loss(y_train, X_train_poly, w)))\n",
    "    pred_tr[ind] = pred_accuracy(predict_labels(w,X_train_poly),y_train)\n",
    "    \n",
    "    #test the model\n",
    "    \n",
    "    rmse_ts[ind] = (np.sqrt(2 * compute_loss(y_test, X_test_poly, w)))\n",
    "    pred_ts[ind] = (pred_accuracy(predict_labels(w, X_test_poly),y_test))\n",
    "    \n",
    "    # print the update\n",
    "    print(\"degree: {d} \\t rmse_ts: {a} \\t  pred_ts: {b}\".format(d = degree, a = rmse_ts[ind], b = pred_ts[ind]))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# plot the loss\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(degrees,rmse_tr, c='blue')\n",
    "plt.plot(degrees,rmse_ts, c='red')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['training set', 'testing set'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#plot the accuracy\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(degrees,pred_tr, c='blue')\n",
    "plt.plot(degrees,pred_ts, c='red')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['training set', 'testing set'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression using ridge regression\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-5, 0, 15)\n",
    "rmse_tr = []\n",
    "rmse_ts = []\n",
    "pred_tr = []\n",
    "pred_ts = []\n",
    "\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    \n",
    "    w = ridge_regression(y_train, X_train, lambda_)\n",
    "    rmse_tr.append(np.sqrt(2 * compute_loss(y_train, X_train, w)))\n",
    "    pred_tr.append(pred_accuracy(predict_labels(w, X_train),y_train))\n",
    "    \n",
    "    \n",
    "    rmse_ts.append(np.sqrt(2 * compute_loss(y_test, X_test, w)))\n",
    "    pred_ts.append(pred_accuracy(predict_labels(w, X_test),y_test))\n",
    "    \n",
    "plt.style.use('seaborn')\n",
    "plt.semilogx(lambdas,rmse_tr, c='blue')\n",
    "plt.semilogx(lambdas,rmse_ts, c='red')\n",
    "plt.legend(['training set', 'testing set'], loc='upper left')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.semilogx(lambdas,pred_tr, c='blue')\n",
    "plt.semilogx(lambdas,pred_ts, c='red')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['training set', 'testing set'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# print(np.max(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression using gradient descent and stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[y_train == -1] = 0\n",
    "y_test[y_test == -1] = 0\n",
    "y[y == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "losses, ws = logistic_regression(y_train, X_train, initial_w, method = 'gd', max_iters = 2000, gamma = 0.0005) # fit model, retrieve parameters ws\n",
    "test_losses = list(map(lambda x: compute_loss(y_test, X_test, x, lam = 0, method = \"logistic\"), ws)) # retrieve losses using test set with ws\n",
    "\n",
    "\n",
    "\n",
    "# Make plot with label prediction accuracy\n",
    "\n",
    "pred_ytrain = list(map(lambda x: predict_labels_logistic(x, X_train), ws)) # Training prediction\n",
    "pred_accuracytrain = list(map(lambda x: pred_accuracy(x, y_train), pred_ytrain))\n",
    "pred_ytest = list(map(lambda x: predict_labels_logistic(x, X_test), ws)) # Test prediction\n",
    "pred_accuracytest = list(map(lambda x: pred_accuracy(x, y_test), pred_ytest))\n",
    "\n",
    "\n",
    "plotCurves(losses, pred_accuracytrain, test_losses, pred_accuracytest, \"Logistic Regression\")\n",
    "\n",
    "\"\"\"\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(losses, label='Training set loss', c='blue')\n",
    "plt.plot(test_losses, label='Test set loss', c='red')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Logistic Regression: Cost Function Loss Over Iterations of Stochastic Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(pred_accuracytrain, label='Training set prediction accuracy', c='blue')\n",
    "plt.plot(pred_accuracytest, label='Test set prediction accuracy', c='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Prediction Accuracy')\n",
    "plt.title('Logistic Regression Prediction Accuracy Over Iterations of Gradient Descent')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample = y[:10000] \n",
    "tx_sample = tx[:10000,:]\n",
    "\n",
    "X_train_sample, y_train_sample, X_test_sample, y_test_sample = split_data(tx_sample, y_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with varying learning rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_iter = 10000\n",
    "\n",
    "lr = [0.00001,0.00005,0.0001,0.0005,0.001] \n",
    "\n",
    "\n",
    "test_losses  = np.zeros([len(lr),(num_iter+1)])\n",
    "test_acc = np.zeros([len(lr),(num_iter+1)])\n",
    "train_losses = np.zeros([len(lr),(num_iter)])\n",
    "train_acc= np.zeros([len(lr),(num_iter+1)])\n",
    "\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    \n",
    "    l_tr, ws = logistic_regression(y_train_sample, X_train_sample, initial_w, max_iters = num_iter, gamma = lr[i], method = 'gd') # fit model, retrieve parameters ws\n",
    "    train_losses[i,:] = np.array(l_tr)\n",
    "    l_ts = list(map(lambda x: compute_loss(y_test_sample, X_test_sample, x, lam = 0, method = \"logistic\"), ws)) # retrieve losses using test set with ws\n",
    "    test_losses[i,:] = np.array(l_ts)\n",
    "    pred_ytrain = list(map(lambda x: predict_labels_logistic(x, X_train_sample), ws)) # Training prediction\n",
    "    train_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_train_sample), pred_ytrain))\n",
    "    pred_ytest = list(map(lambda x: predict_labels_logistic(x, X_test_sample), ws)) # Test prediction\n",
    "    test_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_test_sample), pred_ytest))\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    titre = \"learning rate \"+str(lr[i])\n",
    "    plotCurves(train_losses[i,:], train_acc[i,:], test_losses[i,:], test_acc[i,:], titre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 1000\n",
    "\n",
    "lr = [0.00001,0.00005,0.0001,0.0005,0.001] \n",
    "\n",
    "\n",
    "test_losses  = np.zeros([len(lr),(num_iter+1)])\n",
    "test_acc = np.zeros([len(lr),(num_iter+1)])\n",
    "train_losses = np.zeros([len(lr),(num_iter)])\n",
    "train_acc= np.zeros([len(lr),(num_iter+1)])\n",
    "\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    \n",
    "    l_tr, ws = logistic_regression(y_train_sample, X_train_sample, initial_w, max_iters = num_iter, gamma = lr[i], method = 'gd') # fit model, retrieve parameters ws\n",
    "    train_losses[i,:] = np.array(l_tr)\n",
    "    l_ts = list(map(lambda x: compute_loss(y_test_sample, X_test_sample, x, lam = 0, method = \"logistic\"), ws)) # retrieve losses using test set with ws\n",
    "    test_losses[i,:] = np.array(l_ts)\n",
    "    pred_ytrain = list(map(lambda x: predict_labels_logistic(x, X_train_sample), ws)) # Training prediction\n",
    "    train_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_train_sample), pred_ytrain))\n",
    "    pred_ytest = list(map(lambda x: predict_labels_logistic(x, X_test_sample), ws)) # Test prediction\n",
    "    test_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_test_sample), pred_ytest))\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    titre = \"learning rate \"+str(lr[i])\n",
    "    plotCurves(train_losses[i,:], train_acc[i,:], test_losses[i,:], test_acc[i,:], titre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 20000\n",
    "\n",
    "lr = [0.00005,0.0001] \n",
    "\n",
    "\n",
    "test_losses  = np.zeros([len(lr),(num_iter+1)])\n",
    "test_acc = np.zeros([len(lr),(num_iter+1)])\n",
    "train_losses = np.zeros([len(lr),(num_iter)])\n",
    "train_acc= np.zeros([len(lr),(num_iter+1)])\n",
    "\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    \n",
    "    l_tr, ws = logistic_regression(y_train_sample, X_train_sample, initial_w, max_iters = num_iter, gamma = lr[i], method = 'gd') # fit model, retrieve parameters ws\n",
    "    train_losses[i,:] = np.array(l_tr)\n",
    "    l_ts = list(map(lambda x: compute_loss(y_test_sample, X_test_sample, x, lam = 0, method = \"logistic\"), ws)) # retrieve losses using test set with ws\n",
    "    test_losses[i,:] = np.array(l_ts)\n",
    "    pred_ytrain = list(map(lambda x: predict_labels_logistic(x, X_train_sample), ws)) # Training prediction\n",
    "    train_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_train_sample), pred_ytrain))\n",
    "    pred_ytest = list(map(lambda x: predict_labels_logistic(x, X_test_sample), ws)) # Test prediction\n",
    "    test_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_test_sample), pred_ytest))\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    titre = \"learning rate \"+str(lr[i])\n",
    "    plotCurves(train_losses[i,:], train_acc[i,:], test_losses[i,:], test_acc[i,:], titre)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent with varying learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_iter = 10000\n",
    "\n",
    "lr = [0.00001,0.00005,0.0001,0.0005,0.001] \n",
    "\n",
    "\n",
    "test_losses  = np.zeros([len(lr),(num_iter+1)])\n",
    "test_acc = np.zeros([len(lr),(num_iter+1)])\n",
    "train_losses = np.zeros([len(lr),(num_iter)])\n",
    "train_acc= np.zeros([len(lr),(num_iter+1)])\n",
    "\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    \n",
    "    l_tr, ws = logistic_regression(y_train_sample, X_train_sample, initial_w, max_iters = num_iter, gamma = lr[i], method = 'sgd') # fit model, retrieve parameters ws\n",
    "    train_losses[i,:] = np.array(l_tr)\n",
    "    l_ts = list(map(lambda x: compute_loss(y_test_sample, X_test_sample, x, lam = 0, method = \"logistic\"), ws)) # retrieve losses using test set with ws\n",
    "    test_losses[i,:] = np.array(l_ts)\n",
    "    pred_ytrain = list(map(lambda x: predict_labels(x, X_train_sample), ws)) # Training prediction\n",
    "    train_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_train_sample), pred_ytrain))\n",
    "    pred_ytest = list(map(lambda x: predict_labels(x, X_test_sample), ws)) # Test prediction\n",
    "    test_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_test_sample), pred_ytest))\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    titre = \"learning rate \"+str(lr[i])\n",
    "    plotCurves(train_losses[i,:], train_acc[i,:], test_losses[i,:], test_acc[i,:], titre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_iter = 20000\n",
    "\n",
    "lr = [0.00005,0.0001,0.0005] \n",
    "\n",
    "\n",
    "test_losses  = np.zeros([len(lr),(num_iter+1)])\n",
    "test_acc = np.zeros([len(lr),(num_iter+1)])\n",
    "train_losses = np.zeros([len(lr),(num_iter)])\n",
    "train_acc= np.zeros([len(lr),(num_iter+1)])\n",
    "\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "\n",
    "for i in range(len(lr)):\n",
    "    \n",
    "    l_tr, ws = logistic_regression(y_train_sample, X_train_sample, initial_w, max_iters = num_iter, gamma = lr[i], method = 'sgd') # fit model, retrieve parameters ws\n",
    "    train_losses[i,:] = np.array(l_tr)\n",
    "    l_ts = list(map(lambda x: compute_loss(y_test_sample, X_test_sample, x, lam = 0, method = \"logistic\"), ws)) # retrieve losses using test set with ws\n",
    "    test_losses[i,:] = np.array(l_ts)\n",
    "    pred_ytrain = list(map(lambda x: predict_labels(x, X_train_sample), ws)) # Training prediction\n",
    "    train_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_train_sample), pred_ytrain))\n",
    "    pred_ytest = list(map(lambda x: predict_labels(x, X_test_sample), ws)) # Test prediction\n",
    "    test_acc[i,:] = list(map(lambda x: pred_accuracy(x, y_test_sample), pred_ytest))\n",
    "    \n",
    "for i in range(len(lr)):\n",
    "    titre = \"learning rate \"+str(lr[i])\n",
    "    plotCurves(train_losses[i,:], train_acc[i,:], test_losses[i,:], test_acc[i,:], titre)\n",
    "    print(\"best test accuracy = {0}\".format(max(test_acc[i,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression using Newton's method\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_w = np.random.rand(X_train.shape[1])\n",
    "# losses, ws = logistic_hessian(y_train, X_train, initial_w) # fit model, retrieve parameters ws\n",
    "# test_losses = list(map(lambda x: compute_loss(y_test, X_test, x, method = 'MSE'), ws)) # retrieve losses using test set with ws\n",
    "\n",
    "# plt.style.use('seaborn')\n",
    "# plt.plot(losses, label='Training set loss', c='blue')\n",
    "# plt.plot(test_losses, label='Test set loss', c='red')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Logistic Regression: Cost Function Loss Over Iterations of Stochastic Gradient Descent')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Make plot with label prediction accuracy\n",
    "\n",
    "# pred_ytrain = list(map(lambda x: predict_labels(x, X_train), ws)) # Training prediction\n",
    "# pred_accuracytrain = list(map(lambda x: pred_accuracy(x, y_train), pred_ytrain))\n",
    "# pred_ytest = list(map(lambda x: predict_labels(x, X_test), ws)) # Test prediction\n",
    "# pred_accuracytest = list(map(lambda x: pred_accuracy(x, y_test), pred_ytest))\n",
    "\n",
    "\n",
    "# plt.plot(pred_accuracytrain, label='Training set prediction accuracy', c='blue')\n",
    "# plt.plot(pred_accuracytest, label='Test set prediction accuracy', c='red')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Prediction Accuracy')\n",
    "# plt.title('Logistic Regression Prediction Accuracy Over Iterations of Gradient Descent')\n",
    "# plt.show()\n",
    "\n",
    "\"\"\"IF YOU READ THIS: I tried doing something here so that LR is faster and more accurate,\n",
    "the problem is that we can't calculate things that way, it's too big, but maybe I'll find a way\n",
    "another time.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-10,0, 11)\n",
    "loss_tr = []\n",
    "loss_ts = []\n",
    "pred_tr = []\n",
    "pred_ts = []\n",
    "initial_w = np.random.rand(X_train.shape[1])\n",
    "\n",
    "# There is a runtime warning but just be patient\n",
    "\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    \n",
    "    losses, ws = reg_logistic_regression(y_train_sample, X_train_sample, initial_w, lamb = lambda_, methods ='gd', gamma = 0.0001, max_iters=5000)\n",
    "    loss_tr.append(losses[-1])\n",
    "    best_w = ws[-1]\n",
    "    \n",
    "    pred_tr.append(pred_accuracy(predict_labels_logistic(best_w, X_train_sample),y_train_sample))\n",
    "    \n",
    "    test_losses = compute_loss(y_test_sample, X_test_sample, best_w, lam = lambda_, method = 'reg_logistic') # retrieve losses using test set with ws\n",
    "    loss_ts.append(test_losses)\n",
    "    pred_ts.append(pred_accuracy(predict_labels_logistic(best_w, X_test_sample),y_test_sample))\n",
    "\n",
    "    \n",
    "plt.style.use('seaborn')\n",
    "plt.semilogx(lambdas,loss_tr, c='blue')\n",
    "plt.semilogx(lambdas,loss_ts, c='red')\n",
    "plt.legend(['training set', 'testing set'], loc='upper left')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.semilogx(lambdas,pred_tr, c='blue')\n",
    "plt.semilogx(lambdas,pred_ts, c='red')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['training set', 'testing set'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lamb = 0.1\n",
    "# losses, ws = reg_logistic_regression(y_train, X_train, \n",
    "#                          initial_w, lamb, methods = 'gd', \n",
    "#                          gamma = 0.01, max_iters= 6000)\n",
    "\n",
    "# test_losses = list(map(lambda x: compute_loss(y_test, X_test, x, lam = lamb, method = 'reg_logistic'), ws)) # retrieve losses using test set with ws\n",
    "# plt.style.use('seaborn')\n",
    "# plt.plot(losses, label='Training set loss', c='blue')\n",
    "# plt.plot(test_losses, label='Test set loss', c='red')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Regularized Logistic Regression: Cost Function Loss Over Iterations of Gradient Descent')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Make plot with label prediction accuracy\n",
    "\n",
    "# pred_ytrain = list(map(lambda x: predict_labels(x, X_train), ws)) # Training prediction\n",
    "# pred_accuracytrain = list(map(lambda x: pred_accuracy(x, y_train), pred_ytrain))\n",
    "# pred_ytest = list(map(lambda x: predict_labels(x, X_test), ws)) # Test prediction\n",
    "# pred_accuracytest = list(map(lambda x: pred_accuracy(x, y_test), pred_ytest))\n",
    "\n",
    "\n",
    "# plt.plot(pred_accuracytrain, label='Training set prediction accuracy', c='blue')\n",
    "# plt.plot(pred_accuracytest, label='Test set prediction accuracy', c='red')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Prediction Accuracy')\n",
    "# plt.title('Regularized Logistic Regression Prediction Accuracy Over Iterations of Gradient Descent')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the polynomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 12\n",
    "X_test_poly = build_poly(data_test,degree)\n",
    "X_train_poly = build_poly(data,degree)\n",
    "w = least_squares(labels, X_train_poly)\n",
    "rmse_tr = (np.sqrt(2 * compute_loss(labels, X_train_poly, w)))\n",
    "pred_tr = pred_accuracy(predict_labels(w,X_train_poly),labels)\n",
    "\n",
    "test_prediction = predict_labels(w, X_test_poly)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"prediction.csv\"\n",
    "ids = ids_test\n",
    "y_pred = test_prediction\n",
    "create_csv_submission(ids, y_pred, name)\n",
    "\n",
    "# this yielded like 0.62 on kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.ones(tx.shape[1])\n",
    "y[y == -1] = 0\n",
    "losses, ws = logistic_regression(y, tx, initial_w, method = 'sgd', max_iters = 6000) # fit model, retrieve parameters ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tx_t.dot(ws[-1])\n",
    "pred[np.where(pred <= 0.5)] = -1\n",
    "pred[np.where(pred > 0.5)] = 1\n",
    "\n",
    "name = \"prediction.csv\"\n",
    "create_csv_submission(t_ids, pred, name)\n",
    "\n",
    "# this yields 0.73 on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.ones(tx.shape[1])\n",
    "y[y == -1] = 0\n",
    "losses, ws = reg_logistic_regression(y, tx, initial_w, lamb = 0.1, methods = 'sgd', max_iters = 20000, gamma = 0.0001)\n",
    "\n",
    "pred = tx_t.dot(ws[-1])\n",
    "pred[np.where(pred <= 0.5)] = -1\n",
    "pred[np.where(pred > 0.5)] = 1\n",
    "\n",
    "name = \"prediction_2.csv\"\n",
    "create_csv_submission(t_ids, pred, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
