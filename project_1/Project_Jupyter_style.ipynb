{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the imports\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the important functions\n",
    "from helpers import *\n",
    "from logistic_regression import *\n",
    "from pca import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload data\n",
    "(labels, data, ids) = load_csv_data(\"C:/Users/remy/Documents/data/train.csv\")  # load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)\n",
    "print(data.shape)\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set values of -999 to NaN. \n",
    "# Then calculate the means of the features. \n",
    "# Replace NaN values with new values.\n",
    "\n",
    "data_process = np.array(data[:1000,:])\n",
    "labels_select = np.array(labels[:1000])\n",
    "lab = []\n",
    "for entry in labels_select:\n",
    "    if int(entry) == 1:\n",
    "        lab.append(1)\n",
    "    else:\n",
    "        lab.append(0)\n",
    "lab = np.array(lab)\n",
    "data_process[data_process == -999] = np.nan\n",
    "\n",
    "# print(data_process.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out really shitty features\n",
    "# retrieve percentage for each feature \n",
    "# - how many nan's are there?\n",
    "\n",
    "nan_count = []\n",
    "for c in data_process.T:\n",
    "    count = 0\n",
    "    for e in c:\n",
    "        if np.isnan(e):\n",
    "            count += 1\n",
    "    pcent = count / data_process.shape[0]\n",
    "    nan_count.append(pcent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out features which have nan values of 50%\n",
    "\n",
    "data_set_filtered = []\n",
    "for idx, entry in enumerate(nan_count):\n",
    "    if entry < 0.6:\n",
    "        #append the column of the original dataset that is good\n",
    "        data_set_filtered.append(data_process.T[idx]) \n",
    "#save that shit as an np array\n",
    "data_set_filtered = np.array(data_set_filtered).T #save that shit as an np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out samples that have more than half of their features as nan (crappy samples)\n",
    "\n",
    "nan_count_2 = []\n",
    "data_set_filtered_2 = [] # dataset filtered for columns and samples\n",
    "y = [] # array that gets rid of entries that are no longer corresponding in the dataframe\n",
    "for sample in data_set_filtered:\n",
    "    count = 0\n",
    "    for col in sample:\n",
    "        if np.isnan(col):\n",
    "            count += 1\n",
    "    pcent = count / data_set_filtered.shape[1]\n",
    "    nan_count_2.append(pcent)\n",
    "\n",
    "for idx, entry in enumerate(nan_count_2):\n",
    "    if entry < 0.15:\n",
    "        y.append(lab[idx])\n",
    "        data_set_filtered_2.append(data_set_filtered[idx])\n",
    "data_set_filtered_2 = np.array(data_set_filtered_2) # turn dat shit into an array\n",
    "y = np.array(y) # also this one gotta be an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dimensions of the training data set was 1000 samples and 30 columns. After feature and sample filtering, there are 909 samples and 23 columns\n"
     ]
    }
   ],
   "source": [
    "# print new dimensions of the dataframe after filtering\n",
    "\n",
    "print('The original dimensions of the training data set was {0} samples'\n",
    "      ' and {1} columns. After feature and sample filtering, there are'\n",
    "      ' {2} samples and {3} columns'.format(data_process.shape[0],\n",
    "                                            data_process.shape[1],\n",
    "                                            data_set_filtered_2.shape[0],\n",
    "                                            data_set_filtered_2.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nan = data_set_filtered_2.copy() # variable reassigned\n",
    "column_means = [] # create list with average values of columns, excluding nans\n",
    "for column in data_nan.T:\n",
    "    column_means.append(np.nanmean(column))\n",
    "\n",
    "inds = np.where(np.isnan(data_nan)) # variable containing locations of nan in data frame\n",
    "data_nan[inds] = np.take(column_means, inds[1]) # reassign locations of nan to the column means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize and normalize the features\n",
    "column_variance = []\n",
    "for idx, column in enumerate(data_nan.T):\n",
    "    mean = np.mean(column)\n",
    "    variance = np.var(column)\n",
    "    for entry, value in enumerate(column):\n",
    "        data_nan[entry, idx] = (value - mean)/variance # scale dat shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPRESS DATA BY PCA. Note that this is an unsupervised method and might not be a good alternative\n",
    "\n",
    "# pca_ = pca(percent_rep=0.9)\n",
    "# pca_.fit(data_nan)\n",
    "# pca_data = pca_.transform(data_nan)\n",
    "#\n",
    "# pca_data_A = []\n",
    "# pca_data_B = []\n",
    "# for idx, entry in enumerate(labels_select):\n",
    "#     if entry == 1:\n",
    "#         pca_data_A.append(pca_data[idx])\n",
    "#     else:\n",
    "#         pca_data_B.append(pca_data[idx])\n",
    "# pca_data_A = np.array(pca_data_A)\n",
    "# pca_data_B = np.array(pca_data_B)\n",
    "#\n",
    "# fig = plt.figure()\n",
    "# scattered = plt.axes(projection = '3d')\n",
    "# scattered.scatter3D(pca_data_A[:,0], pca_data_A[:,1], pca_data_A[:,2], c='blue')\n",
    "# scattered.scatter3D(pca_data_B[:,0], pca_data_B[:,1], pca_data_B[:,2], c='red')\n",
    "#\n",
    "# plt.show()\n",
    "# pca_int = logreg.add_int(pca_data)\n",
    "# prediction = logreg.predict_class(pca_int)\n",
    "# print(prediction)\n",
    "\n",
    "# add some non-linear variants of each of the features that are included in the data set\n",
    "# do this by constructing a new array, which will then be concatenated with the old one\n",
    "# note that the addition of the squared features increases accuracy by a few percentage\n",
    "# but there is a smaller, but clear, difference with the cubed features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_me = []\n",
    "\n",
    "for col in data_nan.T:\n",
    "    col_squared = [entry**2 for entry in col]\n",
    "    col_cubed = [entry ** 3 for entry in col]\n",
    "    concatenate_me.append(col_squared)\n",
    "    concatenate_me.append(col_cubed)\n",
    "\n",
    "concatenate_me = np.array(concatenate_me).T\n",
    "data_concatenated = np.concatenate((data_nan, concatenate_me), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7161716171617162\n",
      "0.7161716171617162\n"
     ]
    }
   ],
   "source": [
    "# perform the linear regression\n",
    "# i just noticed that the prediction accuracy also depends quite a bit on the initialized w.\n",
    "# for example if you initialize them all to ones instead of zeros, accuracy increases. overfitting?\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(data_concatenated, y)\n",
    "data_con_int = logreg.add_int(data_concatenated)\n",
    "prediction = logreg.predict_class(data_con_int)\n",
    "prediction = np.array(prediction)\n",
    "\n",
    "\n",
    "print(pred_accuracy(prediction, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT DATA\n",
    "\n",
    "#(X_train, y_train, X_test, y_test) = test_train(data, labels, test_ratio=0.3)  # split into train and test set\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "# # CALCULATE FISHER SCORE VARIANT\n",
    "#\n",
    "# scores = []\n",
    "# means_A = []\n",
    "# var_A = []\n",
    "# means_B = []\n",
    "# var_B = []\n",
    "# class_A = np.array(data_nan[np.argwhere(y == 1)])# separate out classes\n",
    "# class_A = class_A[:,0,:]\n",
    "# class_B = np.array(data_nan[np.argwhere(y == 0)])\n",
    "# class_B = class_B[:,0,:]\n",
    "#\n",
    "# for c in class_A.T:\n",
    "#     means_A.append(np.mean(c))\n",
    "#     var_A.append(np.var(c))\n",
    "# for c in class_B.T:\n",
    "#     means_B.append(np.mean(c))\n",
    "#     var_B.append(np.mean(c))\n",
    "# for x in range(len(means_A)):\n",
    "#     scores.append(means_A[x] - means_B[x]) # score function\n",
    "# print(np.array(scores))\n",
    "#\n",
    "#\n",
    "# # fig = plt.figure()\n",
    "# # scattered = plt.axes(projection = '3d')\n",
    "# # scattered.scatter3D(class_A[:,12], class_A[:,28], class_A[:,4], c='blue')\n",
    "# # scattered.scatter3D(class_B[:,12], class_B[:,28], class_B[:,4], c='red')\n",
    "# fig = plt.figure()\n",
    "# plt.scatter(class_B[:,12], class_B[:,10], c='blue', alpha=0.2)\n",
    "# plt.scatter(class_A[:,12], class_A[:,10], c='red', alpha=0.2)\n",
    "#\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
