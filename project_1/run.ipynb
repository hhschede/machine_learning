{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation of modules and functions\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import sys\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# Functions\n",
    "sys.path.insert(0, './implementations/')\n",
    "from implementations import *\n",
    "from preprocessing import *\n",
    "from pca import *\n",
    "from plot import *\n",
    "from helpers import *\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data loading\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels_raw, data_raw, ids_raw) = load_csv_data(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data and jets\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "X_train, y_train, X_test, y_test = split_data(data_raw, labels_raw)\n",
    "\n",
    "# Get feature jet_num\n",
    "jets = X_train[:,22]\n",
    "jets_t = X_test[:,22]\n",
    "\n",
    "# Get index of samples with appropriate jet\n",
    "idx_jet0 = np.argwhere(jets == 0)[:,0]\n",
    "idx_jet1 = np.argwhere(jets == 1)[:,0]\n",
    "idx_jet2 = np.argwhere(jets >= 2)[:,0]\n",
    "\n",
    "idx_jet0_t = np.argwhere(jets_t == 0)[:,0]\n",
    "idx_jet1_t = np.argwhere(jets_t == 1)[:,0]\n",
    "idx_jet2_t = np.argwhere(jets_t >= 2)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.delete(X_train, 22, axis=1)\n",
    "X_test = np.delete(X_test, 22, axis=1)\n",
    "\n",
    "# Split data relatitve to jets\n",
    "data_tr_j0 = X_train[idx_jet0,:]\n",
    "data_tr_j1 = X_train[idx_jet1,:]\n",
    "data_tr_j2 = X_train[idx_jet2,:]\n",
    "\n",
    "data_ts_j0 = X_test[idx_jet0_t,:]\n",
    "data_ts_j1 = X_test[idx_jet1_t,:]\n",
    "data_ts_j2 = X_test[idx_jet2_t,:]\n",
    "\n",
    "# Split labels relative to jets\n",
    "lab_j0 = y_train[idx_jet0]\n",
    "lab_j1 = y_train[idx_jet1]\n",
    "lab_j2 = y_train[idx_jet2]\n",
    "\n",
    "lab_j0_t = y_test[idx_jet0_t]\n",
    "lab_j1_t = y_test[idx_jet1_t]\n",
    "lab_j2_t = y_test[idx_jet2_t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data filtering and normalization\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dimensions of the training data set was 61985 samples and 29 columns\n",
      " After feature and sample filtering, there are 61985 samples and 22 columns\n"
     ]
    }
   ],
   "source": [
    "# Filtering missing values and outliers\n",
    "# data_j0, data_j0_t = process_data(data_tr_j0, data_ts_j0)\n",
    "data_j1, data_j1_t = process_data(data_tr_j1, data_ts_j1)\n",
    "# data_j2, data_j2_t = process_data(data_tr_j2, data_ts_j2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have reduce the number of feature with PCA to 531\n"
     ]
    }
   ],
   "source": [
    "# Transforming data using polynomials, log and interaction terms\n",
    "# y_j0, tx_j0, y_j0_t, tx_j0_t = transform_data(data_j0, data_j0_t, lab_j0, lab_j0_t, 6)\n",
    "y_j1, tx_j1, y_j1_t, tx_j1_t = transform_data(data_j1, data_j1_t, lab_j1, lab_j1_t, log = False)\n",
    "# y_j2, tx_j2, y_j2_t, tx_j2_t = transform_data(data_j2, data_j2_t, lab_j2, lab_j2_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression using Newton's method\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/100\t train acc : 0.8187787367911592 \t | test acc : 0.8164406452856867\n",
      "50/100\t train acc : 0.8189561990804227 \t | test acc : 0.8158622019409988\n",
      "75/100\t train acc : 0.8189723320158103 \t | test acc : 0.815990744906485\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-53e21d6d9ac2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0minitial_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx_j1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_hessian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_j1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_j1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_j1_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_j1_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_lambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fit model, retrieve parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# initial_w = np.zeros(tx_j2.shape[1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\machine_learning\\project_1\\implementations\\implementations.py\u001b[0m in \u001b[0;36mlogistic_hessian\u001b[1;34m(y, tx, y_t, tx_t, initial_w, gamma, lam, max_iters, momentum, tol, patience, writing, threshold)\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;31m# compute gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[0mgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"logistic\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m         \u001b[0mhess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_hessian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[1;31m# compute next w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\machine_learning\\project_1\\implementations\\implementations.py\u001b[0m in \u001b[0;36mcompute_hessian\u001b[1;34m(y, tx, w, lam)\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m     \u001b[0mXD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m     \u001b[0mhess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXD\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhess\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_gamma = 0.07\n",
    "best_lambda = 0\n",
    "\n",
    "# initial_w = np.zeros(tx_j0.shape[1])\n",
    "# losses, losses_t, acc, acc_t, w_0 = logistic_hessian(y_j0, tx_j0, y_j0_t, tx_j0_t, initial_w, best_gamma, best_lambda, 100) # fit model, retrieve parameters\n",
    "\n",
    "initial_w = np.zeros(tx_j1.shape[1])\n",
    "losses, losses_t, acc, acc_t, w_1 = logistic_hessian(y_j1, tx_j1, y_j1_t, tx_j1_t, initial_w, best_gamma, best_lambda, 100) # fit model, retrieve parameters\n",
    "\n",
    "# initial_w = np.zeros(tx_j2.shape[1])\n",
    "# losses, losses_t, acc, acc_t, w_2 = logistic_hessian(y_j2, tx_j2, y_j2_t, tx_j2_t, initial_w, best_gamma, best_lambda, 100) # fit model, retrieve parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for best paramaters\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 100\n",
    "lr = np.arange(0.05,0.1,0.005)\n",
    "lamb = np.logspace(-3,3,7)\n",
    "\n",
    "initial_w = np.ones(tx.shape[1])\n",
    "\n",
    "best_gamma = 0\n",
    "best_lambda = 0\n",
    "best_acc = 0\n",
    "grid_acc = np.zeros([len(lr),len(lamb)])\n",
    "\n",
    "for g in range(len(lr)):\n",
    "    for l in range(len(lamb)):\n",
    "        grid_acc[g,l] = Grid_Search_logistic(y, tx, y_t, tx_t, initial_w, gamma=lr[g], lam=lamb[l], max_iters = num_iter, momentum = 0)\n",
    "        print(\"gamma: {0} \\t| lambda: {1} \\t| test acc: {2}\".format(lr[g],lamb[l], grid_acc[g,l]))\n",
    "        if (grid_acc[g,l] > best_acc):\n",
    "            best_acc = grid_acc[g,l]\n",
    "            best_gamma = lr[g]\n",
    "            best_lambda = lamb[l]\n",
    "\n",
    "file = \"grid_search_logistic_test_acc.npy\"\n",
    "np.save(file, grid_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gamma\n",
    "best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grid_acc = np.load(file)\n",
    "import seaborn as sns; sns.set()\n",
    "ax = sns.heatmap(grid_acc,xticklabels=lamb, yticklabels=lr,vmin= 0.6, vmax=0.782)\n",
    "ax.set_xlabel('lambda')\n",
    "ax.set_ylabel('gamma')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_iter = 100\n",
    "lr = np.arange(0.05,0.06, 0.07)\n",
    "lamb = np.logspace(-3,3,7)\n",
    "\n",
    "initial_w = np.ones(tx.shape[1])\n",
    "\n",
    "best_gamma = 0\n",
    "best_lambda = 0\n",
    "best_acc = 0\n",
    "grid_acc = np.zeros([len(lr),len(lamb)])\n",
    "\n",
    "for g in range(len(lr)):\n",
    "    for l in range(len(lamb)):\n",
    "        grid_acc[g,l] = Grid_Search_logistic(y, tx, y_t, tx_t, initial_w, gamma=lr[g], lam=lamb[l], max_iters = num_iter, momentum = 0)\n",
    "        print(\"gamma: {0} \\t| lambda: {1} \\t| test acc: {2}\".format(lr[g],lamb[l], grid_acc[g,l]))\n",
    "        if (grid_acc[g,l] > best_acc):\n",
    "            best_acc = grid_acc[g,l]\n",
    "            best_gamma = lr[g]\n",
    "            best_lambda = lamb[l]\n",
    "\n",
    "file = \"grid_search_logistic_test_acc_mean.npy\"\n",
    "np.save(file, grid_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_acc = np.load(file)\n",
    "import seaborn as sns; sns.set()\n",
    "ax = sns.heatmap(grid_acc,xticklabels=lamb, yticklabels=lr,vmin= 0.6, vmax=0.782)\n",
    "ax.set_xlabel('lambda')\n",
    "ax.set_ylabel('gamma')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle \n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data loading\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels_t, data_raw_t, ids_t) = load_csv_data(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get jet indexes\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature jet_num\n",
    "jets = data_raw[:,22]\n",
    "jets_t = data_raw_t[:,22]\n",
    "\n",
    "# Get index of samples with appropriate jet\n",
    "idx_jet0 = np.argwhere(jets == 0)[:,0]\n",
    "idx_jet1 = np.argwhere(jets == 1)[:,0]\n",
    "idx_jet2 = np.argwhere(jets >= 2)[:,0]\n",
    "# idx_jet3 = np.argwhere(jets == 3)[:,0]\n",
    "\n",
    "idx_jet0_t = np.argwhere(jets_t == 0)[:,0]\n",
    "idx_jet1_t = np.argwhere(jets_t == 1)[:,0]\n",
    "idx_jet2_t = np.argwhere(jets_t >= 2)[:,0]\n",
    "# idx_jet3_t = np.argwhere(jets_t == 3)[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate data relative to jets\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = np.delete(data_raw, 22, axis=1)\n",
    "data_raw_t = np.delete(data_raw_t, 22, axis=1)\n",
    "# Split data relatitve to jets\n",
    "data_tr_j0 = data_raw[idx_jet0,:]\n",
    "data_tr_j1 = data_raw[idx_jet1,:]\n",
    "data_tr_j2 = data_raw[idx_jet2,:]\n",
    "# data_tr_j3 = data_raw[idx_jet3,:]\n",
    "\n",
    "data_ts_j0 = data_raw_t[idx_jet0_t,:]\n",
    "data_ts_j1 = data_raw_t[idx_jet1_t,:]\n",
    "data_ts_j2 = data_raw_t[idx_jet2_t,:]\n",
    "# data_ts_j3 = data_raw_t[idx_jet3_t,:]\n",
    "\n",
    "# Split labels relative to jets\n",
    "lab_j0 = labels_raw[idx_jet0]\n",
    "lab_j1 = labels_raw[idx_jet1]\n",
    "lab_j2 = labels_raw[idx_jet2]\n",
    "# lab_j3 = labels_raw[idx_jet3]\n",
    "\n",
    "lab_j0_t = labels_t[idx_jet0_t]\n",
    "lab_j1_t = labels_t[idx_jet1_t]\n",
    "lab_j2_t = labels_t[idx_jet2_t]\n",
    "# lab_j3_t = labels_t[idx_jet3_t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data filtering and transformation\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dimensions of the training data set was 99913 samples and 29 columns\n",
      " After feature and sample filtering, there are 99913 samples and 19 columns\n",
      "The original dimensions of the training data set was 77544 samples and 29 columns\n",
      " After feature and sample filtering, there are 77544 samples and 22 columns\n",
      "The original dimensions of the training data set was 72543 samples and 29 columns\n",
      " After feature and sample filtering, there are 72543 samples and 29 columns\n"
     ]
    }
   ],
   "source": [
    "# Filtering missing values and outliers\n",
    "data_j0, data_j0_t = process_data(data_tr_j0, data_ts_j0)\n",
    "data_j1, data_j1_t = process_data(data_tr_j1, data_ts_j1)\n",
    "data_j2, data_j2_t = process_data(data_tr_j2, data_ts_j2)\n",
    "# data_j3, data_j3_t = process_data(data_tr_j3, data_ts_j3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have reduce the number of feature with PCA to 464\n",
      "we have reduce the number of feature with PCA to 542\n",
      "we have reduce the number of feature with PCA to 956\n"
     ]
    }
   ],
   "source": [
    "# Transforming data using polynomials, log and interaction terms\n",
    "y_j0, tx_j0, y_j0_t, tx_j0_t = transform_data(data_j0, data_j0_t, lab_j0, lab_j0_t, 6)\n",
    "y_j1, tx_j1, y_j1_t, tx_j1_t = transform_data(data_j1, data_j1_t, lab_j1, lab_j1_t, log = False)\n",
    "y_j2, tx_j2, y_j2_t, tx_j2_t = transform_data(data_j2, data_j2_t, lab_j2, lab_j2_t)\n",
    "# y_j3, tx_j3, y_j3_t, tx_j3_t = transform_data(data_j3, data_j3_t, lab_j3, lab_j3_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression using Newton's method\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/100\t train acc : 0.8507801787555173 \t | test acc : 0.21315143894697042\n",
      "50/100\t train acc : 0.8509203006615755 \t | test acc : 0.21630366924882835\n",
      "75/100\t train acc : 0.850910291954 \t | test acc : 0.21629927283278672\n",
      "100/100\t train acc : 0.850910291954 \t | test acc : 0.21630806566487\n",
      "25/100\t train acc : 0.8183482925822758 \t | test acc : 0.3330196534692993\n",
      "50/100\t train acc : 0.819341277210358 \t | test acc : 0.33433710889824225\n",
      "75/100\t train acc : 0.819341277210358 \t | test acc : 0.3343599219792629\n",
      "25/100\t train acc : 0.8503508264064072 \t | test acc : 0.44589040267888447\n",
      "50/100\t train acc : 0.8508470837985747 \t | test acc : 0.4446331644926923\n",
      "75/100\t train acc : 0.8508470837985747 \t | test acc : 0.4446452533214057\n"
     ]
    }
   ],
   "source": [
    "best_gamma = 0.07\n",
    "best_lambda = 0\n",
    "\n",
    "initial_w = np.zeros(tx_j0.shape[1])\n",
    "losses, losses_t, acc, acc_t, w_0 = logistic_hessian(y_j0, tx_j0, y_j0_t, tx_j0_t, initial_w, best_gamma, best_lambda, 100) # fit model, retrieve parameters\n",
    "\n",
    "initial_w = np.zeros(tx_j1.shape[1])\n",
    "losses, losses_t, acc, acc_t, w_1 = logistic_hessian(y_j1, tx_j1, y_j1_t, tx_j1_t, initial_w, best_gamma, best_lambda, 100) # fit model, retrieve parameters\n",
    "\n",
    "initial_w = np.zeros(tx_j2.shape[1])\n",
    "losses, losses_t, acc, acc_t, w_2 = logistic_hessian(y_j2, tx_j2, y_j2_t, tx_j2_t, initial_w, best_gamma, best_lambda, 100) # fit model, retrieve parameters\n",
    "\n",
    "# initial_w = np.zeros(tx_j3.shape[1])\n",
    "# losses, losses_t, acc, acc_t, w_3 = logistic_hessian(y_j3, tx_j3, y_j3_t, tx_j3_t, initial_w, best_gamma, best_lambda, 100) # fit model, retrieve parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle submission\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_t = np.zeros(ids_t.shape)\n",
    "\n",
    "pred_t[idx_jet0_t] = predict_labels_logistic(w_0, tx_j0_t, 0.5)\n",
    "pred_t[idx_jet1_t] = predict_labels_logistic(w_1, tx_j1_t, 0.5)\n",
    "pred_t[idx_jet2_t] = predict_labels_logistic(w_2, tx_j2_t, 0.5)\n",
    "# pred_t[idx_jet3_t] = predict_labels_logistic(w_3, tx_j3_t, 0.5)\n",
    "\n",
    "name = \"semi_final.csv\"\n",
    "create_csv_submission(ids_t, pred_t, name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
