{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation of modules and functions\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import sys\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# Functions\n",
    "sys.path.insert(0, './implementations/')\n",
    "from implementations import *\n",
    "from preprocessing import *\n",
    "from pca import *\n",
    "from plot import *\n",
    "from helpers import *\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data loading\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels_raw, data_raw, ids_raw) = load_csv_data(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data loading\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels_t, data_raw_t, ids_t) = load_csv_data(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get jet indexes\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature jet_num\n",
    "jets = data_raw[:,22]\n",
    "jets_t = data_raw_t[:,22]\n",
    "\n",
    "# Get index of samples with appropriate jet\n",
    "idx_jet0 = np.argwhere(jets == 0)[:,0]\n",
    "idx_jet1 = np.argwhere(jets == 1)[:,0]\n",
    "idx_jet2 = np.argwhere(jets >= 2)[:,0]\n",
    "\n",
    "idx_jet0_t = np.argwhere(jets_t == 0)[:,0]\n",
    "idx_jet1_t = np.argwhere(jets_t == 1)[:,0]\n",
    "idx_jet2_t = np.argwhere(jets_t >= 2)[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate data relative to jets\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = np.delete(data_raw, 22, axis=1)\n",
    "data_raw_t = np.delete(data_raw_t, 22, axis=1)\n",
    "\n",
    "# Split data relatitve to jets\n",
    "data_tr_j0 = data_raw[idx_jet0,:]\n",
    "data_tr_j1 = data_raw[idx_jet1,:]\n",
    "data_tr_j2 = data_raw[idx_jet2,:]\n",
    "\n",
    "data_ts_j0 = data_raw_t[idx_jet0_t,:]\n",
    "data_ts_j1 = data_raw_t[idx_jet1_t,:]\n",
    "data_ts_j2 = data_raw_t[idx_jet2_t,:]\n",
    "\n",
    "# Split labels relative to jets\n",
    "lab_j0 = labels_raw[idx_jet0]\n",
    "lab_j1 = labels_raw[idx_jet1]\n",
    "lab_j2 = labels_raw[idx_jet2]\n",
    "\n",
    "lab_j0_t = labels_t[idx_jet0_t]\n",
    "lab_j1_t = labels_t[idx_jet1_t]\n",
    "lab_j2_t = labels_t[idx_jet2_t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data filtering and transformation\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dimensions of the training data set was 99913 samples and 29 columns\n",
      " After feature and sample filtering, there are 99913 samples and 19 columns\n",
      "The original dimensions of the training data set was 77544 samples and 29 columns\n",
      " After feature and sample filtering, there are 77544 samples and 22 columns\n",
      "The original dimensions of the training data set was 72543 samples and 29 columns\n",
      " After feature and sample filtering, there are 72543 samples and 29 columns\n"
     ]
    }
   ],
   "source": [
    "# Filtering missing values and outliers\n",
    "data_j0, data_j0_t = process_data(data_tr_j0, data_ts_j0)\n",
    "data_j1, data_j1_t = process_data(data_tr_j1, data_ts_j1)\n",
    "data_j2, data_j2_t = process_data(data_tr_j2, data_ts_j2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have reduce the number of feature with PCA to 464\n",
      "we have reduce the number of feature with PCA to 542\n",
      "we have reduce the number of feature with PCA to 956\n"
     ]
    }
   ],
   "source": [
    "# Transforming data using polynomials, log and interaction terms\n",
    "y_j0, tx_j0, y_j0_t, tx_j0_t = transform_data(data_j0, data_j0_t, lab_j0, lab_j0_t, 6)\n",
    "y_j1, tx_j1, y_j1_t, tx_j1_t = transform_data(data_j1, data_j1_t, lab_j1, lab_j1_t, log = False)\n",
    "y_j2, tx_j2, y_j2_t, tx_j2_t = transform_data(data_j2, data_j2_t, lab_j2, lab_j2_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression using Newton's method\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/100\t train acc : 0.8496792209222023 \t | test acc : 0.20791970385741543\n",
      "50/100\t train acc : 0.850690100387337 \t | test acc : 0.21410106481196528\n",
      "75/100\t train acc : 0.8509303093691511 \t | test acc : 0.21614539827132923\n",
      "100/100\t train acc : 0.8508602484161221 \t | test acc : 0.21643556173007764\n",
      "25/100\t train acc : 0.8161817806664603 \t | test acc : 0.32772131540225163\n",
      "50/100\t train acc : 0.8185159393376663 \t | test acc : 0.33149117704091524\n",
      "75/100\t train acc : 0.8187738574228824 \t | test acc : 0.3323808872007209\n",
      "100/100\t train acc : 0.8188641287527081 \t | test acc : 0.33254628203812064\n",
      "25/100\t train acc : 0.8483106571274968 \t | test acc : 0.4489609651720845\n",
      "50/100\t train acc : 0.8504611058268888 \t | test acc : 0.4458964470932411\n",
      "75/100\t train acc : 0.850598955102491 \t | test acc : 0.44502000701152067\n",
      "100/100\t train acc : 0.8508608687261349 \t | test acc : 0.44478427485160965\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(tx_j0.shape[1])\n",
    "losses, losses_t, acc, acc_t, w_0 = logistic_hessian(y_j0, tx_j0, y_j0_t, tx_j0_t, initial_w, 0.075, 1, 100) # fit model, retrieve parameters\n",
    "\n",
    "initial_w = np.zeros(tx_j1.shape[1])\n",
    "losses, losses_t, acc, acc_t, w_1 = logistic_hessian(y_j1, tx_j1, y_j1_t, tx_j1_t, initial_w, 0.075, 100, 100) # fit model, retrieve parameters\n",
    "\n",
    "initial_w = np.zeros(tx_j2.shape[1])\n",
    "losses, losses_t, acc, acc_t, w_2 = logistic_hessian(y_j2, tx_j2, y_j2_t, tx_j2_t, initial_w, 0.07, 0.001, 100) # fit model, retrieve parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle submission\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_t = np.zeros(ids_t.shape)\n",
    "\n",
    "pred_t[idx_jet0_t] = predict_labels_logistic(w_0, tx_j0_t, 0.5)\n",
    "pred_t[idx_jet1_t] = predict_labels_logistic(w_1, tx_j1_t, 0.5)\n",
    "pred_t[idx_jet2_t] = predict_labels_logistic(w_2, tx_j2_t, 0.5)\n",
    "\n",
    "name = \"based_on_train.csv\"\n",
    "create_csv_submission(ids_t, pred_t, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
