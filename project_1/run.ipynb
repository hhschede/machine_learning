{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation of modules and functions\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import sys\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# Functions\n",
    "sys.path.insert(0, './implementations/')\n",
    "from implementations import *\n",
    "from preprocessing import *\n",
    "from pca import *\n",
    "from plot import *\n",
    "from helpers import *\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data loading\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels_raw, data_raw, ids_raw) = load_csv_data(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data and jets\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "X_train, y_train, X_test, y_test = split_data(data_raw, labels_raw)\n",
    "\n",
    "# Get feature jet_num\n",
    "jets = X_train[:,22]\n",
    "jets_t = X_test[:,22]\n",
    "\n",
    "# Get index of samples with appropriate jet\n",
    "idx_jet0 = np.argwhere(jets == 0)[:,0]\n",
    "idx_jet1 = np.argwhere(jets == 1)[:,0]\n",
    "idx_jet2 = np.argwhere(jets >= 2)[:,0]\n",
    "# idx_jet3 = np.argwhere(jets == 3)[:,0]\n",
    "\n",
    "idx_jet0_t = np.argwhere(jets_t == 0)[:,0]\n",
    "idx_jet1_t = np.argwhere(jets_t == 1)[:,0]\n",
    "idx_jet2_t = np.argwhere(jets_t >= 2)[:,0]\n",
    "# idx_jet3_t = np.argwhere(jets_t == 3)[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.delete(X_train, 22, axis=1)\n",
    "X_test = np.delete(X_test, 22, axis=1)\n",
    "\n",
    "# Split data relatitve to jets\n",
    "data_tr_j0 = X_train[idx_jet0,:]\n",
    "data_tr_j1 = X_train[idx_jet1,:]\n",
    "data_tr_j2 = X_train[idx_jet2,:]\n",
    "# data_tr_j3 = X_train[idx_jet3,:]\n",
    "\n",
    "data_ts_j0 = X_test[idx_jet0_t,:]\n",
    "data_ts_j1 = X_test[idx_jet1_t,:]\n",
    "data_ts_j2 = X_test[idx_jet2_t,:]\n",
    "# data_ts_j3 = X_test[idx_jet3_t,:]\n",
    "\n",
    "# Split labels relative to jets\n",
    "lab_j0 = y_train[idx_jet0]\n",
    "lab_j1 = y_train[idx_jet1]\n",
    "lab_j2 = y_train[idx_jet2]\n",
    "# lab_j3 = y_train[idx_jet3]\n",
    "\n",
    "lab_j0_t = y_test[idx_jet0_t]\n",
    "lab_j1_t = y_test[idx_jet1_t]\n",
    "lab_j2_t = y_test[idx_jet2_t]\n",
    "# lab_j3_t = y_test[idx_jet3_t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data filtering and normalization\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dimensions of the training data set was 79923 samples and 29 columns\n",
      " After feature and sample filtering, there are 79923 samples and 19 columns\n",
      "The original dimensions of the training data set was 61985 samples and 29 columns\n",
      " After feature and sample filtering, there are 61985 samples and 22 columns\n",
      "The original dimensions of the training data set was 58092 samples and 29 columns\n",
      " After feature and sample filtering, there are 58092 samples and 29 columns\n"
     ]
    }
   ],
   "source": [
    "# Filtering missing values and outliers\n",
    "data_j0, data_j0_t = process_data(data_tr_j0, data_ts_j0)\n",
    "data_j1, data_j1_t = process_data(data_tr_j1, data_ts_j1)\n",
    "data_j2, data_j2_t = process_data(data_tr_j2, data_ts_j2)\n",
    "# data_j3, data_j3_t = process_data(data_tr_j3, data_ts_j3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have reduce the number of feature with PCA to 380\n",
      "we have reduce the number of feature with PCA to 545\n",
      "we have reduce the number of feature with PCA to 957\n"
     ]
    }
   ],
   "source": [
    "# Transforming data using polynomials, log and interaction terms\n",
    "y_j0, tx_j0, y_j0_t, tx_j0_t = transform_data(data_j0, data_j0_t, lab_j0, lab_j0_t)\n",
    "y_j1, tx_j1, y_j1_t, tx_j1_t = transform_data(data_j1, data_j1_t, lab_j1, lab_j1_t)\n",
    "y_j2, tx_j2, y_j2_t, tx_j2_t = transform_data(data_j2, data_j2_t, lab_j2, lab_j2_t)\n",
    "# y_j3, tx_j3, y_j3_t, tx_j3_t = transform_data(data_j3, data_j3_t, lab_j3, lab_j3_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression using Newton's method\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/100\t train acc : 0.851306882874767 \t | test acc : 0.8461230615307653\n",
      "50/100\t train acc : 0.8513444190032907 \t | test acc : 0.8463231615807904\n",
      "75/100\t train acc : 0.8512943708319257 \t | test acc : 0.8462731365682842\n",
      "25/100\t train acc : 0.8196660482374768 \t | test acc : 0.8167620026994022\n",
      "50/100\t train acc : 0.820375897394531 \t | test acc : 0.8164406452856867\n",
      "75/100\t train acc : 0.8203436315237558 \t | test acc : 0.8163763738029436\n",
      "25/100\t train acc : 0.8515974660882738 \t | test acc : 0.8411182617119922\n",
      "50/100\t train acc : 0.8524409557254011 \t | test acc : 0.8412566604387239\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7909e5b7f849>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0minitial_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx_j2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_hessian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_j2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_j2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_j2_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_j2_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_lambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fit model, retrieve parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# initial_w = np.zeros(tx_j3.shape[1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\machine_learning\\project_1\\implementations\\implementations.py\u001b[0m in \u001b[0;36mlogistic_hessian\u001b[1;34m(y, tx, y_t, tx_t, initial_w, gamma, lam, max_iters, momentum, tol, patience, writing, threshold)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpatience\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnb_ES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[1;31m# compute gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 546\u001b[1;33m         \u001b[0mgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"logistic\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    547\u001b[0m         \u001b[0mhess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_hessian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\machine_learning\\project_1\\implementations\\implementations.py\u001b[0m in \u001b[0;36mcompute_gradient\u001b[1;34m(y, tx, w, lambda_, method)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"logistic\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mz0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_gamma = 0.07\n",
    "best_lambda = 0\n",
    "\n",
    "initial_w = np.zeros(tx_j0.shape[1])\n",
    "losses, losses_t, acc, acc_t, w_0 = logistic_hessian(y_j0, tx_j0, y_j0_t, tx_j0_t, initial_w, best_gamma, best_lambda, 100) # fit model, retrieve parameters\n",
    "\n",
    "initial_w = np.zeros(tx_j1.shape[1])\n",
    "losses, losses_t, acc, acc_t, w_1 = logistic_hessian(y_j1, tx_j1, y_j1_t, tx_j1_t, initial_w, best_gamma, best_lambda, 100) # fit model, retrieve parameters\n",
    "\n",
    "initial_w = np.zeros(tx_j2.shape[1])\n",
    "losses, losses_t, acc, acc_t, w_2 = logistic_hessian(y_j2, tx_j2, y_j2_t, tx_j2_t, initial_w, best_gamma, best_lambda, 100) # fit model, retrieve parameters\n",
    "\n",
    "# initial_w = np.zeros(tx_j3.shape[1])\n",
    "# losses, losses_t, acc, acc_t, w_3 = logistic_hessian(y_j3, tx_j3, y_j3_t, tx_j3_t, initial_w, best_gamma, best_lambda, 100) # fit model, retrieve parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for best paramaters\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_data(data_raw, labels_raw)\n",
    "\n",
    "# Filtering missing values and outliers\n",
    "X_train, X_test, y_train = process_data(X_train, X_test, y_train, ids_raw, sample_filtering = False, feature_filtering = False, replace = 'median',remove_outlier = True)\n",
    "\n",
    "# Build interaction terms\n",
    "data_tr_int = build_interact_terms(X_train)\n",
    "data_ts_int = build_interact_terms(X_test)\n",
    "\n",
    "# Build polynomial of degree 3\n",
    "data_tr_poly = build_poly(X_train, 4)\n",
    "data_ts_poly = build_poly(X_test, 4)\n",
    "\n",
    "# Build log \n",
    "data_tr_log = np.log(abs(X_train)+1)\n",
    "data_ts_log = np.log(abs(X_test)+1)\n",
    "\n",
    "\n",
    "# Combine polynomial and int term\n",
    "data_train = np.c_[data_tr_poly, data_tr_int, data_tr_log]\n",
    "data_test = np.c_[data_ts_poly, data_ts_int, data_ts_log]\n",
    "\n",
    "# Perform PCA\n",
    "eigVal, eigVec, sumEigVal = PCA(data_train, threshold = 0.98)\n",
    "data = data_train.dot(eigVec)\n",
    "data_t = data_test.dot(eigVec)\n",
    "print(\"we have reduce the number of feature with PCA to {0}\".format(eigVec.shape[1]))\n",
    "\n",
    "y, tx = build_model_data(data, y_train)\n",
    "y_t, tx_t = build_model_data(data_t,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 150\n",
    "lr = np.arange(0.05,0.1,0.005)\n",
    "lamb = np.logspace(-3,3,7)\n",
    "\n",
    "initial_w = np.ones(tx.shape[1])\n",
    "\n",
    "best_gamma = 0\n",
    "best_lambda = 0\n",
    "best_acc = 0\n",
    "grid_acc = np.zeros([len(lr),len(lamb)])\n",
    "\n",
    "for g in range(len(lr)):\n",
    "    for l in range(len(lamb)):\n",
    "        grid_acc[g,l] = Grid_Search_logistic(y, tx, y_t, tx_t, initial_w, gamma=lr[g], lam=lamb[l], max_iters = num_iter, momentum = 0)\n",
    "        print(\"gamma: {0} \\t| lambda: {1} \\t| test acc: {2}\".format(lr[g],lamb[l], grid_acc[g,l]))\n",
    "        if (grid_acc[g,l] > best_acc):\n",
    "            best_acc = grid_acc[g,l]\n",
    "            best_gamma = lr[g]\n",
    "            best_lambda = lamb[l]\n",
    "\n",
    "file = \"grid_search_logistic_test_acc.npy\"\n",
    "np.save(file, grid_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gamma\n",
    "best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grid_acc = np.load(file)\n",
    "import seaborn as sns; sns.set()\n",
    "ax = sns.heatmap(grid_acc,xticklabels=lamb, yticklabels=lr,vmin= 0.6, vmax=0.782)\n",
    "ax.set_xlabel('lambda')\n",
    "ax.set_ylabel('gamma')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_data(data_raw, labels_raw)\n",
    "\n",
    "# Filtering missing values and outliers\n",
    "X_train, X_test, y_train = process_data(X_train, X_test, y_train, ids_raw, sample_filtering = False, feature_filtering = False, replace = 'mean',remove_outlier = True)\n",
    "\n",
    "# Build interaction terms\n",
    "data_tr_int = build_interact_terms(X_train)\n",
    "data_ts_int = build_interact_terms(X_test)\n",
    "\n",
    "# Build polynomial of degree 3\n",
    "data_tr_poly = build_poly(X_train, 4)\n",
    "data_ts_poly = build_poly(X_test, 4)\n",
    "\n",
    "# Build log \n",
    "data_tr_log = np.log(abs(X_train)+1)\n",
    "data_ts_log = np.log(abs(X_test)+1)\n",
    "\n",
    "\n",
    "# Combine polynomial and int term\n",
    "data_train = np.c_[data_tr_poly, data_tr_int, data_tr_log]\n",
    "data_test = np.c_[data_ts_poly, data_ts_int, data_ts_log]\n",
    "\n",
    "# Perform PCA\n",
    "eigVal, eigVec, sumEigVal = PCA(data_train, threshold = 0.98)\n",
    "data = data_train.dot(eigVec)\n",
    "data_t = data_test.dot(eigVec)\n",
    "print(\"we have reduce the number of feature with PCA to {0}\".format(eigVec.shape[1]))\n",
    "\n",
    "y, tx = build_model_data(data, y_train)\n",
    "y_t, tx_t = build_model_data(data_t,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.logspace(-3,3,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_iter = 100\n",
    "lr = np.arange(0.05,0.06, 0.07)\n",
    "lamb = np.logspace(-3,3,7)\n",
    "\n",
    "initial_w = np.ones(tx.shape[1])\n",
    "\n",
    "best_gamma = 0\n",
    "best_lambda = 0\n",
    "best_acc = 0\n",
    "grid_acc = np.zeros([len(lr),len(lamb)])\n",
    "\n",
    "for g in range(len(lr)):\n",
    "    for l in range(len(lamb)):\n",
    "        grid_acc[g,l] = Grid_Search_logistic(y, tx, y_t, tx_t, initial_w, gamma=lr[g], lam=lamb[l], max_iters = num_iter, momentum = 0)\n",
    "        print(\"gamma: {0} \\t| lambda: {1} \\t| test acc: {2}\".format(lr[g],lamb[l], grid_acc[g,l]))\n",
    "        if (grid_acc[g,l] > best_acc):\n",
    "            best_acc = grid_acc[g,l]\n",
    "            best_gamma = lr[g]\n",
    "            best_lambda = lamb[l]\n",
    "\n",
    "file = \"grid_search_logistic_test_acc_mean.npy\"\n",
    "np.save(file, grid_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_acc = np.load(file)\n",
    "import seaborn as sns; sns.set()\n",
    "ax = sns.heatmap(grid_acc,xticklabels=lamb, yticklabels=lr,vmin= 0.6, vmax=0.782)\n",
    "ax.set_xlabel('lambda')\n",
    "ax.set_ylabel('gamma')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle \n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data loading\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels_t, data_raw_t, ids_t) = load_csv_data(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get jet indexes\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature jet_num\n",
    "jets = data_raw[:,22]\n",
    "jets_t = data_raw_t[:,22]\n",
    "\n",
    "# Get index of samples with appropriate jet\n",
    "idx_jet0 = np.argwhere(jets == 0)[:,0]\n",
    "idx_jet1 = np.argwhere(jets == 1)[:,0]\n",
    "idx_jet2 = np.argwhere(jets >= 2)[:,0]\n",
    "# idx_jet3 = np.argwhere(jets == 3)[:,0]\n",
    "\n",
    "idx_jet0_t = np.argwhere(jets_t == 0)[:,0]\n",
    "idx_jet1_t = np.argwhere(jets_t == 1)[:,0]\n",
    "idx_jet2_t = np.argwhere(jets_t >= 2)[:,0]\n",
    "# idx_jet3_t = np.argwhere(jets_t == 3)[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate data relative to jets\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = np.delete(data_raw, 22, axis=1)\n",
    "data_raw_t = np.delete(data_raw_t, 22, axis=1)\n",
    "# Split data relatitve to jets\n",
    "data_tr_j0 = data_raw[idx_jet0,:]\n",
    "data_tr_j1 = data_raw[idx_jet1,:]\n",
    "data_tr_j2 = data_raw[idx_jet2,:]\n",
    "# data_tr_j3 = data_raw[idx_jet3,:]\n",
    "\n",
    "data_ts_j0 = data_raw_t[idx_jet0_t,:]\n",
    "data_ts_j1 = data_raw_t[idx_jet1_t,:]\n",
    "data_ts_j2 = data_raw_t[idx_jet2_t,:]\n",
    "# data_ts_j3 = data_raw_t[idx_jet3_t,:]\n",
    "\n",
    "# Split labels relative to jets\n",
    "lab_j0 = labels_raw[idx_jet0]\n",
    "lab_j1 = labels_raw[idx_jet1]\n",
    "lab_j2 = labels_raw[idx_jet2]\n",
    "# lab_j3 = labels_raw[idx_jet3]\n",
    "\n",
    "lab_j0_t = labels_t[idx_jet0_t]\n",
    "lab_j1_t = labels_t[idx_jet1_t]\n",
    "lab_j2_t = labels_t[idx_jet2_t]\n",
    "# lab_j3_t = labels_t[idx_jet3_t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data filtering and transformation\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dimensions of the training data set was 99913 samples and 29 columns\n",
      " After feature and sample filtering, there are 99913 samples and 19 columns\n",
      "The original dimensions of the training data set was 77544 samples and 29 columns\n",
      " After feature and sample filtering, there are 77544 samples and 22 columns\n",
      "The original dimensions of the training data set was 72543 samples and 29 columns\n",
      " After feature and sample filtering, there are 72543 samples and 29 columns\n"
     ]
    }
   ],
   "source": [
    "# Filtering missing values and outliers\n",
    "data_j0, data_j0_t = process_data(data_tr_j0, data_ts_j0)\n",
    "data_j1, data_j1_t = process_data(data_tr_j1, data_ts_j1)\n",
    "data_j2, data_j2_t = process_data(data_tr_j2, data_ts_j2)\n",
    "# data_j3, data_j3_t = process_data(data_tr_j3, data_ts_j3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4ddc2dd70433>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# y_j0, tx_j0, y_j0_t, tx_j0_t = transform_data(data_j0, data_j0_t, lab_j0, lab_j0_t, long = True, pca_t = 0.9999)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# y_j1, tx_j1, y_j1_t, tx_j1_t = transform_data(data_j1, data_j1_t, lab_j1, lab_j1_t, long = True, pca_t = 0.9999)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my_j2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_j2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_j2_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_j2_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_j2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_j2_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlab_j2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlab_j2_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlong\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpca_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# y_j3, tx_j3, y_j3_t, tx_j3_t = transform_data(data_j3, data_j3_t, lab_j3, lab_j3_t)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\machine_learning\\project_1\\implementations\\preprocessing.py\u001b[0m in \u001b[0;36mtransform_data\u001b[1;34m(data, data_t, y, y_t, poly, interact, log, three, long, pca_t)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minteract\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;31m# Build interaction terms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mdata_tr_int\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_interact_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[0mdata_ts_int\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_interact_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mdata_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_tr_int\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\machine_learning\\project_1\\implementations\\implementations.py\u001b[0m in \u001b[0;36mbuild_interact_terms\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0minteract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0minteract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minteract\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minteract\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    338\u001b[0m                 \u001b[0mobjs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Transforming data using polynomials, log and interaction terms\n",
    "# y_j0, tx_j0, y_j0_t, tx_j0_t = transform_data(data_j0, data_j0_t, lab_j0, lab_j0_t, long = True, pca_t = 0.9999)\n",
    "# y_j1, tx_j1, y_j1_t, tx_j1_t = transform_data(data_j1, data_j1_t, lab_j1, lab_j1_t, long = True, pca_t = 0.9999)\n",
    "y_j2, tx_j2, y_j2_t, tx_j2_t = transform_data(data_j2, data_j2_t, lab_j2, lab_j2_t, long = True, pca_t = 0.999)\n",
    "# y_j3, tx_j3, y_j3_t, tx_j3_t = transform_data(data_j3, data_j3_t, lab_j3, lab_j3_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression using Newton's method\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gamma = 0.07\n",
    "best_lambda = 0\n",
    "\n",
    "initial_w = np.zeros(tx_j0.shape[1])\n",
    "losses, losses_t, acc, acc_t, w_0 = logistic_hessian(y_j0, tx_j0, y_j0_t, tx_j0_t, initial_w, best_gamma, best_lambda, 100) # fit model, retrieve parameters\n",
    "\n",
    "initial_w = np.zeros(tx_j1.shape[1])\n",
    "losses, losses_t, acc, acc_t, w_1 = logistic_hessian(y_j1, tx_j1, y_j1_t, tx_j1_t, initial_w, best_gamma, best_lambda, 100) # fit model, retrieve parameters\n",
    "\n",
    "initial_w = np.zeros(tx_j2.shape[1])\n",
    "losses, losses_t, acc, acc_t, w_2 = logistic_hessian(y_j2, tx_j2, y_j2_t, tx_j2_t, initial_w, best_gamma, best_lambda, 100) # fit model, retrieve parameters\n",
    "\n",
    "# initial_w = np.zeros(tx_j3.shape[1])\n",
    "# losses, losses_t, acc, acc_t, w_3 = logistic_hessian(y_j3, tx_j3, y_j3_t, tx_j3_t, initial_w, best_gamma, best_lambda, 100) # fit model, retrieve parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle submission\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_t = np.zeros(ids_t.shape)\n",
    "\n",
    "pred_t[idx_jet0_t] = predict_labels_logistic(w_0, tx_j0_t, 0.5)\n",
    "pred_t[idx_jet1_t] = predict_labels_logistic(w_1, tx_j1_t, 0.5)\n",
    "pred_t[idx_jet2_t] = predict_labels_logistic(w_2, tx_j2_t, 0.5)\n",
    "# pred_t[idx_jet3_t] = predict_labels_logistic(w_3, tx_j3_t, 0.5)\n",
    "\n",
    "name = \"jets_better.csv\"\n",
    "create_csv_submission(ids_t, pred_t, name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
