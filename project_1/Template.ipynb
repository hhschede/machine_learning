{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing\n",
    "\n",
    "# modules\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import datetime\n",
    "\n",
    "# functions\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "\n",
    "#data\n",
    "(labels, data, ids) = load_csv_data(\"data/train.csv\")  # load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dimensions of the training data set was 250000 samples and 30 columns. After feature and sample filtering, there are 223877 samples and 23 columns\n",
      "Standardized and randomized samples are found as the variables X_train, y_train, X_test, y_test. Values are split for testing and training sets with the test_ratio of 0.3\n"
     ]
    }
   ],
   "source": [
    "# Function that returns tuple (data_processed and new_labels)\n",
    "\n",
    "def process_data(data, labels, sample_filtering=True, feature_filtering=True):\n",
    "    # Preparing data\n",
    "\n",
    "    # set values of -999 to NaN. \n",
    "    # Then calculate the means of the features. \n",
    "    # Replace NaN values with new values.\n",
    "\n",
    "    data_process = np.array(data[:,:])\n",
    "    labels_select = np.array(labels[:])\n",
    "    lab = []\n",
    "    for entry in labels_select:\n",
    "        if int(entry) == 1:\n",
    "            lab.append(1)\n",
    "        else:\n",
    "            lab.append(0)\n",
    "    lab = np.array(lab)\n",
    "    data_process[data_process == -999] = np.nan\n",
    "\n",
    "    # Filtering weak features and samples\n",
    "\n",
    "    # retrieve percentage for each feature \n",
    "    # - how many nan's are there?\n",
    "    \n",
    "    if feature_filtering:\n",
    "        nan_count = []\n",
    "        for c in data_process.T:\n",
    "            count = 0\n",
    "            for e in c:\n",
    "                if np.isnan(e):\n",
    "                    count += 1\n",
    "            pcent = count / data_process.shape[0]\n",
    "            nan_count.append(pcent)\n",
    "\n",
    "        # filter out features which have nan values of 50%\n",
    "\n",
    "        data_set_filtered = []\n",
    "        for idx, entry in enumerate(nan_count):\n",
    "            if entry < 0.6:\n",
    "                #append the column of the original dataset that is good\n",
    "                data_set_filtered.append(data_process.T[idx]) \n",
    "        #save that shit as an np array\n",
    "        data_set_filtered = np.array(data_set_filtered).T #save that shit as an np array\n",
    "        \n",
    "    if sample_filtering:\n",
    "        nan_count_2 = []\n",
    "        data_set_filtered_2 = [] # dataset filtered for columns and samples\n",
    "        y = [] # array that gets rid of entries that are no longer corresponding in the dataframe\n",
    "        for sample in data_set_filtered:\n",
    "            count = 0\n",
    "            for col in sample:\n",
    "                if np.isnan(col):\n",
    "                    count += 1\n",
    "            pcent = count / data_set_filtered.shape[1]\n",
    "            nan_count_2.append(pcent)\n",
    "\n",
    "        for idx, entry in enumerate(nan_count_2):\n",
    "            if entry < 0.15:\n",
    "                y.append(lab[idx])\n",
    "                data_set_filtered_2.append(data_set_filtered[idx])\n",
    "        data_set_filtered_2 = np.array(data_set_filtered_2) # turn dat shit into an array\n",
    "        y = np.array(y) # also this one gotta be an array\n",
    "\n",
    "\n",
    "    # print new dimensions of the dataframe after filtering\n",
    "\n",
    "    print('The original dimensions of the training data set was {0} samples'\n",
    "          ' and {1} columns. After feature and sample filtering, there are'\n",
    "          ' {2} samples and {3} columns'.format(data_process.shape[0],\n",
    "                                                data_process.shape[1],\n",
    "                                                data_set_filtered_2.shape[0],\n",
    "                                                data_set_filtered_2.shape[1]))\n",
    "\n",
    "    # Getting Rid of NAN and Replacing with Mean\n",
    "\n",
    "    # variable reassigned\n",
    "    data_nan = data_set_filtered_2.copy() \n",
    "    # create list with average values of columns, excluding nans\n",
    "    column_means = [] \n",
    "    for column in data_nan.T:\n",
    "        column_means.append(np.nanmean(column))\n",
    "    # variable containing locations of nan in data frame\n",
    "    inds = np.where(np.isnan(data_nan)) \n",
    "    # reassign locations of nan to the column means\n",
    "    data_nan[inds] = np.take(column_means, inds[1])\n",
    "    return (data_nan, y)\n",
    "\n",
    "def standardize(data):\n",
    "     # standardize and normalize the features\n",
    "    for idx, column in enumerate(data.T):\n",
    "        mean = np.mean(column)\n",
    "        variance = np.std(column)\n",
    "        for entry, value in enumerate(column):\n",
    "            data[entry, idx] = (value - mean)/variance\n",
    "    return data\n",
    "\n",
    "(new_data, new_labels) = process_data(data, labels)\n",
    "data_std = standardize(new_data)\n",
    "\n",
    "# Shuffle data and create training and testing sections\n",
    "test_ratio_ = 0.3\n",
    "X_train, y_train, X_test, y_test = test_train(data_std, new_labels, test_ratio = test_ratio_)\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "\n",
    "print('Standardized and randomized samples are found as the variables X_train, y_train, X_test, y_test. Values' +\n",
    "      ' are split for testing and training sets with the test_ratio of {}'.format(test_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
